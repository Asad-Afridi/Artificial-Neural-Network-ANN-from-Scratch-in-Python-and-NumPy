{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.0 2.0 6.0 1.0\n",
      "6.0\n",
      "6.0\n"
     ]
    }
   ],
   "source": [
    "# Forward pass\n",
    "x = [1.0, -2.0, 3.0] # input values\n",
    "w = [-3.0, -1.0, 2.0] # weights\n",
    "b = 1.0 # bias\n",
    "\n",
    "# Multiplying inputs by weights\n",
    "xw0 = x[0] * w[0]\n",
    "xw1 = x[1] * w[1]\n",
    "xw2 = x[2] * w[2]\n",
    "print(xw0, xw1, xw2, b)\n",
    "\n",
    "# Adding weighted inputs and a bias\n",
    "z = xw0 + xw1 + xw2 + b\n",
    "print(z)\n",
    "\n",
    "# ReLU activation function\n",
    "y = max(z, 0)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### derivative of the ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "dvalue = 1 # derivative of the next layer\n",
    "drelu_dz = dvalue * (1. if z > 0 else 0.)\n",
    "print(drelu_dz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving backward through our neural network, what is the function that comes immediately \n",
    "before we perform the activation function?\n",
    "\n",
    "It’s a sum of the weighted inputs and bias. This means that we — want to calculate the partial \n",
    "derivative of the sum function, and then, using the chain rule, multiply this by the partial \n",
    "derivative of the subsequent, outer, function, which is ReLU.\n",
    "\n",
    "- `drelu_dxw0` — the partial derivative of the ReLU w.r.t. the first weighed input, w0\n",
    "x0\n",
    ",\n",
    "- `drelu_dxw1` — the partial derivative of the ReLU w.r.t. the second weighed input, w1\n",
    "x1\n",
    ",\n",
    "- `drelu_dxw2` — the partial derivative of the ReLU w.r.t. the third weighed input, w2\n",
    "x2\n",
    ",\n",
    "- `drelu_db` — the partial derivative of the ReLU with respect to the bias, b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "dsum_dxw0 = 1\n",
    "drelu_dxw0 = drelu_dz * dsum_dxw0\n",
    "print(drelu_dxw0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "dsum_dxw1 = 1\n",
    "drelu_dxw1 = drelu_dz * dsum_dxw1\n",
    "print(drelu_dxw1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "dsum_dxw2 = 1\n",
    "drelu_dxw2 = drelu_dz * dsum_dxw2\n",
    "print(drelu_dxw2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "dsum_db = 1\n",
    "drelu_db = drelu_dz * dsum_db\n",
    "print(drelu_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s add these partial derivatives, with the applied chain rule, to our code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0 1.0 1.0 1.0\n"
     ]
    }
   ],
   "source": [
    "# Forward pass\n",
    "x = [ 1.0, -2.0, 3.0] # input values\n",
    "w = [-3.0, -1.0, 20]  # weights\n",
    "b = 1.0 # bias\n",
    "\n",
    "# Multiplying inputs by weights\n",
    "xw0 = x[0] * w[0]\n",
    "xw1 = x[1] * w[1]\n",
    "xw2 = x[2] * w[2]\n",
    "\n",
    "# Adding weighted input and bias\n",
    "z = xw0 + xw1 + xw2 + b\n",
    "\n",
    "# ReLU activation function\n",
    "y = max(z, 0)\n",
    "\n",
    "# Backward pass\n",
    "\n",
    "# The derivative from the next layer\n",
    "dvalue = 1\n",
    "\n",
    "# Derivative of the ReLU and the chain rule\n",
    "drelu_dz = dvalue * (1. if z >  0 else 0)\n",
    "print(drelu_dz)\n",
    "\n",
    "# Partial derivatives of the multiplication, the chain rule\n",
    "dsum_dxw0 = 1\n",
    "dsum_dxw1 = 1\n",
    "dsum_dxw2 = 1\n",
    "dsum_db = 1\n",
    "\n",
    "drelu_dxw0 = drelu_dz * dsum_dxw0\n",
    "drelu_dxw1 = drelu_dz * dsum_dxw1\n",
    "drelu_dxw2 = drelu_dz * dsum_dxw2\n",
    "drelu_db = drelu_dz * dsum_db\n",
    "\n",
    "print(drelu_dxw0, drelu_dxw1, drelu_dxw2, drelu_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuing backward, the function that comes before the sum is the multiplication of weights and \n",
    "inputs. The derivative for a product is whatever the input is being multiplied by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.0\n"
     ]
    }
   ],
   "source": [
    "# Partial derivatives of the multiplication, the chain rule\n",
    "dmul_dx0 = w[0]\n",
    "drelu_dx0 = drelu_dxw0 * dmul_dx0\n",
    "print(drelu_dx0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform the same operation for other inputs and weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0 1.0 1.0 1.0\n",
      "-3.0 1.0 -1.0 -2.0 2.0 3.0\n"
     ]
    }
   ],
   "source": [
    "# Forward pass\n",
    "x = [ 1.0, -2.0, 3.0] # input values\n",
    "w = [-3.0, -1.0, 2.0]  # weights\n",
    "b = 1.0 # bias\n",
    "\n",
    "# Multiplying inputs by weights\n",
    "xw0 = x[0] * w[0]\n",
    "xw1 = x[1] * w[1]\n",
    "xw2 = x[2] * w[2]\n",
    "\n",
    "# Adding weighted input and bias\n",
    "z = xw0 + xw1 + xw2 + b\n",
    "\n",
    "# ReLU activation function\n",
    "y = max(z, 0)\n",
    "\n",
    "# Backward pass\n",
    "\n",
    "# The derivative from the next layer\n",
    "dvalue = 1\n",
    "\n",
    "# Derivative of the ReLU and the chain rule\n",
    "drelu_dz = dvalue * (1. if z >  0 else 0)\n",
    "print(drelu_dz)\n",
    "\n",
    "# Partial derivatives of the multiplication, the chain rule\n",
    "dsum_dxw0 = 1\n",
    "dsum_dxw1 = 1\n",
    "dsum_dxw2 = 1\n",
    "dsum_db = 1\n",
    "\n",
    "drelu_dxw0 = drelu_dz * dsum_dxw0\n",
    "drelu_dxw1 = drelu_dz * dsum_dxw1\n",
    "drelu_dxw2 = drelu_dz * dsum_dxw2\n",
    "drelu_db = drelu_dz * dsum_db\n",
    "\n",
    "print(drelu_dxw0, drelu_dxw1, drelu_dxw2, drelu_db)\n",
    "\n",
    "# Partial derivatives of the multiplication, the chain rule\n",
    "dmul_dx0 = w[0]\n",
    "dmul_dx1 = w[1]\n",
    "dmul_dx2 = w[2]\n",
    "\n",
    "dmul_dw0 = x[0]\n",
    "dmul_dw1 = x[1]\n",
    "dmul_dw2 = x[2]\n",
    "\n",
    "drelu_dx0 = drelu_dxw0 * dmul_dx0\n",
    "drelu_dx1 = drelu_dxw1 * dmul_dx1\n",
    "drelu_dx2 = drelu_dxw2 * dmul_dx2\n",
    "\n",
    "drelu_dw0 = drelu_dxw0 * dmul_dw0\n",
    "drelu_dw1 = drelu_dxw1 * dmul_dw1\n",
    "drelu_dw2 = drelu_dxw2 * dmul_dw2\n",
    "\n",
    "print(drelu_dx0, drelu_dw0, drelu_dx1, drelu_dw1, drelu_dx2, drelu_dw2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All together, the partial derivatives above, combined into a vector, make up our gradients. Our \n",
    "gradients could be represented as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dx = [drelu_dx0, drelu_dx1, drelu_dx2] # Gradient on inputs\n",
    "dw = [drelu_dw0, drelu_dw1, drelu_dw2] # Gradient on weights\n",
    "db = drelu_db # Gradient on bias ... just 1 bias here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply a negative fraction to this gradient since \n",
    "we want to decrease the final output value, and the gradient shows the direction of the steepest \n",
    "ascent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.0, -1.0, 2.0] 1.0\n"
     ]
    }
   ],
   "source": [
    "print(w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.001, -0.998, 1.997] 0.999\n"
     ]
    }
   ],
   "source": [
    "w[0] += -0.001 * dw[0]\n",
    "w[1] += -0.001 * dw[1]\n",
    "w[2] += -0.001 * dw[2]\n",
    "b += -0.001 * db\n",
    "\n",
    "print(w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we’ve slightly changed the weights and bias in such a way so as to decrease the output \n",
    "somewhat intelligently. We can see the effects of our tweaks on the output by doing another \n",
    "forward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.985\n"
     ]
    }
   ],
   "source": [
    "# Multiplying inputs by weights\n",
    "xw0 = x[0] * w[0]\n",
    "xw1 = x[1] * w[1]\n",
    "xw2 = x[2] * w[2]\n",
    "\n",
    "# Adding\n",
    "z = xw0 + xw1 + xw2 + b\n",
    "\n",
    "# ReLU activation function\n",
    "y = max(z, 0)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ve successfully decreased this neuron’s output from 6.000 to 5.985."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During backpropagation, each neuron from the \n",
    "current layer will receive a vector of partial derivatives the same way that we described for a \n",
    "single neuron. With a layer of neurons, it’ll take the form of a list of these vectors, or a 2D array.\n",
    "\n",
    "To calculate the partial derivatives with respect to inputs, we need the weights — the partial \n",
    "derivative with respect to the input equals the related weight. This means that the array of \n",
    "partial derivatives with respect to all of the inputs equals the array of weights. Since this array is \n",
    "transposed, we’ll need to sum its rows instead of columns. \n",
    "\n",
    "In the code to show this, we take the transposed weights, which are the transposed array of the \n",
    "derivatives with respect to inputs, and multiply them by their respective gradients (related to \n",
    "given neurons) to apply the chain rule. Then we sum along with the inputs. Then we calculate \n",
    "the gradient for the next layer in backpropagation. The “next” layer in backpropagation is the \n",
    "previous layer in the order of creation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.44 -0.38 -0.07  1.37]\n"
     ]
    }
   ],
   "source": [
    "# Passed-in gradient from the next layer\n",
    "# for the purpose of this example we're going to use\n",
    "# a vector of 1s\n",
    "dvalues = np.array([[1., 1., 1.]])\n",
    "\n",
    "# We have 3 sets of weights - one set for each neuron\n",
    "# we have 4 inputs, thus 4 weights\n",
    "# recall that we keep weights transposed\n",
    "weights = np.array([[0.2, 0.8, -0.5, 1],\n",
    "                    [0.5, -0.91, 0.26, -0.5],\n",
    "                    [-0.26, -0.27, 0.17, 0.87]]).T\n",
    "\n",
    "# Sum weights related to the given input multiplied by\n",
    "# the gradient related to the given neuron\n",
    "dx0 = sum([weights[0][0]*dvalues[0][0], weights[0][1]*dvalues[0][1],\n",
    "           weights[0][2]*dvalues[0][2]])\n",
    "dx1 = sum([weights[1][0]*dvalues[0][0], weights[1][1]*dvalues[0][1],\n",
    "           weights[1][2]*dvalues[0][2]])\n",
    "dx2 = sum([weights[2][0]*dvalues[0][0], weights[2][1]*dvalues[0][1],\n",
    "           weights[2][2]*dvalues[0][2]])\n",
    "dx3 = sum([weights[3][0]*dvalues[0][0], weights[3][1]*dvalues[0][1],\n",
    "           weights[3][2]*dvalues[0][2]])\n",
    "\n",
    "dinputs = np.array([dx0, dx1, dx2, dx3])\n",
    "print(dinputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From NumPy’s perspective, and since both weights and dvalues are NumPy arrays, we \n",
    "can simplify the dx0 to dx3 calculation. Since the weights array is formatted so that the rows \n",
    "contain weights related to each input (weights for all neurons for the given input), we can multiply \n",
    "them by the gradient vector directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.44 -0.38 -0.07  1.37]\n"
     ]
    }
   ],
   "source": [
    "# Sum weights related to the given input multiplied by\n",
    "# the gradient related to the given neuron\n",
    "\n",
    "dx0 = sum(weights[0] * dvalues[0])\n",
    "dx1 = sum(weights[1] * dvalues[0])\n",
    "dx2 = sum(weights[2] * dvalues[0])\n",
    "dx3 = sum(weights[3] * dvalues[0])\n",
    "\n",
    "dinputs = np.array([dx0, dx1, dx2, dx3])\n",
    "print(dinputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dot product takes \n",
    "rows from the first argument and columns from the second to perform multiplication and sum; \n",
    "thus, we need to transpose the weights for this calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.44 -0.38 -0.07  1.37]\n"
     ]
    }
   ],
   "source": [
    "# Sum weights related to the given input multiplied by\n",
    "# the gradient related to the given neuron\n",
    "dinputs = np.dot(dvalues[0], weights.T)\n",
    "\n",
    "print(dinputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to account for one more thing — a batch of samples. So far, we have been using a single \n",
    "sample responsible for a single gradient vector that is backpropagated between layers. The row \n",
    "vector that we created for dvalues is in preparation for a batch of data. With more samples, \n",
    "the layer will return a list of gradients, which we almost handle correctly for. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.44 -0.38 -0.07  1.37]\n",
      " [ 0.88 -0.76 -0.14  2.74]\n",
      " [ 1.32 -1.14 -0.21  4.11]]\n"
     ]
    }
   ],
   "source": [
    "# Passed-in gradient from the next layer\n",
    "# for the purpose of this example we're going to use\n",
    "# an array of an incremental gradient values\n",
    "dvalues = np.array([[1., 1., 1.],\n",
    "                     [2., 2., 2.],\n",
    "                     [3., 3., 3.]])\n",
    "\n",
    "# We have 3 sets of weights - one set for each neuron\n",
    "# we have 4 inputs, thus 4 weights\n",
    "# recall that we keep weights transposed\n",
    "weights = np.array([[0.2, 0.8, -0.5, 1],\n",
    "                     [0.5, -0.91, 0.26, -0.5],\n",
    "                     [-0.26, -0.27, 0.17, 0.87]]).T\n",
    "\n",
    "# sum weights of given input\n",
    "# and multiply by the passed-in gradient for this neuron\n",
    "dinputs = np.dot(dvalues, weights.T)\n",
    "print(dinputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the gradients with respect to weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5  0.5  0.5]\n",
      " [20.1 20.1 20.1]\n",
      " [10.9 10.9 10.9]\n",
      " [ 4.1  4.1  4.1]]\n"
     ]
    }
   ],
   "source": [
    "# Passed-in gradient from the next layer\n",
    "# for the purpose of this example we're going to use\n",
    "# an array of an incremental gradient values\n",
    "\n",
    "dvalues = np.array([[1., 1., 1.],\n",
    "                    [2., 2., 2.],\n",
    "                    [3., 3., 3.]])\n",
    "\n",
    "# We have 3 sets of inputs - samples\n",
    "inputs = np.array([[1, 2, 3, 2.5],\n",
    "                    [2., 5., -1., 2],\n",
    "                    [-1.5, 2.7, 3.3, -0.8]])\n",
    "\n",
    "dweights = np.dot(inputs.T, dvalues)\n",
    "print(dweights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the biases and derivatives with respect to them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6. 6. 6.]]\n"
     ]
    }
   ],
   "source": [
    "# Passed-in gradient from the next layer\n",
    "# for the purpose of this example we're going to use\n",
    "# an array of an incremental gradient values\n",
    "dvalues = np.array([[1., 1., 1.],\n",
    "                    [2., 2., 2.],\n",
    "                    [3., 3., 3.]])\n",
    "\n",
    "# One bias for each neuron\n",
    "# biases are the row vector with a shape (1, neurons)\n",
    "biases = np.array([[2, 3, 0.5]])\n",
    "\n",
    "# dbiases - sum values, do this over samples (first axis), keepdims\n",
    "# since this by default will produce a plain list -\n",
    "# we explained this in the chapter 4\n",
    "dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "print(dbiases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivative of the ReLU function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0 0]\n",
      " [1 0 0 1]\n",
      " [0 1 1 0]]\n",
      "[[ 1  2  0  0]\n",
      " [ 5  0  0  8]\n",
      " [ 0 10 11  0]]\n"
     ]
    }
   ],
   "source": [
    "# Example layer output\n",
    "z = np.array([[1, 2, -3, -4],\n",
    "              [2, -7, -1, 3],\n",
    "              [-1, 2, 5, -1]])\n",
    "\n",
    "dvalues = np.array([[1, 2, 3, 4],\n",
    "                    [5, 6, 7, 8],\n",
    "                    [9, 10, 11, 12]])\n",
    "\n",
    "# ReLU activation derivative\n",
    "drelu = np.zeros_like(z)\n",
    "drelu[z > 0] = 1\n",
    "print(drelu)\n",
    "\n",
    "# The chain rule\n",
    "drelu *= dvalues\n",
    "print(drelu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simplified version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  0  0]\n",
      " [ 5  0  0  8]\n",
      " [ 0 10 11  0]]\n"
     ]
    }
   ],
   "source": [
    "# ReLU activation's derivative\n",
    "# with the chain rule applied\n",
    "drelu = dvalues.copy()\n",
    "drelu[z <= 0] = 0\n",
    "print(drelu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s combine the forward and backward pass of a single neuron with a full layer and batch-based \n",
    "partial derivatives. We’ll minimize ReLU’s output, once again, only for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.179515   0.5003665 -0.262746 ]\n",
      " [ 0.742093  -0.9152577 -0.2758402]\n",
      " [-0.510153   0.2529017  0.1629592]\n",
      " [ 0.971328  -0.5021842  0.8636583]]\n",
      "[[1.98489  2.997739 0.497389]]\n"
     ]
    }
   ],
   "source": [
    "dvalues = np.array([[1., 1., 1.],\n",
    " [2., 2., 2.],\n",
    " [3., 3., 3.]])\n",
    "\n",
    "inputs = np.array([[1, 2, 3, 2.5],\n",
    " [2., 5., -1., 2],\n",
    " [-1.5, 2.7, 3.3, -0.8]])\n",
    "\n",
    "weights = np.array([[0.2, 0.8, -0.5, 1],\n",
    " [0.5, -0.91, 0.26, -0.5],\n",
    " [-0.26, -0.27, 0.17, 0.87]]).T\n",
    "\n",
    "biases = np.array([[2, 3, 0.5]])\n",
    "\n",
    "# Forward pass\n",
    "layer_outputs = np.dot(inputs, weights) + biases # Dense layer\n",
    "relu_outputs = np.maximum(0, layer_outputs) # ReLU activation\n",
    "\n",
    "# Let's optimize and test backpropagation here\n",
    "# ReLU activation - simulates derivative with respect to input values\n",
    "# from next layer passed to current layer during backpropagation\n",
    "drelu = relu_outputs.copy()\n",
    "drelu[layer_outputs <= 0] = 0\n",
    "\n",
    "# Dense layer\n",
    "# dinputs - multiply by weights\n",
    "dinputs = np.dot(drelu, weights.T)\n",
    "# dweights - multiply by inputs\n",
    "dweights = np.dot(inputs.T, drelu)\n",
    "# dbiases - sum values, do this over samples (first axis), keepdims\n",
    "# since this by default will produce a plain list -\n",
    "# we explained this in the chapter 4\n",
    "dbiases = np.sum(drelu, axis=0, keepdims=True)\n",
    "\n",
    "# Update parameters\n",
    "weights += -0.001 * dweights\n",
    "biases += -0.001 * dbiases\n",
    "\n",
    "print(weights)\n",
    "print(biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the forward method for our Layer_Dense class, we will want to remember what the \n",
    "inputs were (recall that we’ll need them when calculating the partial derivative with respect to \n",
    "weights during backpropagation), which can be easily implemented using an object property \n",
    "(self.inputs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "     ...\n",
    "     # Forward pass\n",
    "     def forward(self, inputs):\n",
    "         ...\n",
    "         self.inputs = inputs\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will add our backward pass (backpropagation) code that we developed previously into a \n",
    "new method in the layer class, which we’ll call backward:\n",
    "\n",
    "#### Dense Layer Derivative and Backward  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Dense:\n",
    "    ...\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.bias = np.sum(dvalues, axis = 0, keepdims = True)\n",
    "        \n",
    "        # Gradients on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then do the same for our ReLU class\n",
    "\n",
    "\n",
    "#### ReLU Activation Derivative and Backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_ReLU:\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.max(0,inputs)\n",
    "    \n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Since we need to modify the original variable,\n",
    "        # let's make a copy of the values first\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[self.inputs <= 0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical Cross-Entropy Loss derivative and backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-entropy loss\n",
    "class Loss_CategoricalCrossentropy:\n",
    "    ...\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        \n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        \n",
    "        # Number of labels in every samples\n",
    "        # We will use the first sample to count them\n",
    "        labels = len(dvalues[0])\n",
    "        \n",
    "        # If labels are sparse, turn them into one-hot vector\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "            \n",
    "        # Calculate gradient\n",
    "        self.dinputs = - y_true / dvalues\n",
    "        \n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### np.eye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(np.eye(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(np.eye(5)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(np.eye(5)[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax activation derivative code implementation\n",
    "\n",
    "First part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_output = [0.7, 0.1, 0.2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shape it as a list of samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.7]\n",
      " [0.1]\n",
      " [0.2]]\n"
     ]
    }
   ],
   "source": [
    "softmax_output = np.array(softmax_output).reshape(-1, 1)\n",
    "print(softmax_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Softmax’s output multiplied by the Kronecker delta.\n",
    " \n",
    "The Kronecker \n",
    "delta equals 1 when both inputs are equal, and 0 otherwise. If we visualize this as an array, we’ll \n",
    "have an array of zeros with ones on the diagonal — you might remember that we already have \n",
    "implemented such a solution using the np.eye method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(np.eye(softmax_output.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we’ll do the multiplication of both of the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.7 0.  0. ]\n",
      " [0.  0.1 0. ]\n",
      " [0.  0.  0.2]]\n"
     ]
    }
   ],
   "source": [
    "print(softmax_output * np.eye(softmax_output.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that we can gain some speed by replacing this by the np.diagflat method call, \n",
    "which computes the same solution — the diagflat method creates an array using an input vector as \n",
    "the diagonal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.7 0.  0. ]\n",
      " [0.  0.1 0. ]\n",
      " [0.  0.  0.2]]\n"
     ]
    }
   ],
   "source": [
    "print(np.diagflat(softmax_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.49 0.07 0.14]\n",
      " [0.07 0.01 0.02]\n",
      " [0.14 0.02 0.04]]\n"
     ]
    }
   ],
   "source": [
    "print(np.dot(softmax_output, softmax_output.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.21 -0.07 -0.14]\n",
      " [-0.07  0.09 -0.02]\n",
      " [-0.14 -0.02  0.16]]\n"
     ]
    }
   ],
   "source": [
    "print(np.diagflat(softmax_output) -\n",
    " np.dot(softmax_output, softmax_output.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix result of the equation and the array solution provided by the code is called the \n",
    "Jacobian matrix. In our case, the Jacobian matrix is an array of partial derivatives in all of the \n",
    "combinations of both input vectors. Remember, we are calculating the partial derivatives of every \n",
    "output of the Softmax function with respect to each input separately. We do this because each \n",
    "input influences each output due to the normalization process, which takes the sum of all the \n",
    "exponentiated inputs. The result of this operation, performed on a batch of samples, is a list of the \n",
    "Jacobian matrices, which effectively forms a 3D matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This raises a question — if sample-wise gradients are the Jacobian matrices, how do we perform \n",
    "the chain rule with the gradient back-propagated from the loss function, since it’s a vector for each \n",
    "sample? Also, what do we do with the fact that the previous layer, which is the Dense layer, will \n",
    "expect the gradients to be a 2D array?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perform this operation on each of the Jacobian matrices directly, \n",
    "applying the chain rule at the same time (applying the gradient from the loss function) using \n",
    "np.dot() — For each sample, it’ll take the row from the Jacobian matrix and multiply it by the \n",
    "corresponding value from the loss function’s gradient. As a result, the dot product of each of these \n",
    "vectors and values will return a singular value, forming a vector of the partial derivatives sample\u0002wise and a 2D array (a list of the resulting vectors) batch-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Softmax:\n",
    "    ...\n",
    "    \n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        \n",
    "        \n",
    "        # Create uninitialized array\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "        \n",
    "        # Enumerate outputs and gradients\n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
    "            # Flatten output array\n",
    "            single_output = single_output.reshape(-1,1)\n",
    "            # Calculate Jacobian matrix of the output\n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "            \n",
    "            \n",
    "            # Calculate sample-wise gradient and add it to the array of sample gradient\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Common Categorical Cross-Entropy loss and Softmax activation derivative - code implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax classifier - combined Softmax activation and cross-entropy loss for faster backward step\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "    \n",
    "    # create activation and loss function objects\n",
    "    def __int__(self):\n",
    "        self.activation = Activation_Softamx()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self, inputs, y_true):\n",
    "        # Output layer's activation function\n",
    "        self.activation.forward(inputs)\n",
    "        # Set the output\n",
    "        self.output = self.activation.output\n",
    "        # Calculate the loss and return\n",
    "        return self.loss.forward(self.output, y_true)\n",
    "    \n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        \n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        \n",
    "        # If labels are one-hot encoded, turn them into discrete values\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis = 1)\n",
    "            \n",
    "        # Copy so we can easily modify\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Calculate gradient\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now test if the combined backward step returns the same values compared to when we \n",
    "backpropagate gradients through both of the functions separately. For this example, let’s make up \n",
    "an output of the Softmax function and some target values. Next, let’s backpropagate them using \n",
    "both solutions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients: Combined loss and activation:\n",
      "[[-0.1         0.03333333  0.06666667]\n",
      " [ 0.03333333 -0.16666667  0.13333333]\n",
      " [ 0.00666667 -0.03333333  0.02666667]]\n",
      "Gradients: Separate loss and activation:\n",
      "[[-0.1         0.03333333  0.06666667]\n",
      " [ 0.03333333 -0.16666667  0.13333333]\n",
      " [ 0.00666667 -0.03333333  0.02666667]]\n"
     ]
    }
   ],
   "source": [
    "softmax_outputs = np.array([[0.7, 0.1, 0.2],\n",
    "                            [0.1, 0.5, 0.4],\n",
    "                            [0.02, 0.9, 0.08]])\n",
    "\n",
    "class_targets = np.array([0, 1, 1])\n",
    "\n",
    "softmax_loss = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "softmax_loss.backward(softmax_outputs, class_targets)\n",
    "dvalues1 = softmax_loss.dinputs\n",
    "\n",
    "activation = Activation_Softmax()\n",
    "activation.output = softmax_outputs\n",
    "loss = Loss_CategoricalCrossentropy()\n",
    "loss.backward(activation.output, class_targets)\n",
    "activation.backward(loss.dinputs)\n",
    "dvalues2 = activation.dinputs\n",
    "\n",
    "print('Gradients: Combined loss and activation:')\n",
    "print(dvalues1)\n",
    "print('Gradients: Separate loss and activation:')\n",
    "print(dvalues2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To answer the question of how \n",
    "many times faster this solution is, we can take advantage of Python’s timeit module, running \n",
    "both solutions multiple times and combining the execution times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the gradients separately is about 2.857090630239817 times slower\n"
     ]
    }
   ],
   "source": [
    "from timeit import timeit\n",
    "\n",
    "softmax_outputs = np.array([[0.7, 0.1, 0.2],\n",
    "                            [0.1, 0.5, 0.4],\n",
    "                            [0.02, 0.9, 0.08]])\n",
    "\n",
    "class_targets = np.array([0, 1, 1])\n",
    "\n",
    "def f1():\n",
    "    softmax_loss = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "    softmax_loss.backward(softmax_outputs, class_targets)\n",
    "    dvalues1 = softmax_loss.dinputs\n",
    "    \n",
    "def f2():\n",
    "    activation = Activation_Softmax()\n",
    "    activation.output = softmax_outputs\n",
    "    loss = Loss_CategoricalCrossentropy()\n",
    "    loss.backward(softmax_outputs, class_targets)\n",
    "    activation.backward(loss.dinputs)\n",
    "    dvalues2 = activation.dinputs\n",
    "\n",
    "t1 = timeit(lambda: f1(), number=10000)\n",
    "t2 = timeit(lambda: f2(), number=10000)\n",
    "print(f'Calculating the gradients separately is about {t2/t1} times slower')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Code upto this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "nnfs.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Dense:\n",
    "    \n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "    \n",
    "    def backward(self, dvalues):\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis = 0, keepdims = True)\n",
    "        \n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_ReLU:\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "    \n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[self.inputs <= 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Softmax:\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis = 1, keepdims = True))\n",
    "        probabilities = exp_values / np.sum(exp_values, axis = 1, keepdims = True)\n",
    "        self.output = probabilities\n",
    "    \n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "        \n",
    "        for index, (single_output, single_davlues) in enumerate(zip(self.output, dvalues)):\n",
    "            # Flatten output array\n",
    "            single_output = single_output.reshape(-1,1)\n",
    "            \n",
    "            # Calculate Jacobian matrix of the output\n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "            \n",
    "            # Calculate Sample-wise gradient\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    \n",
    "    def calculate(self,output, y):\n",
    "        sample_losses = self.forward(output,y)\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        return data_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        samples = len(y_pred)\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        \n",
    "        # Probabilities for target classes\n",
    "        # If sparse labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[\n",
    "                range(samples), y_true\n",
    "            ]\n",
    "        # If one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true, axis = 1)\n",
    "        # Losses\n",
    "        negative_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_likelihoods\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "        labels = len(dvalues[0])\n",
    "        \n",
    "        # If labels are sparse turn them into one-hot encoded vector\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "        \n",
    "        self.dinputs = -y_true / dvalues\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "        \n",
    "    def forward(self, inputs, y_true):\n",
    "        self.activation.forward(inputs)\n",
    "        self.output = self.activation.output\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "        \n",
    "        # If labels are one-hot encoded turn then into discrete values\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis = 1)\n",
    "        \n",
    "        self.dinputs = dvalues\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create  Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = spiral_data(samples=100, classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0acc: 0.297loss: 1.099\n",
      "epoch: 100acc: 0.333loss: 1.099\n",
      "epoch: 200acc: 0.333loss: 1.099\n",
      "epoch: 300acc: 0.333loss: 1.099\n",
      "epoch: 400acc: 0.333loss: 1.099\n",
      "epoch: 500acc: 0.333loss: 1.099\n",
      "epoch: 600acc: 0.333loss: 1.099\n",
      "epoch: 700acc: 0.333loss: 1.099\n",
      "epoch: 800acc: 0.333loss: 1.099\n",
      "epoch: 900acc: 0.333loss: 1.099\n",
      "epoch: 1000acc: 0.333loss: 1.099\n",
      "epoch: 1100acc: 0.333loss: 1.099\n",
      "epoch: 1200acc: 0.333loss: 1.099\n",
      "epoch: 1300acc: 0.333loss: 1.099\n",
      "epoch: 1400acc: 0.333loss: 1.099\n",
      "epoch: 1500acc: 0.333loss: 1.099\n",
      "epoch: 1600acc: 0.333loss: 1.099\n",
      "epoch: 1700acc: 0.333loss: 1.099\n",
      "epoch: 1800acc: 0.333loss: 1.099\n",
      "epoch: 1900acc: 0.333loss: 1.099\n",
      "epoch: 2000acc: 0.333loss: 1.099\n",
      "epoch: 2100acc: 0.333loss: 1.099\n",
      "epoch: 2200acc: 0.333loss: 1.099\n",
      "epoch: 2300acc: 0.333loss: 1.099\n",
      "epoch: 2400acc: 0.333loss: 1.099\n",
      "epoch: 2500acc: 0.333loss: 1.099\n",
      "epoch: 2600acc: 0.333loss: 1.099\n",
      "epoch: 2700acc: 0.333loss: 1.099\n",
      "epoch: 2800acc: 0.333loss: 1.099\n",
      "epoch: 2900acc: 0.333loss: 1.099\n",
      "epoch: 3000acc: 0.333loss: 1.099\n",
      "epoch: 3100acc: 0.333loss: 1.099\n",
      "epoch: 3200acc: 0.333loss: 1.099\n",
      "epoch: 3300acc: 0.333loss: 1.099\n",
      "epoch: 3400acc: 0.333loss: 1.099\n",
      "epoch: 3500acc: 0.333loss: 1.099\n",
      "epoch: 3600acc: 0.333loss: 1.099\n",
      "epoch: 3700acc: 0.333loss: 1.099\n",
      "epoch: 3800acc: 0.333loss: 1.099\n",
      "epoch: 3900acc: 0.333loss: 1.099\n",
      "epoch: 4000acc: 0.333loss: 1.099\n",
      "epoch: 4100acc: 0.333loss: 1.099\n",
      "epoch: 4200acc: 0.333loss: 1.099\n",
      "epoch: 4300acc: 0.333loss: 1.099\n",
      "epoch: 4400acc: 0.333loss: 1.099\n",
      "epoch: 4500acc: 0.333loss: 1.099\n",
      "epoch: 4600acc: 0.333loss: 1.099\n",
      "epoch: 4700acc: 0.333loss: 1.099\n",
      "epoch: 4800acc: 0.333loss: 1.099\n",
      "epoch: 4900acc: 0.333loss: 1.099\n",
      "epoch: 5000acc: 0.333loss: 1.099\n",
      "epoch: 5100acc: 0.333loss: 1.099\n",
      "epoch: 5200acc: 0.333loss: 1.099\n",
      "epoch: 5300acc: 0.333loss: 1.099\n",
      "epoch: 5400acc: 0.333loss: 1.099\n",
      "epoch: 5500acc: 0.333loss: 1.099\n",
      "epoch: 5600acc: 0.333loss: 1.099\n",
      "epoch: 5700acc: 0.333loss: 1.099\n",
      "epoch: 5800acc: 0.333loss: 1.099\n",
      "epoch: 5900acc: 0.333loss: 1.099\n",
      "epoch: 6000acc: 0.333loss: 1.099\n",
      "epoch: 6100acc: 0.333loss: 1.099\n",
      "epoch: 6200acc: 0.333loss: 1.099\n",
      "epoch: 6300acc: 0.333loss: 1.099\n",
      "epoch: 6400acc: 0.333loss: 1.099\n",
      "epoch: 6500acc: 0.333loss: 1.099\n",
      "epoch: 6600acc: 0.333loss: 1.099\n",
      "epoch: 6700acc: 0.333loss: 1.099\n",
      "epoch: 6800acc: 0.333loss: 1.099\n",
      "epoch: 6900acc: 0.333loss: 1.099\n",
      "epoch: 7000acc: 0.333loss: 1.099\n",
      "epoch: 7100acc: 0.333loss: 1.099\n",
      "epoch: 7200acc: 0.333loss: 1.099\n",
      "epoch: 7300acc: 0.333loss: 1.099\n",
      "epoch: 7400acc: 0.333loss: 1.099\n",
      "epoch: 7500acc: 0.333loss: 1.099\n",
      "epoch: 7600acc: 0.333loss: 1.099\n",
      "epoch: 7700acc: 0.333loss: 1.099\n",
      "epoch: 7800acc: 0.333loss: 1.099\n",
      "epoch: 7900acc: 0.333loss: 1.099\n",
      "epoch: 8000acc: 0.333loss: 1.099\n",
      "epoch: 8100acc: 0.333loss: 1.099\n",
      "epoch: 8200acc: 0.333loss: 1.099\n",
      "epoch: 8300acc: 0.333loss: 1.099\n",
      "epoch: 8400acc: 0.333loss: 1.099\n",
      "epoch: 8500acc: 0.333loss: 1.099\n",
      "epoch: 8600acc: 0.333loss: 1.099\n",
      "epoch: 8700acc: 0.333loss: 1.099\n",
      "epoch: 8800acc: 0.333loss: 1.099\n",
      "epoch: 8900acc: 0.333loss: 1.099\n",
      "epoch: 9000acc: 0.333loss: 1.099\n",
      "epoch: 9100acc: 0.333loss: 1.099\n",
      "epoch: 9200acc: 0.333loss: 1.099\n",
      "epoch: 9300acc: 0.333loss: 1.099\n",
      "epoch: 9400acc: 0.333loss: 1.099\n",
      "epoch: 9500acc: 0.333loss: 1.099\n",
      "epoch: 9600acc: 0.333loss: 1.099\n",
      "epoch: 9700acc: 0.333loss: 1.099\n",
      "epoch: 9800acc: 0.333loss: 1.099\n",
      "epoch: 9900acc: 0.333loss: 1.099\n",
      "epoch: 10000acc: 0.333loss: 1.099\n"
     ]
    }
   ],
   "source": [
    "dense1 = Layer_Dense(2,3)\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = Layer_Dense(3,3)\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Forward pass\n",
    "dense1.forward(X)\n",
    "activation1.forward(dense1.output)\n",
    "dense2.forward(activation1.output)\n",
    "loss = loss_activation.forward(dense2.output,y)\n",
    "\n",
    "print(loss_activation.output[:5])\n",
    "print('loss: ',loss)\n",
    "\n",
    "predictions = np.argmax(loss_activation.output, axis = 1)\n",
    "if len(y.shape) == 2:\n",
    "    y = np.argmax(y, axis = 1)\n",
    "accuracy = np.mean(predictions == y)\n",
    "print('acc: ', accuracy)\n",
    "\n",
    "# Backward pass\n",
    "loss_activation.backward(loss_activation.output, y)\n",
    "dense2.backward(loss_activation.dinputs)\n",
    "activation1.backward(dense2.dinputs)\n",
    "dense1.backward(activation1.dinputs)\n",
    "\n",
    "print(dense1.dweights)\n",
    "print(dense1.dbiases)\n",
    "print(dense2.dweights)\n",
    "print(dense2.dbiases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
