{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent (SGD)\n",
    "\n",
    "First, in the context of deep learning and this book, we call slices of data batches, where, \n",
    "historically, the term to refer to slices of data in the context of Stochastic Gradient Descent was \n",
    "mini-batches. In our context, it does not matter if the batch contains a single sample, a slice of \n",
    "the dataset, or the full dataset — as a batch of the data. Additionally, with the current code, we are \n",
    "fitting the full dataset; following this naming convention, we would use Batch Gradient Descent. \n",
    "In a future chapter, we’ll introduce data slices, or batches, so we should start by using the Mini\u0002batch Gradient Descent optimizer. That said, current naming trends and conventions with \n",
    "Stochastic Gradient Descent in use with deep learning today have merged and normalized all of \n",
    "these variants, to the point where we think of the Stochastic Gradient Descent optimizer as one \n",
    "that assumes a batch of data, whether that batch happens to be a single sample, every sample in a \n",
    "dataset, or some subset of the full dataset at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_SGD:\n",
    "    \n",
    "    # Initialize optimizer - set settings, learning rate of 1.0 is default for this optimizer\n",
    "    def __init__(self, learning_rate = 1.0):\n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    def update_params(self, layer):\n",
    "        layer.weights += -self.learning_rate * layer.dweights\n",
    "        layer.biases += -self.learning_rate * layer.dbiases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Add this after the backward portion in the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "nnfs.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Dense:\n",
    "    \n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "    \n",
    "    def backward(self, dvalues):\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis = 0, keepdims = True)\n",
    "        \n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_ReLU:\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "    \n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[self.inputs <= 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Softmax:\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis = 1, keepdims = True))\n",
    "        probabilities = exp_values / np.sum(exp_values, axis = 1, keepdims = True)\n",
    "        self.output = probabilities\n",
    "    \n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "        \n",
    "        for index, (single_output, single_davlues) in enumerate(zip(self.output, dvalues)):\n",
    "            # Flatten output array\n",
    "            single_output = single_output.reshape(-1,1)\n",
    "            \n",
    "            # Calculate Jacobian matrix of the output\n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "            \n",
    "            # Calculate Sample-wise gradient\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    \n",
    "    def calculate(self,output, y):\n",
    "        sample_losses = self.forward(output,y)\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        return data_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        samples = len(y_pred)\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        \n",
    "        # Probabilities for target classes\n",
    "        # If sparse labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[\n",
    "                range(samples),\n",
    "                y_true\n",
    "            ]\n",
    "            \n",
    "        # If one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true, axis = 1)\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "        labels = len(dvalues[0])\n",
    "        \n",
    "        # If labels are sparse turn them into one-hot encoded vector\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "        \n",
    "        self.dinputs = -y_true / dvalues\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "        \n",
    "    def forward(self, inputs, y_true):\n",
    "        self.activation.forward(inputs)\n",
    "        self.output = self.activation.output\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "        \n",
    "        # If labels are one-hot encoded turn then into discrete values\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis = 1)\n",
    "        \n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create  Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = spiral_data(samples=100, classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense1 = Layer_Dense(2,64)\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = Layer_Dense(64,3)\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Optimizer_SGD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  1.0985943\n",
      "acc:  0.36\n"
     ]
    }
   ],
   "source": [
    "# Forward pass\n",
    "dense1.forward(X)\n",
    "activation1.forward(dense1.output)\n",
    "dense2.forward(activation1.output)\n",
    "loss = loss_activation.forward(dense2.output,y)\n",
    "\n",
    "print('loss: ',loss)\n",
    "\n",
    "predictions = np.argmax(loss_activation.output, axis = 1)\n",
    "if len(y.shape) == 2:\n",
    "    y = np.argmax(y, axis = 1)\n",
    "accuracy = np.mean(predictions == y)\n",
    "print('acc: ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward pass\n",
    "loss_activation.backward(loss_activation.output, y)\n",
    "dense2.backward(loss_activation.dinputs)\n",
    "activation1.backward(dense2.dinputs)\n",
    "dense1.backward(activation1.dinputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update weights and biases\n",
    "optimizer.update_params(dense1)\n",
    "optimizer.update_params(dense2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is everything we need to train our model! But why would we only perform this optimization \n",
    "once, when we can perform it lots of times by leveraging Python’s looping capabilities? We will \n",
    "repeatedly perform a forward pass, backward pass, and optimization until we reach some stopping \n",
    "point. Each full pass through all of the training data is called an epoch.\n",
    "\n",
    "To add multiple \n",
    "epochs of training into our code, we will initialize our model and run a loop around all the code \n",
    "performing the forward pass, backward pass, and optimization calculations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create  Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = spiral_data(samples=100, classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.343, loss: 1.099, \n",
      "epoch: 100, acc: 0.407, loss: 1.083, \n",
      "epoch: 200, acc: 0.397, loss: 1.071, \n",
      "epoch: 300, acc: 0.410, loss: 1.070, \n",
      "epoch: 400, acc: 0.410, loss: 1.069, \n",
      "epoch: 500, acc: 0.413, loss: 1.067, \n",
      "epoch: 600, acc: 0.410, loss: 1.064, \n",
      "epoch: 700, acc: 0.423, loss: 1.058, \n",
      "epoch: 800, acc: 0.450, loss: 1.047, \n",
      "epoch: 900, acc: 0.430, loss: 1.050, \n",
      "epoch: 1000, acc: 0.427, loss: 1.045, \n",
      "epoch: 1100, acc: 0.440, loss: 1.038, \n",
      "epoch: 1200, acc: 0.453, loss: 1.029, \n",
      "epoch: 1300, acc: 0.400, loss: 1.019, \n",
      "epoch: 1400, acc: 0.480, loss: 1.026, \n",
      "epoch: 1500, acc: 0.413, loss: 1.003, \n",
      "epoch: 1600, acc: 0.397, loss: 0.994, \n",
      "epoch: 1700, acc: 0.443, loss: 0.976, \n",
      "epoch: 1800, acc: 0.403, loss: 0.995, \n",
      "epoch: 1900, acc: 0.463, loss: 0.973, \n",
      "epoch: 2000, acc: 0.487, loss: 0.969, \n",
      "epoch: 2100, acc: 0.470, loss: 0.956, \n",
      "epoch: 2200, acc: 0.497, loss: 0.951, \n",
      "epoch: 2300, acc: 0.480, loss: 0.936, \n",
      "epoch: 2400, acc: 0.470, loss: 0.915, \n",
      "epoch: 2500, acc: 0.493, loss: 0.904, \n",
      "epoch: 2600, acc: 0.587, loss: 0.862, \n",
      "epoch: 2700, acc: 0.557, loss: 0.821, \n",
      "epoch: 2800, acc: 0.543, loss: 0.827, \n",
      "epoch: 2900, acc: 0.580, loss: 0.814, \n",
      "epoch: 3000, acc: 0.637, loss: 0.801, \n",
      "epoch: 3100, acc: 0.650, loss: 0.761, \n",
      "epoch: 3200, acc: 0.613, loss: 0.813, \n",
      "epoch: 3300, acc: 0.570, loss: 0.728, \n",
      "epoch: 3400, acc: 0.667, loss: 0.716, \n",
      "epoch: 3500, acc: 0.670, loss: 0.714, \n",
      "epoch: 3600, acc: 0.657, loss: 0.755, \n",
      "epoch: 3700, acc: 0.673, loss: 0.687, \n",
      "epoch: 3800, acc: 0.637, loss: 0.651, \n",
      "epoch: 3900, acc: 0.637, loss: 0.647, \n",
      "epoch: 4000, acc: 0.660, loss: 0.630, \n",
      "epoch: 4100, acc: 0.637, loss: 0.634, \n",
      "epoch: 4200, acc: 0.630, loss: 0.632, \n",
      "epoch: 4300, acc: 0.653, loss: 0.622, \n",
      "epoch: 4400, acc: 0.670, loss: 0.605, \n",
      "epoch: 4500, acc: 0.653, loss: 0.612, \n",
      "epoch: 4600, acc: 0.677, loss: 0.614, \n",
      "epoch: 4700, acc: 0.717, loss: 0.615, \n",
      "epoch: 4800, acc: 0.650, loss: 0.611, \n",
      "epoch: 4900, acc: 0.670, loss: 0.599, \n",
      "epoch: 5000, acc: 0.673, loss: 0.602, \n",
      "epoch: 5100, acc: 0.517, loss: 1.290, \n",
      "epoch: 5200, acc: 0.687, loss: 0.588, \n",
      "epoch: 5300, acc: 0.673, loss: 0.591, \n",
      "epoch: 5400, acc: 0.677, loss: 0.586, \n",
      "epoch: 5500, acc: 0.683, loss: 0.587, \n",
      "epoch: 5600, acc: 0.683, loss: 0.596, \n",
      "epoch: 5700, acc: 0.547, loss: 1.120, \n",
      "epoch: 5800, acc: 0.673, loss: 0.647, \n",
      "epoch: 5900, acc: 0.690, loss: 0.570, \n",
      "epoch: 6000, acc: 0.690, loss: 0.569, \n",
      "epoch: 6100, acc: 0.693, loss: 0.568, \n",
      "epoch: 6200, acc: 0.693, loss: 0.565, \n",
      "epoch: 6300, acc: 0.703, loss: 0.571, \n",
      "epoch: 6400, acc: 0.537, loss: 1.350, \n",
      "epoch: 6500, acc: 0.693, loss: 0.587, \n",
      "epoch: 6600, acc: 0.700, loss: 0.583, \n",
      "epoch: 6700, acc: 0.703, loss: 0.576, \n",
      "epoch: 6800, acc: 0.710, loss: 0.568, \n",
      "epoch: 6900, acc: 0.720, loss: 0.568, \n",
      "epoch: 7000, acc: 0.700, loss: 0.561, \n",
      "epoch: 7100, acc: 0.597, loss: 0.951, \n",
      "epoch: 7200, acc: 0.697, loss: 0.602, \n",
      "epoch: 7300, acc: 0.720, loss: 0.570, \n",
      "epoch: 7400, acc: 0.717, loss: 0.557, \n",
      "epoch: 7500, acc: 0.700, loss: 0.540, \n",
      "epoch: 7600, acc: 0.703, loss: 0.638, \n",
      "epoch: 7700, acc: 0.720, loss: 0.563, \n",
      "epoch: 7800, acc: 0.717, loss: 0.556, \n",
      "epoch: 7900, acc: 0.713, loss: 0.543, \n",
      "epoch: 8000, acc: 0.713, loss: 0.535, \n",
      "epoch: 8100, acc: 0.670, loss: 0.669, \n",
      "epoch: 8200, acc: 0.723, loss: 0.545, \n",
      "epoch: 8300, acc: 0.720, loss: 0.541, \n",
      "epoch: 8400, acc: 0.740, loss: 0.507, \n",
      "epoch: 8500, acc: 0.723, loss: 0.542, \n",
      "epoch: 8600, acc: 0.723, loss: 0.545, \n",
      "epoch: 8700, acc: 0.723, loss: 0.531, \n",
      "epoch: 8800, acc: 0.727, loss: 0.528, \n",
      "epoch: 8900, acc: 0.720, loss: 0.505, \n",
      "epoch: 9000, acc: 0.730, loss: 0.520, \n",
      "epoch: 9100, acc: 0.743, loss: 0.521, \n",
      "epoch: 9200, acc: 0.743, loss: 0.495, \n",
      "epoch: 9300, acc: 0.767, loss: 0.503, \n",
      "epoch: 9400, acc: 0.720, loss: 0.606, \n",
      "epoch: 9500, acc: 0.777, loss: 0.497, \n",
      "epoch: 9600, acc: 0.770, loss: 0.503, \n",
      "epoch: 9700, acc: 0.777, loss: 0.489, \n",
      "epoch: 9800, acc: 0.780, loss: 0.496, \n",
      "epoch: 9900, acc: 0.777, loss: 0.474, \n",
      "epoch: 10000, acc: 0.787, loss: 0.485, \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-2dd8f7c447cc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[0mep\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mep\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "dense1 = Layer_Dense(2,64)\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = Layer_Dense(64,3)\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "optimizer = Optimizer_SGD()\n",
    "ep = []\n",
    "l = []\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "    # Forward pass\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "    loss = loss_activation.forward(dense2.output,y)\n",
    "\n",
    "    predictions = np.argmax(loss_activation.output, axis = 1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis = 1)\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    \n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' + \n",
    "              f'acc: {accuracy:.3f}, ' + \n",
    "              f'loss: {loss:.3f}, ')\n",
    "    \n",
    "    # Backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    \n",
    "    # Update weights and biases\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    ep.append(epoch)\n",
    "    l.append(loss)\n",
    "plt.plot(ep,l)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we make the learning rate 0.85 rather than 1.0 with the SGD optimizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.390, loss: 1.099, \n",
      "epoch: 100, acc: 0.390, loss: 1.088, \n",
      "epoch: 200, acc: 0.400, loss: 1.071, \n",
      "epoch: 300, acc: 0.407, loss: 1.070, \n",
      "epoch: 400, acc: 0.407, loss: 1.069, \n",
      "epoch: 500, acc: 0.400, loss: 1.068, \n",
      "epoch: 600, acc: 0.413, loss: 1.067, \n",
      "epoch: 700, acc: 0.420, loss: 1.065, \n",
      "epoch: 800, acc: 0.420, loss: 1.063, \n",
      "epoch: 900, acc: 0.443, loss: 1.059, \n",
      "epoch: 1000, acc: 0.450, loss: 1.054, \n",
      "epoch: 1100, acc: 0.470, loss: 1.046, \n",
      "epoch: 1200, acc: 0.400, loss: 1.055, \n",
      "epoch: 1300, acc: 0.407, loss: 1.054, \n",
      "epoch: 1400, acc: 0.403, loss: 1.052, \n",
      "epoch: 1500, acc: 0.400, loss: 1.053, \n",
      "epoch: 1600, acc: 0.410, loss: 1.052, \n",
      "epoch: 1700, acc: 0.417, loss: 1.091, \n",
      "epoch: 1800, acc: 0.397, loss: 1.054, \n",
      "epoch: 1900, acc: 0.460, loss: 1.048, \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-e918dbce6723>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;31m# Backward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mloss_activation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_activation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m     \u001b[0mdense2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_activation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdinputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m     \u001b[0mactivation1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdense2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdinputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0mdense1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactivation1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdinputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-c7b355a3000b>\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, dvalues)\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdbiases\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdinputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nnfs\\core.py\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0morig_dot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0morig_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float64'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\overrides.py\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "dense1 = Layer_Dense(2,64)\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = Layer_Dense(64,3)\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "optimizer = Optimizer_SGD(learning_rate = 0.85)\n",
    "l = []\n",
    "ep = []\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "    # Forward pass\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "    loss = loss_activation.forward(dense2.output,y)\n",
    "\n",
    "    predictions = np.argmax(loss_activation.output, axis = 1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis = 1)\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    ep.append(epoch)\n",
    "    l.append(loss)\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' + \n",
    "              f'acc: {accuracy:.3f}, ' + \n",
    "              f'loss: {loss:.3f}, ')\n",
    "    \n",
    "    # Backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    \n",
    "    # Update weights and biases\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "plt.plot(ep,l)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9090909090909091\n"
     ]
    }
   ],
   "source": [
    "starting_learning_rate = 1.0\n",
    "learning_rate_decay = 0.1\n",
    "step = 1\n",
    "\n",
    "learning_rate = starting_learning_rate * (1.0 / (1+ learning_rate_decay * step))\n",
    "print(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, 0.1 would be considered a fairly aggressive decay rate, but this should give you a \n",
    "sense of the concept. If we are on step 20:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "starting_learning_rate = 1.\n",
    "learning_rate_decay = 0.1\n",
    "step = 20\n",
    "\n",
    "learning_rate = starting_learning_rate * \\\n",
    "                (1. / (1 + learning_rate_decay * step))\n",
    "print(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also simulate this in a loop, which is more comparable to how we will be applying \n",
    "learning rate decay:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.9090909090909091\n",
      "0.8333333333333334\n",
      "0.7692307692307692\n",
      "0.7142857142857143\n",
      "0.6666666666666666\n",
      "0.625\n",
      "0.588235294117647\n",
      "0.5555555555555556\n",
      "0.5263157894736842\n",
      "0.5\n",
      "0.47619047619047616\n",
      "0.45454545454545453\n",
      "0.4347826086956522\n",
      "0.41666666666666663\n",
      "0.4\n",
      "0.3846153846153846\n",
      "0.37037037037037035\n",
      "0.35714285714285715\n",
      "0.3448275862068965\n",
      "0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "starting_learning_rate = 1.\n",
    "learning_rate_decay = 0.1\n",
    "\n",
    "for step in range(21):\n",
    "    learning_rate = starting_learning_rate * (1. / (1 + learning_rate_decay * step))\n",
    "    print(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now update our SGD optimizer \n",
    "class to allow for the learning rate decay:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_SGD:\n",
    "    \n",
    "    def __init__(self, learning_rate=1., decay=0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        \n",
    "    # Call once before any parameter updates\\\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / (1. + self.decay * self.iterations))\n",
    "                 \n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        layer.weights += -self.current_learning_rate * layer.dweights\n",
    "        layer.biases += -self.current_learning_rate * layer.dbiases\n",
    "    \n",
    "    # Call once any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let’s use a decay rate of 1e-2 (0.01) and train our model again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.283, loss: 1.099, lr: 1.0\n",
      "epoch: 100, acc: 0.410, loss: 1.094, lr: 0.5025125628140703\n",
      "epoch: 200, acc: 0.403, loss: 1.079, lr: 0.33444816053511706\n",
      "epoch: 300, acc: 0.413, loss: 1.073, lr: 0.2506265664160401\n",
      "epoch: 400, acc: 0.400, loss: 1.071, lr: 0.2004008016032064\n",
      "epoch: 500, acc: 0.400, loss: 1.070, lr: 0.1669449081803005\n",
      "epoch: 600, acc: 0.407, loss: 1.070, lr: 0.14306151645207438\n",
      "epoch: 700, acc: 0.407, loss: 1.070, lr: 0.1251564455569462\n",
      "epoch: 800, acc: 0.407, loss: 1.070, lr: 0.11123470522803114\n",
      "epoch: 900, acc: 0.410, loss: 1.070, lr: 0.10010010010010009\n",
      "epoch: 1000, acc: 0.413, loss: 1.070, lr: 0.09099181073703366\n",
      "epoch: 1100, acc: 0.413, loss: 1.070, lr: 0.08340283569641367\n",
      "epoch: 1200, acc: 0.413, loss: 1.069, lr: 0.07698229407236336\n",
      "epoch: 1300, acc: 0.413, loss: 1.069, lr: 0.07147962830593281\n",
      "epoch: 1400, acc: 0.413, loss: 1.069, lr: 0.066711140760507\n",
      "epoch: 1500, acc: 0.413, loss: 1.069, lr: 0.06253908692933083\n",
      "epoch: 1600, acc: 0.413, loss: 1.069, lr: 0.05885815185403177\n",
      "epoch: 1700, acc: 0.413, loss: 1.069, lr: 0.055586436909394105\n",
      "epoch: 1800, acc: 0.413, loss: 1.069, lr: 0.052659294365455495\n",
      "epoch: 1900, acc: 0.413, loss: 1.069, lr: 0.05002501250625312\n",
      "epoch: 2000, acc: 0.413, loss: 1.069, lr: 0.047641734159123386\n",
      "epoch: 2100, acc: 0.413, loss: 1.069, lr: 0.04547521600727603\n",
      "epoch: 2200, acc: 0.413, loss: 1.069, lr: 0.04349717268377555\n",
      "epoch: 2300, acc: 0.413, loss: 1.069, lr: 0.04168403501458941\n",
      "epoch: 2400, acc: 0.413, loss: 1.069, lr: 0.04001600640256102\n",
      "epoch: 2500, acc: 0.413, loss: 1.069, lr: 0.03847633705271258\n",
      "epoch: 2600, acc: 0.413, loss: 1.069, lr: 0.03705075954057058\n",
      "epoch: 2700, acc: 0.413, loss: 1.069, lr: 0.03572704537334762\n",
      "epoch: 2800, acc: 0.413, loss: 1.069, lr: 0.03449465332873405\n",
      "epoch: 2900, acc: 0.410, loss: 1.069, lr: 0.03334444814938312\n",
      "epoch: 3000, acc: 0.410, loss: 1.069, lr: 0.03226847370119393\n",
      "epoch: 3100, acc: 0.410, loss: 1.069, lr: 0.03125976867771178\n",
      "epoch: 3200, acc: 0.413, loss: 1.069, lr: 0.03031221582297666\n",
      "epoch: 3300, acc: 0.413, loss: 1.069, lr: 0.02942041776993233\n",
      "epoch: 3400, acc: 0.413, loss: 1.069, lr: 0.028579594169762787\n",
      "epoch: 3500, acc: 0.413, loss: 1.069, lr: 0.027785495971103084\n",
      "epoch: 3600, acc: 0.413, loss: 1.069, lr: 0.02703433360367667\n",
      "epoch: 3700, acc: 0.413, loss: 1.069, lr: 0.026322716504343247\n",
      "epoch: 3800, acc: 0.410, loss: 1.069, lr: 0.025647601949217745\n",
      "epoch: 3900, acc: 0.407, loss: 1.069, lr: 0.02500625156289072\n",
      "epoch: 4000, acc: 0.407, loss: 1.069, lr: 0.02439619419370578\n",
      "epoch: 4100, acc: 0.407, loss: 1.069, lr: 0.023815194093831864\n",
      "epoch: 4200, acc: 0.407, loss: 1.069, lr: 0.02326122354035822\n",
      "epoch: 4300, acc: 0.407, loss: 1.069, lr: 0.022732439190725165\n",
      "epoch: 4400, acc: 0.407, loss: 1.069, lr: 0.02222716159146477\n",
      "epoch: 4500, acc: 0.407, loss: 1.068, lr: 0.021743857360295715\n",
      "epoch: 4600, acc: 0.407, loss: 1.068, lr: 0.021281123643328365\n",
      "epoch: 4700, acc: 0.407, loss: 1.068, lr: 0.02083767451552407\n",
      "epoch: 4800, acc: 0.407, loss: 1.068, lr: 0.020412329046744233\n",
      "epoch: 4900, acc: 0.407, loss: 1.068, lr: 0.020004000800160033\n",
      "epoch: 5000, acc: 0.407, loss: 1.068, lr: 0.019611688566385566\n",
      "epoch: 5100, acc: 0.407, loss: 1.068, lr: 0.019234468166955183\n",
      "epoch: 5200, acc: 0.407, loss: 1.068, lr: 0.018871485185884128\n",
      "epoch: 5300, acc: 0.407, loss: 1.068, lr: 0.018521948508983144\n",
      "epoch: 5400, acc: 0.407, loss: 1.068, lr: 0.01818512456810329\n",
      "epoch: 5500, acc: 0.407, loss: 1.068, lr: 0.01786033220217896\n",
      "epoch: 5600, acc: 0.407, loss: 1.068, lr: 0.01754693805930865\n",
      "epoch: 5700, acc: 0.407, loss: 1.068, lr: 0.01724435247456458\n",
      "epoch: 5800, acc: 0.407, loss: 1.068, lr: 0.016952025767079167\n",
      "epoch: 5900, acc: 0.407, loss: 1.068, lr: 0.01666944490748458\n",
      "epoch: 6000, acc: 0.407, loss: 1.068, lr: 0.016396130513198885\n",
      "epoch: 6100, acc: 0.407, loss: 1.068, lr: 0.016131634134537828\n",
      "epoch: 6200, acc: 0.407, loss: 1.068, lr: 0.015875535799333228\n",
      "epoch: 6300, acc: 0.407, loss: 1.068, lr: 0.01562744178777934\n",
      "epoch: 6400, acc: 0.407, loss: 1.068, lr: 0.015386982612709646\n",
      "epoch: 6500, acc: 0.407, loss: 1.068, lr: 0.015153811183512654\n",
      "epoch: 6600, acc: 0.407, loss: 1.068, lr: 0.014927601134497688\n",
      "epoch: 6700, acc: 0.410, loss: 1.068, lr: 0.014708045300779527\n",
      "epoch: 6800, acc: 0.410, loss: 1.068, lr: 0.014494854326714018\n",
      "epoch: 6900, acc: 0.410, loss: 1.068, lr: 0.014287755393627663\n",
      "epoch: 7000, acc: 0.410, loss: 1.068, lr: 0.014086491055078181\n",
      "epoch: 7100, acc: 0.410, loss: 1.068, lr: 0.013890818169190166\n",
      "epoch: 7200, acc: 0.410, loss: 1.068, lr: 0.013700506918755994\n",
      "epoch: 7300, acc: 0.410, loss: 1.068, lr: 0.013515339910798757\n",
      "epoch: 7400, acc: 0.410, loss: 1.068, lr: 0.013335111348179758\n",
      "epoch: 7500, acc: 0.410, loss: 1.068, lr: 0.013159626266614028\n",
      "epoch: 7600, acc: 0.410, loss: 1.068, lr: 0.012988699831146902\n",
      "epoch: 7700, acc: 0.410, loss: 1.068, lr: 0.012822156686754713\n",
      "epoch: 7800, acc: 0.410, loss: 1.068, lr: 0.0126598303582732\n",
      "epoch: 7900, acc: 0.410, loss: 1.068, lr: 0.012501562695336917\n",
      "epoch: 8000, acc: 0.410, loss: 1.068, lr: 0.012347203358439314\n",
      "epoch: 8100, acc: 0.410, loss: 1.068, lr: 0.012196609342602758\n",
      "epoch: 8200, acc: 0.410, loss: 1.068, lr: 0.012049644535486204\n",
      "epoch: 8300, acc: 0.410, loss: 1.068, lr: 0.011906179307060364\n",
      "epoch: 8400, acc: 0.410, loss: 1.068, lr: 0.011766090128250382\n",
      "epoch: 8500, acc: 0.410, loss: 1.068, lr: 0.01162925921618793\n",
      "epoch: 8600, acc: 0.410, loss: 1.068, lr: 0.011495574203931488\n",
      "epoch: 8700, acc: 0.410, loss: 1.068, lr: 0.011364927832708264\n",
      "epoch: 8800, acc: 0.410, loss: 1.068, lr: 0.011237217664906169\n",
      "epoch: 8900, acc: 0.410, loss: 1.068, lr: 0.011112345816201801\n",
      "epoch: 9000, acc: 0.410, loss: 1.068, lr: 0.010990218705352238\n",
      "epoch: 9100, acc: 0.410, loss: 1.068, lr: 0.010870746820306556\n",
      "epoch: 9200, acc: 0.410, loss: 1.068, lr: 0.010753844499408539\n",
      "epoch: 9300, acc: 0.410, loss: 1.068, lr: 0.010639429726566656\n",
      "epoch: 9400, acc: 0.410, loss: 1.068, lr: 0.010527423939362039\n",
      "epoch: 9500, acc: 0.410, loss: 1.068, lr: 0.010417751849150954\n",
      "epoch: 9600, acc: 0.413, loss: 1.068, lr: 0.010310341272296112\n",
      "epoch: 9700, acc: 0.413, loss: 1.068, lr: 0.010205122971731808\n",
      "epoch: 9800, acc: 0.413, loss: 1.068, lr: 0.010102030508132133\n",
      "epoch: 9900, acc: 0.413, loss: 1.068, lr: 0.01000100010001\n",
      "epoch: 10000, acc: 0.413, loss: 1.068, lr: 0.009901970492127933\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD7CAYAAACMlyg3AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAAsTAAALEwEAmpwYAAAd4klEQVR4nO3df5Bd5X3f8ff3/txfWq20u2AhCRCDHAM2VeQFRBxirNiJwB6LtDRGsQuldpU0dtu4tWszmQ7p1P4DxxMcGg+YxhiTcQQxjWvsAYvYoYFOgHoJshAGwRpstLJAKwlptT/vj/32j/Pc1dWyv7S6u3e1z+c1c+ee8zznXD1nj7QfPT/uvebuiIhInFL1boCIiNSPQkBEJGIKARGRiCkEREQiphAQEYmYQkBEJGIzhoCZ3WNmB81szxT17zCzJ81s1Mw+M6Fui5ntNbMeM/t8Vfk6M3s6lD9gZrnTvxQRETlVs+kJ3Atsmab+CPAfgC9XF5pZGvgqcA1wMbDNzC4O1bcBt7v7hcCbwMdPrdkiIlILmZkOcPfHzez8aeoPAgfN7IMTqi4Hetz9FQAzux/YamYvAJuB3wvHfRP4E+DOmdrS0dHh558/ZVNERGQSzzzzzCF375ysbsYQOA2rgX1V+73AFUA7cNTdS1Xlq6d6ETPbDmwHOPfcc+nu7p6f1oqILFFm9oup6hb9xLC73+3uXe7e1dk5aZCJiMgczWcI7AfWVu2vCWWHgTYzy0woFxGRBTafIfBjYH1YCZQDbgAe8uQT6x4Drg/H3QR8dx7bISIiU5hxTsDMdgBXAx1m1gvcCmQB3P0uM3sb0A20AmNm9kfAxe7eb2afAnYCaeAed38+vOzngPvN7AvAs8DXa3pVIiIyK7NZHbRthvrXSYZ0Jqt7GHh4kvJXSFYPiYhIHS36iWEREZk/CgERkYhFEQLf3/1LvvX0lMtkRUSiFUUIPPLc63x5514KpbF6N0VEZFGJIgT+xbtX8+ZQkf/b01fvpoiILCpRhMCvX9hJa0OGR59/o95NERFZVKIIgVwmxcbzVrBr39F6N0VEZFGJIgQA3rV6OS+9cZyRYrneTRERWTSiCYF1Hc2MOew/OlzvpoiILBrRhMB57U0AvHZ4qM4tERFZPKIJgbUrQwgcUQiIiFREEwLtzXlSBn3HR+vdFBGRRSOaEEinjJXNeQ4PKgRERCqiCQGAjpYcfccL9W6GiMiiEVkIqCcgIlItshDIcWhAISAiUhFVCCxvzHJsqFjvZoiILBpRhcCyhiwDoyWSrzkWEZHIQiDDmMNgQR8dISICkYVAa2MWgOMjGhISEYHIQmBZQwaA4yOlOrdERGRxiCwE1BMQEakWWQgkPYH+YfUERERgFiFgZveY2UEz2zNFvZnZHWbWY2a7zWxjVd1tZrYnPD5SVX6vmb1qZrvCY0NNrmYGrZUQUE9ARASYXU/gXmDLNPXXAOvDYztwJ4CZfRDYCGwArgA+Y2atVed91t03hMeuU275HJwYDlJPQEQEZhEC7v44cGSaQ7YC93niKaDNzFYBFwOPu3vJ3QeB3UwfJvOuMZcGYFhLREVEgNrMCawG9lXt94aynwBbzKzJzDqA9wFrq477Yhg+ut3M8lO9uJltN7NuM+vu6+s7rYY2ZkMI6CsmRUSAeZwYdvdHgYeBfwR2AE8Cld++twDvAC4DVgKfm+Z17nb3Lnfv6uzsPK02ZdMpsmljSD0BERGgNiGwn5P/h78mlOHuXwxj/h8ADHgplB8Iw0ejwDeAy2vQjllpzKb1ZfMiIkEtQuAh4MawSmgTcMzdD5hZ2szaAczsUuBS4NGwvyo8G3AdMOnKo/nQmEszVNDEsIgIQGamA8xsB3A10GFmvcCtQBbA3e8iGfK5FugBhoCbw6lZ4Ink9zz9wMfcvfLb91tm1knSO9gF/EFtLmdmTbkMw8WxhfrjREQWtRlDwN23zVDvwCcnKR8hWSE02TmbZ9vAWmvMphlWT0BEBIjsHcOQDAdpdZCISCK6EGjKpbU6SEQkiC4EGrJpvVlMRCSILgSaNBwkIjIuuhBozGo4SESkIr4QyKUZUQiIiAARhkBTLs2QhoNERIAIQyCfSVMec0plvWFMRCTCEEguebSkEBARUQiIiEQsvhAI3ykwWtK8gIhIfCFQ6QnoQ+RERGIMgUpPQCEgIhJhCFTmBDQcJCISXwhkNTEsIlIRXwhUhoM0JyAiEmMIaDhIRKQivhDQcJCIyLj4QiCj9wmIiFREGAJ6n4CISEW8IaDhIBGRCEMgfGxEQSEgIhJhCGh1kIjIuBlDwMzuMbODZrZninozszvMrMfMdpvZxqq628xsT3h8pKp8nZk9Hc55wMxytbmcmWVSRso0HCQiArPrCdwLbJmm/hpgfXhsB+4EMLMPAhuBDcAVwGfMrDWccxtwu7tfCLwJfHwObZ8TMyOfSSsERESYRQi4++PAkWkO2Qrc54mngDYzWwVcDDzu7iV3HwR2A1vMzIDNwIPh/G8C153GNZyyfDbFqL5iUkSkJnMCq4F9Vfu9oewnJL/0m8ysA3gfsBZoB466e2nC8ZMys+1m1m1m3X19fTVobjIvoJ6AiMg8Tgy7+6PAw8A/AjuAJ4FT/u+3u9/t7l3u3tXZ2VmTtmk4SEQkUYsQ2E/yP/yKNaEMd/+iu29w9w8ABrwEHCYZMspMPH6hJD0BDQeJiNQiBB4CbgyrhDYBx9z9gJmlzawdwMwuBS4FHnV3Bx4Drg/n3wR8twbtmLVkTkA9ARGRzEwHmNkO4Gqgw8x6gVuBLIC730Uy5HMt0AMMATeHU7PAE8k8MP3Ax6rmAT4H3G9mXwCeBb5eo+uZFQ0HiYgkZgwBd982Q70Dn5ykfIRkhdBk57wCXD7LNtachoNERBLRvWMYkhAY0XCQiEisIZDWZweJiBBrCGQ1HCQiApGGQC6tN4uJiECkIZD0BBQCIiJxhkAmrc8OEhEh2hBIUSirJyAiEmkIpCmWnfKY17spIiJ1FWcIZJPL1jJREYldlCGQS+srJkVEINIQqPQEtEJIRGIXZwhk0gD6JFERiV6kIRDmBMoaDhKRuEUdAvoQORGJXZwhkA3DQZoTEJHIRRkCWh0kIpKIMgS0OkhEJBFnCIQ5Aa0OEpHYRRoCyZyAPj9IRGIXaQhUegKaExCRuMUZApoTEBEBYg2BtJaIiohArCGQ1RJRERGYRQiY2T1mdtDM9kxRb2Z2h5n1mNluM9tYVfclM3vezF4Ix1go/z9mttfMdoXHWbW7pJmNv09Aq4NEJHKz6QncC2yZpv4aYH14bAfuBDCzXwPeA1wKvBO4DHhv1XkfdfcN4XHw1Js+d6mUkUvr28VERGYMAXd/HDgyzSFbgfs88RTQZmarAAcagByQB7LAG6ff5NrIZ1LqCYhI9GoxJ7Aa2Fe13wusdvcngceAA+Gx091fqDruG2Eo6L9WhokmY2bbzazbzLr7+vpq0NxEPpvSnICIRG/eJobN7ELgImANSVBsNrOrQvVH3f1dwFXh8a+meh13v9vdu9y9q7Ozs2bty6VTWh0kItGrRQjsB9ZW7a8JZb8DPOXuA+4+ADwCXAng7vvD83Hgr4HLa9COU5LPphUCIhK9WoTAQ8CNYZXQJuCYux8AXgPea2YZM8uSTAq/EPY7AEL5h4BJVx7Np2ROQMNBIhK3zEwHmNkO4Gqgw8x6gVtJJnlx97uAh4FrgR5gCLg5nPogsBl4jmSS+Afu/j0zawZ2hgBIAz8E/mcNr2lW8hmtDhIRmTEE3H3bDPUOfHKS8jLw+5OUDwLvPoU2zot8Jq3VQSISvSjfMQxaHSQiAhGHgFYHiYhEHAJJT0AhICJxizcEMmkNB4lI9CIOgRQF9QREJHJRh4CGg0QkdvGGQFZLREVEog2BZHVQmeRtDiIicYo2BPKZFGMOpTGFgIjEK94Q0JfNi4hEHAKZ8GXz+hA5EYlYxCGQXLo+RE5EYhZvCGT1ZfMiItGGQC4dhoM0JyAiEYs2BCrDQfroCBGJWbwhoNVBIiIRh8D46iCFgIjEK+IQqKwO0nCQiMQr3hDQ6iARkXhDIJfWnICISLQhkM9WlohqOEhE4hVvCGTUExARiT4ERvTZQSISsVmFgJndY2YHzWzPFPVmZneYWY+Z7TazjVV1XzKz583shXCMhfJ3m9lz4Zzx8oXSGIaDhgvqCYhIvGbbE7gX2DJN/TXA+vDYDtwJYGa/BrwHuBR4J3AZ8N5wzp3Av606b7rXr7lMOkUunWKoWFrIP1ZEZFGZVQi4++PAkWkO2Qrc54mngDYzWwU40ADkgDyQBd4Ida3u/pQnX+11H3Dd3C9jbpryaYYLGg4SkXjVak5gNbCvar8XWO3uTwKPAQfCY6e7vxCO7514/GQvbGbbzazbzLr7+vpq1NxEUzbNkEJARCI2rxPDZnYhcBGwhuSX/GYzu+pUXsPd73b3Lnfv6uzsrGn7GnNphgoaDhKReNUqBPYDa6v214Sy3wGecvcBdx8AHgGuDHVrJjl+QTXlMuoJiEjUahUCDwE3hlVCm4Bj7n4AeA14r5llzCxLMin8QqjrN7NNYVXQjcB3a9SWWWvKaThIROKWmc1BZrYDuBroMLNe4FaSSV7c/S7gYeBaoAcYAm4Opz4IbAaeI5kk/oG7fy/U/SHJqqNGkh7CI6d9NaeoKZfm0EBhof9YEZFFY1Yh4O7bZqh34JOTlJeB35/inG6SZaN105TLMFgYqmcTRETqKtp3DEMyMawloiISs6hDoFlzAiISuahDoDGXUU9ARKIWdQg05dIUymMUy/r8IBGJU/QhAGhISESiFXUINOYqnySqEBCROEUdAs25ZIWsPjpCRGIVdQg0ajhIRCIXdQhoTkBEYhd1CCxryAIwMFqsc0tEROoj6hBobUjmBPqHNScgInGKOgQqPYH+EfUERCROkYdA0hM4PqKegIjEKeoQaMimyWdS9A+rJyAicYo6BCAZEtJwkIjEKvoQaG3M0K/hIBGJlEKgIavhIBGJVvQhsKxBPQERiVf0IdDamOW45gREJFIKgYas3iwmItFSCDRm6B8p4u71boqIyIKLPgRWNuUolMYY1IfIiUiEog+B9pY8AEcGCnVuiYjIwpsxBMzsHjM7aGZ7pqg3M7vDzHrMbLeZbQzl7zOzXVWPETO7LtTda2avVtVtqOVFnYr2lhwAhwZH69UEEZG6yczimHuBvwDum6L+GmB9eFwB3Alc4e6PARsAzGwl0AM8WnXeZ939wTm1uoY6mpOewGH1BEQkQjP2BNz9ceDINIdsBe7zxFNAm5mtmnDM9cAj7j4096bOj5WhJ3B4QD0BEYlPLeYEVgP7qvZ7Q1m1G4AdE8q+GIaPbjez/FQvbmbbzazbzLr7+vpq0NyTtTeHEBhUT0BE4jPvE8OhV/AuYGdV8S3AO4DLgJXA56Y6393vdvcud+/q7Oysefsasmla8hkOqScgIhGqRQjsB9ZW7a8JZRW/C3zH3cffluvuB8Lw0SjwDeDyGrRjztpbchxRT0BEIlSLEHgIuDGsEtoEHHP3A1X125gwFFSZMzAzA64DJl15tFDam3P0HVdPQETiM+PqIDPbAVwNdJhZL3ArkAVw97uAh4FrSVb/DAE3V517Pkkv4R8mvOy3zKwTMGAX8AendxmnZ1VbI8/vP1bPJoiI1MWMIeDu22aod+CTU9T9nLdOEuPum2fZvgWxpq2Rv3v+DcbGnFTK6t0cEZEFE/07hgFWr2ikUB7T5LCIREchAKxuawSg9+hwnVsiIrKwFALAOSEE9r+pEBCRuCgESIaDAParJyAikVEIkHyxTHtzjlf7BuvdFBGRBaUQCC48q4WXDx6vdzNERBaUQiC48KwWeg4O6BvGRCQqCoFg/Vkt9I+U9M5hEYmKQiBYf/YyAF56Y6DOLRERWTgKgeCSc1oB+Env0fo2RERkASkEgramHBd0NvPsa2/WuykiIgtGIVBl47kr+KfXjmpyWESioRCo0nXeCo4MFug5qHkBEYmDQqDKb7w9+eayv3/xYJ1bIiKyMBQCVc5pa+SiVa0KARGJhkJggvdfdBY//vkR3ugfqXdTRETmnUJggn++cQ1jDg8+01vvpoiIzDuFwATrOpq5fN1K/qZ7H2NjWiUkIkubQmASH9t0Hr84PMQPnn+93k0REZlXCoFJfPBdq7igo5k7fvSyegMisqQpBCaRThmf2nwhL75+nEf2qDcgIkuXQmAKH/5n53DRqlZufWiPvoBeRJYshcAUMukUX/nIBvpHSnz6gV2UymP1bpKISM3NGAJmdo+ZHTSzPVPUm5ndYWY9ZrbbzDaG8veZ2a6qx4iZXRfq1pnZ0+GcB8wsV9OrqpFfedsy/vvWS3ji5UN89sHdFEoKAhFZWmbTE7gX2DJN/TXA+vDYDtwJ4O6PufsGd98AbAaGgEfDObcBt7v7hcCbwMfn0viF8JHLzuU/f+DtfOfZ/fzLrz3JT3/ZX+8miYjUzIwh4O6PA0emOWQrcJ8nngLazGzVhGOuBx5x9yEzM5JQeDDUfRO47pRbvoD+/W+u56u/t5F9R4b40P94gs98+ycKAxFZEjI1eI3VwL6q/d5QdqCq7Abgz8J2O3DU3UsTjp+UmW0n6WFw7rnn1qC5c/PBS1fxngvb+coPX+aBH+/jwWd6eefqVq555yquWt/BJecsJ52yurVPRGQuahEC0wq9gncBO+dyvrvfDdwN0NXVVddF+21NOf7kw5fw6fe/nW8/s4/v7T7An+7cy5/u3MuyhgzvPm8Fl5zTyiXnLOeiVa2sXdFIJq25dxFZvGoRAvuBtVX7a0JZxe8C33H3Ytg/TDJklAm9gYnHL3rLm7J84qoL+MRVF3Cwf4QnXznMkz87zK59R3ni5UOUwxvMMinj3JVNrOto5tz2Js5Z3sjbljdwTlsDb1veyFnL8mQVEiJSR7UIgYeAT5nZ/cAVwDF3rx4K2gbcUtlxdzezx0jmCe4HbgK+W4N21MVZrQ1s3bCarRuSEa2RYpmX3jjOiweO8+rhQX5+aJBXDw3y5CuHGSqUTzo3ZdC5LM/bljdy9rI87S15OlpytDfnaG/J096So6MlT3tzjramnIabRKTmZgwBM9sBXA10mFkvcCuQBXD3u4CHgWuBHpIVQDdXnXs+SS/hHya87OeA+83sC8CzwNdP8zoWjYZsmkvXtHHpmraTyt2d/pESrx8b4ZfHhnn92AgHjg5z4NgIB46N8IvDQ/zTa29yZLDAZJ9UkTJY2ZxjZXOO9uY8K0NYtDXlWNGUZUVTjraq57amHMvyGVIKDhGZhp1J36fb1dXl3d3d9W7GvCqPOUeHChweLHB4oMDhwdHkeWCUQ4PJ85HxugL9I0WmuoXplLG8MUtbY5blTclzW1MuKWvKsrwx+5bt1sYsrQ1ZGrLphb1wEZk3ZvaMu3dNVjfvE8NyatIpC0NBeTh75uPLY86x4SJvDhU4OlTg6FCRN4eK49tHh8PzUJFDAwV6+gY4Oljk+Ghp2tfNZVK0NmRobciyrCHDsvBcvd/SkGFZPsOyhgwtDRlawvayhiwt+QxNuTTJimARWawUAme4dMrGh4lORak8Rv9IiWPDxfHH0aEC/SMl+oeL9I8UOR62j4+UOD5S5PX+EY6H8onzG5NJGTTnkoBoziePlnyalvHtqudcevyYynFNuQzNuQxN+TRNuTSNWYWKSK0pBCKVSafmFB4VpfIYg6Pl8bAYLJQYGClxfDR5HhhNygfC/mChlBw3WuLQ8QIDo0nZ4GiJYnl2Q5Jm0JgN4ZBPQqE59DiacmmacxkacyeXNeUy48/N+ZPLGsN2YzatSXeJlkJA5iSTTrG8KcXypuxpv9ZoqczgaJnBqmAYGC0zXCgxOFpmqJD0PAYLZYZGSwwVk+fBQpnhQpmB0RIH+0cZKpYYGi0zWCgxUjy1z3nKpVM0ZFM0hh5HQzY9vt2YTdMwXp4arz/xOFHWmE2Tf8sxKRoyyevlMyn1ZmRRUQhI3eUzafKZ9Jx7JZMpjznDISySAEmehwpVAVIsMxLKRkpJoIwUk/LhSn0x6e0Mh8AZKY2F5/KUE/IzX2/qRDhk0zRkku18JTQyYTscl5/w3JBNJT+zqueGCc/V5+QzabJpU/jIpBQCsiSlU0ZLmG+YD+7OaGmM0eLYiQAZD5IxRkKAjJSq98cYLpYZrdSFc8e3i2WODRc5WFU/WiozWkrqTudL7syS8Mmlk4BJwqE6TMJ2CKDkuInlJ7ZzmZPrKvu5UJZ7S1nyZyuIFh+FgMgcmNn4cM9yTn9IbCbuTmnMJw2HkeIYhVJSVl03WgzP448yo8Wq7RBile2jw0VGi+XwWieOHymVZz1vM5NcJkU+PcvQyJwIo/FQCufmQqjkqs7NpU+cf6J+6jp9pEtCISByBjAzsmkjm06xrGHh//yxMadQPhEOhdLYePgUyidCqBIghdKJstHqsnISPIVyOTyPnXROMj9Uqtqv1CWvUyiPzXkYbqKUJaGUTZ/oqeQmCZFsdYCcFCwhgNI2YzBl3xJeJ79mtvJch2E7hYCIzCiVMhpS6fAmwvnv+Uyl0iMqlE4Oj0K5fFL4TAyXyv6U29VlE/b7iyUKpTGK07xGrZiRhM6EQMqmU3z9pi7Oa2+u2Z9VoRAQkTNGdY+oOV/v1iTck15SsewTguTkYCqWnUK5PGU4VUJmdIpAmq938SsEREROg5mFCXJgkQTTqdDMiIhIxBQCIiIRUwiIiERMISAiEjGFgIhIxBQCIiIRUwiIiERMISAiErEz6juGzawP+MUcT+8ADtWwOWcCXXMcdM1L3+le73nu3jlZxRkVAqfDzLqn+qLlpUrXHAdd89I3n9er4SARkYgpBEREIhZTCNxd7wbUga45DrrmpW/erjeaOQEREXmrmHoCIiIygUJARCRiUYSAmW0xs71m1mNmn693e+bKzNaa2WNm9lMze97M/mMoX2lmf2dmL4fnFaHczOyOcN27zWxj1WvdFI5/2cxuqtc1zZaZpc3sWTP7fthfZ2ZPh2t7wMxyoTwf9ntC/flVr3FLKN9rZr9dp0uZFTNrM7MHzexFM3vBzK5c6vfZzD4d/l7vMbMdZtaw1O6zmd1jZgfNbE9VWc3uq5m928yeC+fcYbP5wmJ3X9IPIA38DLgAyAE/AS6ud7vmeC2rgI1hexnwEnAx8CXg86H888BtYfta4BHAgE3A06F8JfBKeF4RtlfU+/pmuPb/BPw18P2w/zfADWH7LuDfhe0/BO4K2zcAD4Tti8O9zwPrwt+JdL2va5rr/SbwibCdA9qW8n0GVgOvAo1V9/dfL7X7DPwGsBHYU1VWs/sK/L9wrIVzr5mxTfX+oSzAD/1KYGfV/i3ALfVuV42u7bvAB4C9wKpQtgrYG7a/BmyrOn5vqN8GfK2q/KTjFtsDWAP8CNgMfD/8BT8EZCbeY2AncGXYzoTjbOJ9rz5usT2A5eEXok0oX7L3OYTAvvCLLRPu828vxfsMnD8hBGpyX0Pdi1XlJx031SOG4aDKX66K3lB2Rgvd318FngbOdvcDoep14OywPdW1n2k/k68A/wUYC/vtwFF3L4X96vaPX1uoPxaOP5OueR3QB3wjDIH9pZk1s4Tvs7vvB74MvAYcILlvz7C073NFre7r6rA9sXxaMYTAkmNmLcD/Av7I3fur6zz5L8CSWfdrZh8CDrr7M/VuywLKkAwZ3OnuvwoMkgwTjFuC93kFsJUkAM8BmoEtdW1UHdTjvsYQAvuBtVX7a0LZGcnMsiQB8C13/9tQ/IaZrQr1q4CDoXyqaz+TfibvAT5sZj8H7icZEvpzoM3MMuGY6vaPX1uoXw4c5sy65l6g192fDvsPkoTCUr7P7wdedfc+dy8Cf0ty75fyfa6o1X3dH7Ynlk8rhhD4MbA+rDLIkUwiPVTnNs1JmOn/OvCCu/9ZVdVDQGWFwE0kcwWV8hvDKoNNwLHQ7dwJ/JaZrQj/A/utULbouPst7r7G3c8nuXd/7+4fBR4Drg+HTbzmys/i+nC8h/IbwqqSdcB6kkm0RcfdXwf2mdmvhKLfBH7KEr7PJMNAm8ysKfw9r1zzkr3PVWpyX0Ndv5ltCj/DG6tea2r1niRZoImYa0lW0vwM+ON6t+c0ruPXSbqKu4Fd4XEtyVjoj4CXgR8CK8PxBnw1XPdzQFfVa/0boCc8bq73tc3y+q/mxOqgC0j+cfcA3wbyobwh7PeE+guqzv/j8LPYyyxWTdT5WjcA3eFe/2+SVSBL+j4D/w14EdgD/BXJCp8ldZ+BHSRzHkWSHt/Ha3lfga7w8/sZ8BdMWFww2UMfGyEiErEYhoNERGQKCgERkYgpBEREIqYQEBGJmEJARCRiCgERkYgpBEREIvb/ASq6xptOmgbLAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "dense1 = Layer_Dense(2,64)\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = Layer_Dense(64,3)\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "optimizer = Optimizer_SGD(decay = 1e-2)\n",
    "l = []\n",
    "ep = []\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "    # Forward pass\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "    loss = loss_activation.forward(dense2.output,y)\n",
    "\n",
    "    predictions = np.argmax(loss_activation.output, axis = 1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis = 1)\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    ep.append(epoch)\n",
    "    l.append(loss)\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' + \n",
    "              f'acc: {accuracy:.3f}, ' + \n",
    "              f'loss: {loss:.3f}, ' + \n",
    "              f'lr: {optimizer.current_learning_rate}')\n",
    "    \n",
    "    # Backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    \n",
    "    # Update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()\n",
    "plt.plot(ep,l)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model definitely got stuck, and the reason is almost certainly because the learning rate \n",
    "decayed far too quickly and became too small, trapping the model in some local minimum. This is \n",
    "most likely why, rather than wiggling, our accuracy and loss stopped changing at all.\n",
    "We can, instead, try to decay a bit slower by making our decay a smaller number. For example, \n",
    "let’s go with 1e-3 (0.001):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.390, loss: 1.099, lr: 1.0\n",
      "epoch: 100, acc: 0.393, loss: 1.083, lr: 0.9099181073703367\n",
      "epoch: 200, acc: 0.400, loss: 1.071, lr: 0.8340283569641367\n",
      "epoch: 300, acc: 0.407, loss: 1.070, lr: 0.7698229407236336\n",
      "epoch: 400, acc: 0.407, loss: 1.069, lr: 0.7147962830593281\n",
      "epoch: 500, acc: 0.400, loss: 1.068, lr: 0.66711140760507\n",
      "epoch: 600, acc: 0.407, loss: 1.067, lr: 0.6253908692933083\n",
      "epoch: 700, acc: 0.417, loss: 1.066, lr: 0.5885815185403178\n",
      "epoch: 800, acc: 0.420, loss: 1.065, lr: 0.5558643690939411\n",
      "epoch: 900, acc: 0.417, loss: 1.063, lr: 0.526592943654555\n",
      "epoch: 1000, acc: 0.423, loss: 1.061, lr: 0.5002501250625312\n",
      "epoch: 1100, acc: 0.437, loss: 1.057, lr: 0.4764173415912339\n",
      "epoch: 1200, acc: 0.443, loss: 1.053, lr: 0.45475216007276037\n",
      "epoch: 1300, acc: 0.450, loss: 1.048, lr: 0.43497172683775553\n",
      "epoch: 1400, acc: 0.480, loss: 1.041, lr: 0.4168403501458941\n",
      "epoch: 1500, acc: 0.503, loss: 1.035, lr: 0.4001600640256102\n",
      "epoch: 1600, acc: 0.517, loss: 1.027, lr: 0.3847633705271258\n",
      "epoch: 1700, acc: 0.537, loss: 1.020, lr: 0.3705075954057058\n",
      "epoch: 1800, acc: 0.553, loss: 1.014, lr: 0.35727045373347627\n",
      "epoch: 1900, acc: 0.550, loss: 1.007, lr: 0.3449465332873405\n",
      "epoch: 2000, acc: 0.543, loss: 1.001, lr: 0.33344448149383127\n",
      "epoch: 2100, acc: 0.547, loss: 0.994, lr: 0.32268473701193934\n",
      "epoch: 2200, acc: 0.543, loss: 0.987, lr: 0.31259768677711786\n",
      "epoch: 2300, acc: 0.540, loss: 0.981, lr: 0.3031221582297666\n",
      "epoch: 2400, acc: 0.530, loss: 0.976, lr: 0.29420417769932333\n",
      "epoch: 2500, acc: 0.533, loss: 0.971, lr: 0.2857959416976279\n",
      "epoch: 2600, acc: 0.540, loss: 0.967, lr: 0.2778549597110308\n",
      "epoch: 2700, acc: 0.503, loss: 0.972, lr: 0.2703433360367667\n",
      "epoch: 2800, acc: 0.500, loss: 0.972, lr: 0.26322716504343247\n",
      "epoch: 2900, acc: 0.497, loss: 0.970, lr: 0.25647601949217746\n",
      "epoch: 3000, acc: 0.497, loss: 0.966, lr: 0.25006251562890724\n",
      "epoch: 3100, acc: 0.497, loss: 0.964, lr: 0.2439619419370578\n",
      "epoch: 3200, acc: 0.510, loss: 0.961, lr: 0.23815194093831865\n",
      "epoch: 3300, acc: 0.513, loss: 0.958, lr: 0.23261223540358225\n",
      "epoch: 3400, acc: 0.513, loss: 0.956, lr: 0.22732439190725165\n",
      "epoch: 3500, acc: 0.523, loss: 0.950, lr: 0.22227161591464767\n",
      "epoch: 3600, acc: 0.520, loss: 0.951, lr: 0.21743857360295715\n",
      "epoch: 3700, acc: 0.523, loss: 0.948, lr: 0.21281123643328367\n",
      "epoch: 3800, acc: 0.527, loss: 0.945, lr: 0.20837674515524068\n",
      "epoch: 3900, acc: 0.533, loss: 0.941, lr: 0.20412329046744235\n",
      "epoch: 4000, acc: 0.533, loss: 0.939, lr: 0.2000400080016003\n",
      "epoch: 4100, acc: 0.533, loss: 0.936, lr: 0.19611688566385566\n",
      "epoch: 4200, acc: 0.537, loss: 0.932, lr: 0.19234468166955185\n",
      "epoch: 4300, acc: 0.533, loss: 0.930, lr: 0.18871485185884126\n",
      "epoch: 4400, acc: 0.533, loss: 0.927, lr: 0.18521948508983144\n",
      "epoch: 4500, acc: 0.537, loss: 0.925, lr: 0.18185124568103292\n",
      "epoch: 4600, acc: 0.543, loss: 0.922, lr: 0.1786033220217896\n",
      "epoch: 4700, acc: 0.547, loss: 0.918, lr: 0.1754693805930865\n",
      "epoch: 4800, acc: 0.553, loss: 0.914, lr: 0.17244352474564578\n",
      "epoch: 4900, acc: 0.547, loss: 0.911, lr: 0.16952025767079165\n",
      "epoch: 5000, acc: 0.553, loss: 0.909, lr: 0.16669444907484582\n",
      "epoch: 5100, acc: 0.553, loss: 0.905, lr: 0.16396130513198884\n",
      "epoch: 5200, acc: 0.557, loss: 0.903, lr: 0.16131634134537828\n",
      "epoch: 5300, acc: 0.553, loss: 0.899, lr: 0.15875535799333226\n",
      "epoch: 5400, acc: 0.557, loss: 0.896, lr: 0.1562744178777934\n",
      "epoch: 5500, acc: 0.560, loss: 0.893, lr: 0.15386982612709646\n",
      "epoch: 5600, acc: 0.570, loss: 0.890, lr: 0.15153811183512653\n",
      "epoch: 5700, acc: 0.560, loss: 0.889, lr: 0.14927601134497687\n",
      "epoch: 5800, acc: 0.567, loss: 0.886, lr: 0.14708045300779526\n",
      "epoch: 5900, acc: 0.567, loss: 0.882, lr: 0.14494854326714016\n",
      "epoch: 6000, acc: 0.573, loss: 0.878, lr: 0.1428775539362766\n",
      "epoch: 6100, acc: 0.570, loss: 0.874, lr: 0.1408649105507818\n",
      "epoch: 6200, acc: 0.573, loss: 0.867, lr: 0.13890818169190167\n",
      "epoch: 6300, acc: 0.583, loss: 0.855, lr: 0.13700506918755992\n",
      "epoch: 6400, acc: 0.587, loss: 0.853, lr: 0.13515339910798757\n",
      "epoch: 6500, acc: 0.587, loss: 0.849, lr: 0.13335111348179757\n",
      "epoch: 6600, acc: 0.583, loss: 0.845, lr: 0.13159626266614027\n",
      "epoch: 6700, acc: 0.587, loss: 0.842, lr: 0.12988699831146902\n",
      "epoch: 6800, acc: 0.590, loss: 0.839, lr: 0.12822156686754713\n",
      "epoch: 6900, acc: 0.593, loss: 0.836, lr: 0.126598303582732\n",
      "epoch: 7000, acc: 0.593, loss: 0.832, lr: 0.12501562695336915\n",
      "epoch: 7100, acc: 0.600, loss: 0.828, lr: 0.12347203358439313\n",
      "epoch: 7200, acc: 0.603, loss: 0.822, lr: 0.12196609342602757\n",
      "epoch: 7300, acc: 0.610, loss: 0.818, lr: 0.12049644535486204\n",
      "epoch: 7400, acc: 0.617, loss: 0.814, lr: 0.11906179307060363\n",
      "epoch: 7500, acc: 0.613, loss: 0.812, lr: 0.11766090128250381\n",
      "epoch: 7600, acc: 0.617, loss: 0.808, lr: 0.11629259216187929\n",
      "epoch: 7700, acc: 0.620, loss: 0.805, lr: 0.11495574203931487\n",
      "epoch: 7800, acc: 0.617, loss: 0.801, lr: 0.11364927832708263\n",
      "epoch: 7900, acc: 0.617, loss: 0.798, lr: 0.11237217664906168\n",
      "epoch: 8000, acc: 0.617, loss: 0.794, lr: 0.11112345816201799\n",
      "epoch: 8100, acc: 0.617, loss: 0.791, lr: 0.10990218705352237\n",
      "epoch: 8200, acc: 0.623, loss: 0.786, lr: 0.10870746820306555\n",
      "epoch: 8300, acc: 0.633, loss: 0.782, lr: 0.1075384449940854\n",
      "epoch: 8400, acc: 0.630, loss: 0.779, lr: 0.10639429726566654\n",
      "epoch: 8500, acc: 0.643, loss: 0.775, lr: 0.10527423939362038\n",
      "epoch: 8600, acc: 0.647, loss: 0.774, lr: 0.10417751849150952\n",
      "epoch: 8700, acc: 0.647, loss: 0.771, lr: 0.10310341272296113\n",
      "epoch: 8800, acc: 0.650, loss: 0.768, lr: 0.1020512297173181\n",
      "epoch: 8900, acc: 0.650, loss: 0.764, lr: 0.10102030508132134\n",
      "epoch: 9000, acc: 0.650, loss: 0.760, lr: 0.1000100010001\n",
      "epoch: 9100, acc: 0.653, loss: 0.756, lr: 0.09901970492127933\n",
      "epoch: 9200, acc: 0.657, loss: 0.753, lr: 0.09804882831650162\n",
      "epoch: 9300, acc: 0.660, loss: 0.749, lr: 0.09709680551509856\n",
      "epoch: 9400, acc: 0.663, loss: 0.746, lr: 0.09616309260505818\n",
      "epoch: 9500, acc: 0.663, loss: 0.743, lr: 0.09524716639679968\n",
      "epoch: 9600, acc: 0.667, loss: 0.740, lr: 0.09434852344560807\n",
      "epoch: 9700, acc: 0.670, loss: 0.737, lr: 0.09346667912889055\n",
      "epoch: 9800, acc: 0.670, loss: 0.733, lr: 0.09260116677470137\n",
      "epoch: 9900, acc: 0.673, loss: 0.730, lr: 0.09175153683824203\n",
      "epoch: 10000, acc: 0.673, loss: 0.727, lr: 0.09091735612328393\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "dense1 = Layer_Dense(2,64)\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = Layer_Dense(64,3)\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "optimizer = Optimizer_SGD(decay = 1e-3)\n",
    "l = []\n",
    "ep = []\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "    # Forward pass\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "    loss = loss_activation.forward(dense2.output,y)\n",
    "\n",
    "    predictions = np.argmax(loss_activation.output, axis = 1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis = 1)\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    ep.append(epoch)\n",
    "    l.append(loss)\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' + \n",
    "              f'acc: {accuracy:.3f}, ' + \n",
    "              f'loss: {loss:.3f}, ' + \n",
    "              f'lr: {optimizer.current_learning_rate}')\n",
    "    \n",
    "    # Backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    \n",
    "    # Update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()\n",
    "plt.plot(ep,l)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastic Gradient Descent with Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_SGD:\n",
    "    \n",
    "    def __init__(self, learning_rate = 1.0, decay = 0.0, momentum = 0.0):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self. momentum = momentum\n",
    "        \n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1 / (1 + self.decay * self.iterations))\n",
    "            \n",
    "    def update_params(self, layer):\n",
    "        \n",
    "        # If we use momentum\n",
    "        if self.momentum:\n",
    "        # If layer does not contain momentum arrays, create them filled with zeros\n",
    "            if not hasattr(layer, 'weight_momentums'):\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                # If there is no momentum array for weights the array doesn't exist for biases yet either\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "\n",
    "            # Build weight updates with momentum - take previous\n",
    "            # updates multiplied by retain factor and update with\n",
    "            # current gradient\n",
    "            weight_updates = self.momentum * layer.weight_momentums - self.current_learning_rate * layer.dweights\n",
    "            layer.weight_momentums = weight_updates\n",
    "            \n",
    "            # Build bias updates\n",
    "            bias_updates = self.momentum * layer.bias_momentums - self.current_learning_rate * layer.dbiases\n",
    "            layer.bias_momentums = bias_updates\n",
    "            \n",
    "        # Vanilla SGD updates (as before momentum update)\n",
    "        else:\n",
    "            weight_updates = -self.current_learning_rate * layer.dweights\n",
    "            bias_updates = -self.current_learning_rate * layer.dbiases\n",
    "        \n",
    "        # Update weights and biases using either\n",
    "        # vanilla or momentum updates\n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "        \n",
    "        \n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s show an example illustrating how adding momentum changes the learning process. Keeping \n",
    "the same starting learning rate (1) and decay (1e-3) from the previous training attempt and using \n",
    "a momentum of 0.5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.300, loss: 1.099, lr: 1.0\n",
      "epoch: 100, acc: 0.403, loss: 1.072, lr: 0.9099181073703367\n",
      "epoch: 200, acc: 0.397, loss: 1.068, lr: 0.8340283569641367\n",
      "epoch: 300, acc: 0.400, loss: 1.065, lr: 0.7698229407236336\n",
      "epoch: 400, acc: 0.430, loss: 1.055, lr: 0.7147962830593281\n",
      "epoch: 500, acc: 0.450, loss: 1.038, lr: 0.66711140760507\n",
      "epoch: 600, acc: 0.543, loss: 1.016, lr: 0.6253908692933083\n",
      "epoch: 700, acc: 0.517, loss: 1.012, lr: 0.5885815185403178\n",
      "epoch: 800, acc: 0.503, loss: 1.003, lr: 0.5558643690939411\n",
      "epoch: 900, acc: 0.543, loss: 0.985, lr: 0.526592943654555\n",
      "epoch: 1000, acc: 0.537, loss: 0.969, lr: 0.5002501250625312\n",
      "epoch: 1100, acc: 0.533, loss: 0.954, lr: 0.4764173415912339\n",
      "epoch: 1200, acc: 0.537, loss: 0.944, lr: 0.45475216007276037\n",
      "epoch: 1300, acc: 0.543, loss: 0.933, lr: 0.43497172683775553\n",
      "epoch: 1400, acc: 0.550, loss: 0.922, lr: 0.4168403501458941\n",
      "epoch: 1500, acc: 0.557, loss: 0.912, lr: 0.4001600640256102\n",
      "epoch: 1600, acc: 0.550, loss: 0.905, lr: 0.3847633705271258\n",
      "epoch: 1700, acc: 0.553, loss: 0.915, lr: 0.3705075954057058\n",
      "epoch: 1800, acc: 0.540, loss: 0.908, lr: 0.35727045373347627\n",
      "epoch: 1900, acc: 0.500, loss: 0.886, lr: 0.3449465332873405\n",
      "epoch: 2000, acc: 0.573, loss: 0.887, lr: 0.33344448149383127\n",
      "epoch: 2100, acc: 0.557, loss: 0.893, lr: 0.32268473701193934\n",
      "epoch: 2200, acc: 0.517, loss: 0.880, lr: 0.31259768677711786\n",
      "epoch: 2300, acc: 0.570, loss: 0.859, lr: 0.3031221582297666\n",
      "epoch: 2400, acc: 0.530, loss: 0.865, lr: 0.29420417769932333\n",
      "epoch: 2500, acc: 0.570, loss: 0.863, lr: 0.2857959416976279\n",
      "epoch: 2600, acc: 0.613, loss: 0.857, lr: 0.2778549597110308\n",
      "epoch: 2700, acc: 0.580, loss: 0.839, lr: 0.2703433360367667\n",
      "epoch: 2800, acc: 0.617, loss: 0.825, lr: 0.26322716504343247\n",
      "epoch: 2900, acc: 0.627, loss: 0.827, lr: 0.25647601949217746\n",
      "epoch: 3000, acc: 0.563, loss: 0.825, lr: 0.25006251562890724\n",
      "epoch: 3100, acc: 0.580, loss: 0.803, lr: 0.2439619419370578\n",
      "epoch: 3200, acc: 0.653, loss: 0.822, lr: 0.23815194093831865\n",
      "epoch: 3300, acc: 0.617, loss: 0.788, lr: 0.23261223540358225\n",
      "epoch: 3400, acc: 0.607, loss: 0.779, lr: 0.22732439190725165\n",
      "epoch: 3500, acc: 0.603, loss: 0.775, lr: 0.22227161591464767\n",
      "epoch: 3600, acc: 0.593, loss: 0.777, lr: 0.21743857360295715\n",
      "epoch: 3700, acc: 0.620, loss: 0.783, lr: 0.21281123643328367\n",
      "epoch: 3800, acc: 0.687, loss: 0.765, lr: 0.20837674515524068\n",
      "epoch: 3900, acc: 0.690, loss: 0.754, lr: 0.20412329046744235\n",
      "epoch: 4000, acc: 0.673, loss: 0.743, lr: 0.2000400080016003\n",
      "epoch: 4100, acc: 0.680, loss: 0.737, lr: 0.19611688566385566\n",
      "epoch: 4200, acc: 0.677, loss: 0.731, lr: 0.19234468166955185\n",
      "epoch: 4300, acc: 0.683, loss: 0.724, lr: 0.18871485185884126\n",
      "epoch: 4400, acc: 0.690, loss: 0.718, lr: 0.18521948508983144\n",
      "epoch: 4500, acc: 0.703, loss: 0.711, lr: 0.18185124568103292\n",
      "epoch: 4600, acc: 0.690, loss: 0.720, lr: 0.1786033220217896\n",
      "epoch: 4700, acc: 0.680, loss: 0.697, lr: 0.1754693805930865\n",
      "epoch: 4800, acc: 0.707, loss: 0.702, lr: 0.17244352474564578\n",
      "epoch: 4900, acc: 0.717, loss: 0.689, lr: 0.16952025767079165\n",
      "epoch: 5000, acc: 0.697, loss: 0.683, lr: 0.16669444907484582\n",
      "epoch: 5100, acc: 0.733, loss: 0.668, lr: 0.16396130513198884\n",
      "epoch: 5200, acc: 0.713, loss: 0.668, lr: 0.16131634134537828\n",
      "epoch: 5300, acc: 0.713, loss: 0.676, lr: 0.15875535799333226\n",
      "epoch: 5400, acc: 0.737, loss: 0.651, lr: 0.1562744178777934\n",
      "epoch: 5500, acc: 0.710, loss: 0.649, lr: 0.15386982612709646\n",
      "epoch: 5600, acc: 0.750, loss: 0.644, lr: 0.15153811183512653\n",
      "epoch: 5700, acc: 0.743, loss: 0.637, lr: 0.14927601134497687\n",
      "epoch: 5800, acc: 0.713, loss: 0.643, lr: 0.14708045300779526\n",
      "epoch: 5900, acc: 0.760, loss: 0.623, lr: 0.14494854326714016\n",
      "epoch: 6000, acc: 0.730, loss: 0.624, lr: 0.1428775539362766\n",
      "epoch: 6100, acc: 0.753, loss: 0.620, lr: 0.1408649105507818\n",
      "epoch: 6200, acc: 0.757, loss: 0.611, lr: 0.13890818169190167\n",
      "epoch: 6300, acc: 0.753, loss: 0.610, lr: 0.13700506918755992\n",
      "epoch: 6400, acc: 0.767, loss: 0.596, lr: 0.13515339910798757\n",
      "epoch: 6500, acc: 0.743, loss: 0.600, lr: 0.13335111348179757\n",
      "epoch: 6600, acc: 0.763, loss: 0.598, lr: 0.13159626266614027\n",
      "epoch: 6700, acc: 0.777, loss: 0.590, lr: 0.12988699831146902\n",
      "epoch: 6800, acc: 0.777, loss: 0.587, lr: 0.12822156686754713\n",
      "epoch: 6900, acc: 0.783, loss: 0.578, lr: 0.126598303582732\n",
      "epoch: 7000, acc: 0.790, loss: 0.574, lr: 0.12501562695336915\n",
      "epoch: 7100, acc: 0.787, loss: 0.570, lr: 0.12347203358439313\n",
      "epoch: 7200, acc: 0.790, loss: 0.566, lr: 0.12196609342602757\n",
      "epoch: 7300, acc: 0.790, loss: 0.563, lr: 0.12049644535486204\n",
      "epoch: 7400, acc: 0.787, loss: 0.558, lr: 0.11906179307060363\n",
      "epoch: 7500, acc: 0.793, loss: 0.555, lr: 0.11766090128250381\n",
      "epoch: 7600, acc: 0.790, loss: 0.551, lr: 0.11629259216187929\n",
      "epoch: 7700, acc: 0.787, loss: 0.549, lr: 0.11495574203931487\n",
      "epoch: 7800, acc: 0.787, loss: 0.546, lr: 0.11364927832708263\n",
      "epoch: 7900, acc: 0.783, loss: 0.541, lr: 0.11237217664906168\n",
      "epoch: 8000, acc: 0.783, loss: 0.540, lr: 0.11112345816201799\n",
      "epoch: 8100, acc: 0.787, loss: 0.537, lr: 0.10990218705352237\n",
      "epoch: 8200, acc: 0.790, loss: 0.534, lr: 0.10870746820306555\n",
      "epoch: 8300, acc: 0.793, loss: 0.531, lr: 0.1075384449940854\n",
      "epoch: 8400, acc: 0.793, loss: 0.529, lr: 0.10639429726566654\n",
      "epoch: 8500, acc: 0.793, loss: 0.526, lr: 0.10527423939362038\n",
      "epoch: 8600, acc: 0.797, loss: 0.524, lr: 0.10417751849150952\n",
      "epoch: 8700, acc: 0.800, loss: 0.522, lr: 0.10310341272296113\n",
      "epoch: 8800, acc: 0.797, loss: 0.519, lr: 0.1020512297173181\n",
      "epoch: 8900, acc: 0.797, loss: 0.517, lr: 0.10102030508132134\n",
      "epoch: 9000, acc: 0.800, loss: 0.514, lr: 0.1000100010001\n",
      "epoch: 9100, acc: 0.800, loss: 0.512, lr: 0.09901970492127933\n",
      "epoch: 9200, acc: 0.800, loss: 0.510, lr: 0.09804882831650162\n",
      "epoch: 9300, acc: 0.803, loss: 0.508, lr: 0.09709680551509856\n",
      "epoch: 9400, acc: 0.807, loss: 0.506, lr: 0.09616309260505818\n",
      "epoch: 9500, acc: 0.807, loss: 0.504, lr: 0.09524716639679968\n",
      "epoch: 9600, acc: 0.810, loss: 0.502, lr: 0.09434852344560807\n",
      "epoch: 9700, acc: 0.817, loss: 0.500, lr: 0.09346667912889055\n",
      "epoch: 9800, acc: 0.813, loss: 0.498, lr: 0.09260116677470137\n",
      "epoch: 9900, acc: 0.820, loss: 0.496, lr: 0.09175153683824203\n",
      "epoch: 10000, acc: 0.817, loss: 0.494, lr: 0.09091735612328393\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "dense1 = Layer_Dense(2,64)\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = Layer_Dense(64,3)\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "optimizer = Optimizer_SGD(decay = 1e-3, momentum = 0.5)\n",
    "l = []\n",
    "ep = []\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "    # Forward pass\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "    loss = loss_activation.forward(dense2.output,y)\n",
    "\n",
    "    predictions = np.argmax(loss_activation.output, axis = 1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis = 1)\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    ep.append(epoch)\n",
    "    l.append(loss)\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' + \n",
    "              f'acc: {accuracy:.3f}, ' + \n",
    "              f'loss: {loss:.3f}, ' + \n",
    "              f'lr: {optimizer.current_learning_rate}')\n",
    "    \n",
    "    # Backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    \n",
    "    # Update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()\n",
    "plt.plot(ep,l)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model achieved the lowest loss and highest accuracy that we’ve seen so far, but can we do \n",
    "even better? Sure we can! Let’s try to set the momentum to 0.8:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.297, loss: 1.099, lr: 1.0\n",
      "epoch: 100, acc: 0.407, loss: 1.068, lr: 0.9099181073703367\n",
      "epoch: 200, acc: 0.447, loss: 1.044, lr: 0.8340283569641367\n",
      "epoch: 300, acc: 0.553, loss: 0.971, lr: 0.7698229407236336\n",
      "epoch: 400, acc: 0.473, loss: 0.977, lr: 0.7147962830593281\n",
      "epoch: 500, acc: 0.507, loss: 0.922, lr: 0.66711140760507\n",
      "epoch: 600, acc: 0.593, loss: 0.821, lr: 0.6253908692933083\n",
      "epoch: 700, acc: 0.620, loss: 0.796, lr: 0.5885815185403178\n",
      "epoch: 800, acc: 0.677, loss: 0.728, lr: 0.5558643690939411\n",
      "epoch: 900, acc: 0.697, loss: 0.690, lr: 0.526592943654555\n",
      "epoch: 1000, acc: 0.747, loss: 0.597, lr: 0.5002501250625312\n",
      "epoch: 1100, acc: 0.730, loss: 0.600, lr: 0.4764173415912339\n",
      "epoch: 1200, acc: 0.747, loss: 0.592, lr: 0.45475216007276037\n",
      "epoch: 1300, acc: 0.787, loss: 0.493, lr: 0.43497172683775553\n",
      "epoch: 1400, acc: 0.817, loss: 0.472, lr: 0.4168403501458941\n",
      "epoch: 1500, acc: 0.803, loss: 0.438, lr: 0.4001600640256102\n",
      "epoch: 1600, acc: 0.817, loss: 0.440, lr: 0.3847633705271258\n",
      "epoch: 1700, acc: 0.833, loss: 0.403, lr: 0.3705075954057058\n",
      "epoch: 1800, acc: 0.837, loss: 0.400, lr: 0.35727045373347627\n",
      "epoch: 1900, acc: 0.857, loss: 0.374, lr: 0.3449465332873405\n",
      "epoch: 2000, acc: 0.860, loss: 0.375, lr: 0.33344448149383127\n",
      "epoch: 2100, acc: 0.860, loss: 0.374, lr: 0.32268473701193934\n",
      "epoch: 2200, acc: 0.863, loss: 0.362, lr: 0.31259768677711786\n",
      "epoch: 2300, acc: 0.870, loss: 0.351, lr: 0.3031221582297666\n",
      "epoch: 2400, acc: 0.873, loss: 0.346, lr: 0.29420417769932333\n",
      "epoch: 2500, acc: 0.877, loss: 0.337, lr: 0.2857959416976279\n",
      "epoch: 2600, acc: 0.873, loss: 0.332, lr: 0.2778549597110308\n",
      "epoch: 2700, acc: 0.873, loss: 0.328, lr: 0.2703433360367667\n",
      "epoch: 2800, acc: 0.873, loss: 0.323, lr: 0.26322716504343247\n",
      "epoch: 2900, acc: 0.877, loss: 0.319, lr: 0.25647601949217746\n",
      "epoch: 3000, acc: 0.873, loss: 0.315, lr: 0.25006251562890724\n",
      "epoch: 3100, acc: 0.877, loss: 0.311, lr: 0.2439619419370578\n",
      "epoch: 3200, acc: 0.877, loss: 0.307, lr: 0.23815194093831865\n",
      "epoch: 3300, acc: 0.880, loss: 0.304, lr: 0.23261223540358225\n",
      "epoch: 3400, acc: 0.873, loss: 0.300, lr: 0.22732439190725165\n",
      "epoch: 3500, acc: 0.873, loss: 0.297, lr: 0.22227161591464767\n",
      "epoch: 3600, acc: 0.873, loss: 0.295, lr: 0.21743857360295715\n",
      "epoch: 3700, acc: 0.880, loss: 0.292, lr: 0.21281123643328367\n",
      "epoch: 3800, acc: 0.873, loss: 0.290, lr: 0.20837674515524068\n",
      "epoch: 3900, acc: 0.880, loss: 0.288, lr: 0.20412329046744235\n",
      "epoch: 4000, acc: 0.870, loss: 0.286, lr: 0.2000400080016003\n",
      "epoch: 4100, acc: 0.870, loss: 0.284, lr: 0.19611688566385566\n",
      "epoch: 4200, acc: 0.883, loss: 0.282, lr: 0.19234468166955185\n",
      "epoch: 4300, acc: 0.873, loss: 0.281, lr: 0.18871485185884126\n",
      "epoch: 4400, acc: 0.883, loss: 0.279, lr: 0.18521948508983144\n",
      "epoch: 4500, acc: 0.883, loss: 0.278, lr: 0.18185124568103292\n",
      "epoch: 4600, acc: 0.883, loss: 0.277, lr: 0.1786033220217896\n",
      "epoch: 4700, acc: 0.880, loss: 0.275, lr: 0.1754693805930865\n",
      "epoch: 4800, acc: 0.883, loss: 0.274, lr: 0.17244352474564578\n",
      "epoch: 4900, acc: 0.883, loss: 0.273, lr: 0.16952025767079165\n",
      "epoch: 5000, acc: 0.887, loss: 0.272, lr: 0.16669444907484582\n",
      "epoch: 5100, acc: 0.883, loss: 0.271, lr: 0.16396130513198884\n",
      "epoch: 5200, acc: 0.883, loss: 0.270, lr: 0.16131634134537828\n",
      "epoch: 5300, acc: 0.883, loss: 0.269, lr: 0.15875535799333226\n",
      "epoch: 5400, acc: 0.883, loss: 0.268, lr: 0.1562744178777934\n",
      "epoch: 5500, acc: 0.883, loss: 0.267, lr: 0.15386982612709646\n",
      "epoch: 5600, acc: 0.883, loss: 0.266, lr: 0.15153811183512653\n",
      "epoch: 5700, acc: 0.887, loss: 0.265, lr: 0.14927601134497687\n",
      "epoch: 5800, acc: 0.887, loss: 0.264, lr: 0.14708045300779526\n",
      "epoch: 5900, acc: 0.890, loss: 0.263, lr: 0.14494854326714016\n",
      "epoch: 6000, acc: 0.897, loss: 0.262, lr: 0.1428775539362766\n",
      "epoch: 6100, acc: 0.897, loss: 0.260, lr: 0.1408649105507818\n",
      "epoch: 6200, acc: 0.893, loss: 0.259, lr: 0.13890818169190167\n",
      "epoch: 6300, acc: 0.893, loss: 0.258, lr: 0.13700506918755992\n",
      "epoch: 6400, acc: 0.897, loss: 0.257, lr: 0.13515339910798757\n",
      "epoch: 6500, acc: 0.893, loss: 0.257, lr: 0.13335111348179757\n",
      "epoch: 6600, acc: 0.897, loss: 0.256, lr: 0.13159626266614027\n",
      "epoch: 6700, acc: 0.897, loss: 0.255, lr: 0.12988699831146902\n",
      "epoch: 6800, acc: 0.897, loss: 0.254, lr: 0.12822156686754713\n",
      "epoch: 6900, acc: 0.897, loss: 0.253, lr: 0.126598303582732\n",
      "epoch: 7000, acc: 0.897, loss: 0.253, lr: 0.12501562695336915\n",
      "epoch: 7100, acc: 0.897, loss: 0.251, lr: 0.12347203358439313\n",
      "epoch: 7200, acc: 0.897, loss: 0.251, lr: 0.12196609342602757\n",
      "epoch: 7300, acc: 0.897, loss: 0.249, lr: 0.12049644535486204\n",
      "epoch: 7400, acc: 0.900, loss: 0.249, lr: 0.11906179307060363\n",
      "epoch: 7500, acc: 0.900, loss: 0.248, lr: 0.11766090128250381\n",
      "epoch: 7600, acc: 0.907, loss: 0.246, lr: 0.11629259216187929\n",
      "epoch: 7700, acc: 0.907, loss: 0.245, lr: 0.11495574203931487\n",
      "epoch: 7800, acc: 0.907, loss: 0.244, lr: 0.11364927832708263\n",
      "epoch: 7900, acc: 0.907, loss: 0.243, lr: 0.11237217664906168\n",
      "epoch: 8000, acc: 0.907, loss: 0.242, lr: 0.11112345816201799\n",
      "epoch: 8100, acc: 0.903, loss: 0.241, lr: 0.10990218705352237\n",
      "epoch: 8200, acc: 0.907, loss: 0.239, lr: 0.10870746820306555\n",
      "epoch: 8300, acc: 0.910, loss: 0.238, lr: 0.1075384449940854\n",
      "epoch: 8400, acc: 0.913, loss: 0.236, lr: 0.10639429726566654\n",
      "epoch: 8500, acc: 0.913, loss: 0.235, lr: 0.10527423939362038\n",
      "epoch: 8600, acc: 0.910, loss: 0.233, lr: 0.10417751849150952\n",
      "epoch: 8700, acc: 0.910, loss: 0.232, lr: 0.10310341272296113\n",
      "epoch: 8800, acc: 0.910, loss: 0.231, lr: 0.1020512297173181\n",
      "epoch: 8900, acc: 0.910, loss: 0.230, lr: 0.10102030508132134\n",
      "epoch: 9000, acc: 0.910, loss: 0.228, lr: 0.1000100010001\n",
      "epoch: 9100, acc: 0.910, loss: 0.227, lr: 0.09901970492127933\n",
      "epoch: 9200, acc: 0.910, loss: 0.226, lr: 0.09804882831650162\n",
      "epoch: 9300, acc: 0.913, loss: 0.224, lr: 0.09709680551509856\n",
      "epoch: 9400, acc: 0.917, loss: 0.222, lr: 0.09616309260505818\n",
      "epoch: 9500, acc: 0.913, loss: 0.221, lr: 0.09524716639679968\n",
      "epoch: 9600, acc: 0.917, loss: 0.219, lr: 0.09434852344560807\n",
      "epoch: 9700, acc: 0.917, loss: 0.217, lr: 0.09346667912889055\n",
      "epoch: 9800, acc: 0.917, loss: 0.216, lr: 0.09260116677470137\n",
      "epoch: 9900, acc: 0.917, loss: 0.215, lr: 0.09175153683824203\n",
      "epoch: 10000, acc: 0.920, loss: 0.214, lr: 0.09091735612328393\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "dense1 = Layer_Dense(2,64)\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = Layer_Dense(64,3)\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "optimizer = Optimizer_SGD(decay = 1e-3, momentum = 0.8)\n",
    "l = []\n",
    "ep = []\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "    # Forward pass\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "    loss = loss_activation.forward(dense2.output,y)\n",
    "\n",
    "    predictions = np.argmax(loss_activation.output, axis = 1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis = 1)\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    ep.append(epoch)\n",
    "    l.append(loss)\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' + \n",
    "              f'acc: {accuracy:.3f}, ' + \n",
    "              f'loss: {loss:.3f}, ' + \n",
    "              f'lr: {optimizer.current_learning_rate}')\n",
    "    \n",
    "    # Backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    \n",
    "    # Update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()\n",
    "plt.plot(ep,l)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaGrad\n",
    "\n",
    "AdaGrad, short for adaptive gradient, institutes a per-parameter learning rate rather than a \n",
    "globally-shared rate. The idea here is to normalize updates made to the features. During the \n",
    "training process, some weights can rise significantly, while others tend to not change by much. \n",
    "It is usually better for weights to not rise too high compared to the other weights, and we’ll talk \n",
    "about this with regularization techniques. AdaGrad provides a way to normalize parameter updates \n",
    "by keeping a history of previous updates — the bigger the sum of the updates is, in either direction \n",
    "(positive or negative), the smaller updates are made further in training. This lets less-frequently \n",
    "updated parameters to keep-up with changes, effectively utilizing more neurons for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_Adagrad:\n",
    "    \n",
    "    def __init__(self, learning_rate = 1.0, decay = 0.0, epsilon = 1e-7):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1.0 / (1.0 + self.decay * self.iterations))\n",
    "            \n",
    "    def update_params(self, layer):\n",
    "        \n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "            \n",
    "        layer.weight_cache += layer.dweights**2\n",
    "        layer.bias_cache += layer.dbiases**2\n",
    "        \n",
    "        layer.weights += -self.current_learning_rate * layer.dweights / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * layer.dbiases / (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "        \n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing this optimizer now with decaying set to 1e-4 as well as 1e-5 works better than 1e-3, which \n",
    "we have used previously. This optimizer with our dataset works better with lesser decaying:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.320, loss: 1.099, lr: 1.0\n",
      "epoch: 100, acc: 0.447, loss: 1.038, lr: 0.9901970492127933\n",
      "epoch: 200, acc: 0.487, loss: 1.024, lr: 0.9804882831650161\n",
      "epoch: 300, acc: 0.513, loss: 1.019, lr: 0.9709680551509855\n",
      "epoch: 400, acc: 0.527, loss: 1.012, lr: 0.9616309260505818\n",
      "epoch: 500, acc: 0.523, loss: 1.006, lr: 0.9524716639679969\n",
      "epoch: 600, acc: 0.520, loss: 1.000, lr: 0.9434852344560807\n",
      "epoch: 700, acc: 0.527, loss: 0.992, lr: 0.9346667912889054\n",
      "epoch: 800, acc: 0.523, loss: 0.988, lr: 0.9260116677470135\n",
      "epoch: 900, acc: 0.513, loss: 0.982, lr: 0.9175153683824203\n",
      "epoch: 1000, acc: 0.500, loss: 0.974, lr: 0.9091735612328392\n",
      "epoch: 1100, acc: 0.517, loss: 0.958, lr: 0.9009820704567978\n",
      "epoch: 1200, acc: 0.517, loss: 0.970, lr: 0.892936869363336\n",
      "epoch: 1300, acc: 0.517, loss: 0.962, lr: 0.8850340738118416\n",
      "epoch: 1400, acc: 0.523, loss: 0.957, lr: 0.8772699359592947\n",
      "epoch: 1500, acc: 0.523, loss: 0.952, lr: 0.8696408383337683\n",
      "epoch: 1600, acc: 0.533, loss: 0.945, lr: 0.8621432882145013\n",
      "epoch: 1700, acc: 0.547, loss: 0.933, lr: 0.8547739123001966\n",
      "epoch: 1800, acc: 0.567, loss: 0.945, lr: 0.8475294516484448\n",
      "epoch: 1900, acc: 0.567, loss: 0.933, lr: 0.8404067568703253\n",
      "epoch: 2000, acc: 0.550, loss: 0.926, lr: 0.8334027835652972\n",
      "epoch: 2100, acc: 0.583, loss: 0.931, lr: 0.8265145879824779\n",
      "epoch: 2200, acc: 0.510, loss: 0.922, lr: 0.8197393228953193\n",
      "epoch: 2300, acc: 0.507, loss: 0.917, lr: 0.8130742336775347\n",
      "epoch: 2400, acc: 0.527, loss: 0.912, lr: 0.8065166545689169\n",
      "epoch: 2500, acc: 0.567, loss: 0.906, lr: 0.8000640051204096\n",
      "epoch: 2600, acc: 0.513, loss: 0.915, lr: 0.7937137868084768\n",
      "epoch: 2700, acc: 0.517, loss: 0.904, lr: 0.7874635798094338\n",
      "epoch: 2800, acc: 0.580, loss: 0.901, lr: 0.7813110399249941\n",
      "epoch: 2900, acc: 0.543, loss: 0.898, lr: 0.7752538956508256\n",
      "epoch: 3000, acc: 0.523, loss: 0.897, lr: 0.7692899453804138\n",
      "epoch: 3100, acc: 0.587, loss: 0.902, lr: 0.7634170547370028\n",
      "epoch: 3200, acc: 0.560, loss: 0.892, lr: 0.7576331540268202\n",
      "epoch: 3300, acc: 0.537, loss: 0.889, lr: 0.7519362358072035\n",
      "epoch: 3400, acc: 0.537, loss: 0.883, lr: 0.7463243525636241\n",
      "epoch: 3500, acc: 0.527, loss: 0.880, lr: 0.7407956144899621\n",
      "epoch: 3600, acc: 0.567, loss: 0.876, lr: 0.735348187366718\n",
      "epoch: 3700, acc: 0.553, loss: 0.872, lr: 0.7299802905321557\n",
      "epoch: 3800, acc: 0.553, loss: 0.867, lr: 0.7246901949416624\n",
      "epoch: 3900, acc: 0.580, loss: 0.863, lr: 0.7194762213108857\n",
      "epoch: 4000, acc: 0.543, loss: 0.861, lr: 0.7143367383384527\n",
      "epoch: 4100, acc: 0.557, loss: 0.857, lr: 0.7092701610043266\n",
      "epoch: 4200, acc: 0.580, loss: 0.855, lr: 0.7042749489400663\n",
      "epoch: 4300, acc: 0.537, loss: 0.851, lr: 0.6993496048674733\n",
      "epoch: 4400, acc: 0.560, loss: 0.849, lr: 0.6944926731022988\n",
      "epoch: 4500, acc: 0.533, loss: 0.845, lr: 0.6897027381198704\n",
      "epoch: 4600, acc: 0.590, loss: 0.844, lr: 0.6849784231796698\n",
      "epoch: 4700, acc: 0.547, loss: 0.841, lr: 0.6803183890060548\n",
      "epoch: 4800, acc: 0.533, loss: 0.840, lr: 0.6757213325224677\n",
      "epoch: 4900, acc: 0.537, loss: 0.838, lr: 0.6711859856366199\n",
      "epoch: 5000, acc: 0.547, loss: 0.836, lr: 0.6667111140742716\n",
      "epoch: 5100, acc: 0.540, loss: 0.834, lr: 0.6622955162593549\n",
      "epoch: 5200, acc: 0.543, loss: 0.832, lr: 0.6579380222383051\n",
      "epoch: 5300, acc: 0.590, loss: 0.831, lr: 0.6536374926465782\n",
      "epoch: 5400, acc: 0.560, loss: 0.829, lr: 0.649392817715436\n",
      "epoch: 5500, acc: 0.553, loss: 0.828, lr: 0.6452029163171817\n",
      "epoch: 5600, acc: 0.570, loss: 0.826, lr: 0.6410667350471184\n",
      "epoch: 5700, acc: 0.570, loss: 0.825, lr: 0.6369832473405949\n",
      "epoch: 5800, acc: 0.567, loss: 0.824, lr: 0.6329514526235838\n",
      "epoch: 5900, acc: 0.580, loss: 0.822, lr: 0.6289703754953141\n",
      "epoch: 6000, acc: 0.577, loss: 0.821, lr: 0.6250390649415589\n",
      "epoch: 6100, acc: 0.580, loss: 0.819, lr: 0.6211565935772407\n",
      "epoch: 6200, acc: 0.583, loss: 0.818, lr: 0.6173220569170937\n",
      "epoch: 6300, acc: 0.587, loss: 0.817, lr: 0.6135345726731701\n",
      "epoch: 6400, acc: 0.587, loss: 0.816, lr: 0.6097932800780536\n",
      "epoch: 6500, acc: 0.590, loss: 0.815, lr: 0.6060973392326807\n",
      "epoch: 6600, acc: 0.590, loss: 0.814, lr: 0.6024459304777396\n",
      "epoch: 6700, acc: 0.587, loss: 0.813, lr: 0.5988382537876519\n",
      "epoch: 6800, acc: 0.587, loss: 0.812, lr: 0.5952735281862016\n",
      "epoch: 6900, acc: 0.587, loss: 0.811, lr: 0.5917509911829102\n",
      "epoch: 7000, acc: 0.587, loss: 0.810, lr: 0.5882698982293076\n",
      "epoch: 7100, acc: 0.593, loss: 0.809, lr: 0.5848295221942803\n",
      "epoch: 7200, acc: 0.593, loss: 0.808, lr: 0.5814291528577243\n",
      "epoch: 7300, acc: 0.593, loss: 0.807, lr: 0.5780680964217585\n",
      "epoch: 7400, acc: 0.597, loss: 0.806, lr: 0.5747456750387954\n",
      "epoch: 7500, acc: 0.600, loss: 0.805, lr: 0.5714612263557918\n",
      "epoch: 7600, acc: 0.603, loss: 0.804, lr: 0.5682141030740383\n",
      "epoch: 7700, acc: 0.607, loss: 0.803, lr: 0.5650036725238714\n",
      "epoch: 7800, acc: 0.607, loss: 0.802, lr: 0.5618293162537221\n",
      "epoch: 7900, acc: 0.610, loss: 0.801, lr: 0.5586904296329404\n",
      "epoch: 8000, acc: 0.610, loss: 0.801, lr: 0.5555864214678593\n",
      "epoch: 8100, acc: 0.610, loss: 0.800, lr: 0.5525167136305873\n",
      "epoch: 8200, acc: 0.610, loss: 0.799, lr: 0.5494807407000385\n",
      "epoch: 8300, acc: 0.613, loss: 0.799, lr: 0.5464779496147331\n",
      "epoch: 8400, acc: 0.620, loss: 0.798, lr: 0.5435077993369205\n",
      "epoch: 8500, acc: 0.620, loss: 0.797, lr: 0.5405697605275961\n",
      "epoch: 8600, acc: 0.620, loss: 0.796, lr: 0.5376633152320017\n",
      "epoch: 8700, acc: 0.620, loss: 0.795, lr: 0.5347879565752179\n",
      "epoch: 8800, acc: 0.623, loss: 0.795, lr: 0.5319431884674717\n",
      "epoch: 8900, acc: 0.623, loss: 0.794, lr: 0.5291285253188\n",
      "epoch: 9000, acc: 0.623, loss: 0.794, lr: 0.5263434917627243\n",
      "epoch: 9100, acc: 0.623, loss: 0.793, lr: 0.5235876223886068\n",
      "epoch: 9200, acc: 0.620, loss: 0.792, lr: 0.5208604614823689\n",
      "epoch: 9300, acc: 0.623, loss: 0.792, lr: 0.5181615627752734\n",
      "epoch: 9400, acc: 0.620, loss: 0.791, lr: 0.5154904892004742\n",
      "epoch: 9500, acc: 0.623, loss: 0.791, lr: 0.5128468126570593\n",
      "epoch: 9600, acc: 0.627, loss: 0.790, lr: 0.5102301137813153\n",
      "epoch: 9700, acc: 0.630, loss: 0.790, lr: 0.5076399817249606\n",
      "epoch: 9800, acc: 0.633, loss: 0.789, lr: 0.5050760139400979\n",
      "epoch: 9900, acc: 0.640, loss: 0.789, lr: 0.5025378159706518\n",
      "epoch: 10000, acc: 0.637, loss: 0.788, lr: 0.5000250012500626\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXEAAAD4CAYAAAAaT9YAAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAAsTAAALEwEAmpwYAAAR9UlEQVR4nO3dfZBddX3H8fd39+4m2UiSDeyEkKAhLVKoVsHVAbFOK6gUn9oZ2+JoBWuHmbbTonbGgTpTxz86Y1uHUaetyqjotApaZNTBVkREGcYa3QgKEgLhGRrIAoGEPO3D/faPe3Zzd7NJNnsve/e3vl8zd+65v/P0/e3Z/Zxzzzl3b2QmkqQydXW6AEnS3BniklQwQ1ySCmaIS1LBDHFJKlhtPld2wgkn5IYNG+ZzlZJUvM2bNz+VmQMzjZvXEN+wYQNDQ0PzuUpJKl5EPHy4cZ5OkaSCGeKSVDBDXJIKZohLUsEMcUkqmCEuSQUzxCWpYEWE+LYdu/nJA093ugxJWnDm9cM+c3X+lbcC8NDH39LhSiRpYSniSFySNDNDXJIKZohLUsEMcUkqmCEuSQUzxCWpYIa4JBXMEJekghniklQwQ1ySCmaIS1LBjhriEfHFiNgREXc1ta2OiJsi4r7quf+FLVOSNJPZHIl/CbhgWtvlwM2ZeSpwc/VakjTPjhrimXkr8My05ncAX66Gvwz8YXvLkiTNxlzPia/JzO3V8BPAmsNNGBGXRsRQRAwNDw/PcXWSpJm0fGEzMxPII4y/KjMHM3NwYGCg1dVJkprMNcSfjIi1ANXzjvaVJEmarbmG+LeBi6vhi4FvtaccSdKxmM0thtcA/wucFhGPRcT7gY8Db4yI+4Dzq9eSpHl21O/YzMx3HWbUeW2uRZJ0jPzEpiQVzBCXpIIZ4pJUMENckgpmiEtSwQxxSSqYIS5JBTPEJalghrgkFcwQl6SCGeKSVDBDXJIKZohLUsEMcUkqmCEuSQUzxCWpYIa4JBXMEJekghniklQwQ1ySCmaIS1LBDHFJKpghLkkFM8QlqWCGuCQVzBCXpIIZ4pJUMENckgpmiEtSwQxxSSqYIS5JBWspxCPigxHxq4i4KyKuiYil7SpMknR0cw7xiFgH/C0wmJkvA7qBi9pVmCTp6Fo9nVIDlkVEDegD/q/1kiRJszXnEM/Mx4FPAI8A24HnMvN706eLiEsjYigihoaHh+deqSTpEK2cTukH3gGcApwELI+I90yfLjOvyszBzBwcGBiYe6WSpEO0cjrlfODBzBzOzFHgeuC17SlLkjQbrYT4I8DZEdEXEQGcB2xpT1mSpNlo5Zz4JuA64OfAndWyrmpTXZKkWai1MnNmfhT4aJtqkSQdIz+xKUkFM8QlqWCGuCQVzBCXpIIZ4pJUMENckgpmiEtSwQxxSSqYIS5JBTPEJalghrgkFcwQl6SCGeKSVDBDXJIKZohLUsEMcUkqmCEuSQUzxCWpYIa4JBXMEJekghniklQwQ1ySCmaIS1LBDHFJKpghLkkFM8QlqWCGuCQVzBCXpIIZ4pJUMENckgrWUohHxKqIuC4i7omILRFxTrsKkyQdXa3F+T8FfDcz3xkRvUBfG2qSJM3SnEM8IlYCrwcuAcjMEWCkPWVJkmajldMppwDDwNURcXtEfD4ilk+fKCIujYihiBgaHh5uYXWSpOlaCfEacBbwmcw8E9gDXD59osy8KjMHM3NwYGCghdVJkqZrJcQfAx7LzE3V6+tohLokaZ7MOcQz8wng0Yg4rWo6D7i7LVVJkmal1btT/gb4SnVnygPA+1ovSZI0Wy2FeGbeAQy2pxRJ0rHyE5uSVDBDXJIKZohLUsEMcUkqmCEuSQUzxCWpYIa4JBXMEJekghniklQwQ1ySCmaIS1LBDHFJKpghLkkFM8QlqWCGuCQVzBCXpIIZ4pJUMENckgpmiEtSwQxxSSqYIS5JBTPEJalghrgkFcwQl6SCGeKSVDBDXJIKZohLUsEMcUkqmCEuSQUzxCWpYIa4JBWs5RCPiO6IuD0ibmhHQZKk2WvHkfhlwJY2LEeSdIxaCvGIWA+8Bfh8e8qRJB2LVo/EPwl8GKgfboKIuDQihiJiaHh4uMXVSZKazTnEI+KtwI7M3Hyk6TLzqswczMzBgYGBua5OkjSDVo7EzwXeHhEPAdcCb4iI/2xLVZKkWZlziGfmFZm5PjM3ABcBP8jM97StMknSUXmfuCQVrNaOhWTmD4EftmNZkqTZ80hckgpmiEtSwQxxSSqYIS5JBTPEJalghrgkFcwQl6SCGeKSVDBDXJIKZohLUsEMcUkqmCEuSQUzxCWpYIa4JBXMEJekghniklQwQ1ySCmaIS1LBDHFJKpghLkkFM8QlqWCGuCQVrKgQz8xOlyBJC0pRIS5JmqqoEPdAXJKmKirEd+4d6XQJkrSgFBXiu/aPdboESVpQigpxL2xK0lRlhXinC5CkBaasEPdIXJKmKCzEO12BJC0scw7xiDg5Im6JiLsj4lcRcVk7C5uJGS5JU9VamHcM+LvM/HlEHAdsjoibMvPuNtV2iLqH4pI0xZyPxDNze2b+vBreDWwB1rWrsJnX+UIuXZLK05Zz4hGxATgT2DTDuEsjYigihoaHh1tajyEuSVO1HOIR8SLgG8AHMnPX9PGZeVVmDmbm4MDAQEvrSs+KS9IULYV4RPTQCPCvZOb17Snp8DwSl6SpWrk7JYAvAFsy88r2lXR4hrgkTdXKkfi5wJ8Bb4iIO6rHhW2qa0bLl3S/kIuXpOLM+RbDzLwNiDbWclTeYihJUxX1ic3xeqcrkKSFpbAQ90hckpoVFeKeTpGkqYoKcY/EJWmqIkI8qsun4x6JS9IURYR4d5XidY/EJWmKIkK8q6sR4p5OkaSpigjx3u5GmaPjhrgkNSsixJf2ND6puW90vMOVSNLCUkSIT9xa+M07Hu9wJZK0sBQR4qv6egD4zi+3d7gSSVpYigjxl6zumxze7ykVSZrUyndszpu3v/Ikbtna+Fag3/nY93jF+pXs3DvKmhVL2LHrAPfteP6QedatWsbrXzrAS9e8iNNOPI6T+/tYsbSH5Uu66e4KIo79f3dd+9NHWL6kxttecVLLfZKkdoicxw/QDA4O5tDQ0JzmzUxu2/YU373rCX7ywNPcP7ynzdXNrL+vh517R484TV9vN+tWLeP0tSt42boVvOaU41nd18uJK5dSq26P7OoK9o+O84N7dnDhy9eyc88IH/r6Hfz7u1/Fsl7/xa6kw4uIzZk5OOO4UkJ8JvV6MlZPtj+3j58++AyPP9t4/vH9T7dtHe1S6wrGjnKf+8aB5SzvrTWel9RYUuvi/uE93Hrvod9NeuaLV/G7pw6wuq+HlX09LO+tsfWJ3TzyzF7+9NUns6qvh+f2jfGP37mbVX29/MngegaOW8qqvh4CePjpvbx8/Ur6+3qp9jM8sWs/9z75PGdvXM2SWjf7R8fZuXeEtSuXHbV/9XpO3s8vqb0WbYjPh/F6snPvCP9953aW1rr548H17BkZ5/ZHdrJl+y42P7yTu7fv4tFn9vFbJx5HX283a1cuY13/MkbH64zXk2f2jLBiWQ9f3fTIEde1clkP9Uz6ersZGaszOp48f2BsnnraHr9/2gBDD+1k9wx1X/r6jSzvrbFvdJzP/uh+LjvvVJ4/MMYXbnsQgL/+/d/gnI0nAI0vAPnFo8/y2M59/NFZ61ixtIexerKsp5vlS7qp16G7u7HTWNbTOEUmLVaG+CIwXk/u27GbXfvGePWGfiKC5/aOsnPvCAfG6oyM1Xnq+QMsX1Jjz8gYz+8fY+ihZzjv9DU8uWs/vbUuxuvJ6HidOx59jgefep43nXEiz+4d4eofP8Tu/WO8+bfXcPraFdyzfTcnrlzKl378EAAXvfpkrv3Zo0etcVlPN+v6l7FthmsUL7QltS4OjDX+4fzSni56urqodcfk9Y9aV9AVjdeNNqa01boOjuuKoNbdeG48mLE9ovEvIbq6GtNMLGviuXl8ABFBdxdNy63mm1ZPRNA90V7N17y+iIk2Jtubl9e43DMxfHAdXdFYXtfkMphsm6i3eb7Gug+/jolauwKCmPwfRxPvyKasq+rHlOFqPNXyDvZ16vKiej29rrlc1yqVIa55N/F7tbPa0fT39dLf18OBsTqPP7uPW+8d5qwX93Pbtqf4lxu3AnDJazdwwctOJIC9I+N88vv38sDwHv7+LaczvPsAV950LwDn/ubxnLPxeHprXWQ2pt1zYIwf3TvM6049gdHxOvtG6oyM1xkdqzOeSWajponhsXrjXVK93vjHavVM6vVsmrax4xyv2uuZ1Ku2ejV9JiRMzjdez8n5JsZPTAvgf41ov4nwP7hjOvwOZKKdGXYqMHUHAdPHHdzh0by8aTua6TupiRkCuPqS1/Di4w/eaXds/TTEpQVjIvQbO46DQZ9ANr2uVzuehMnX9frUnUNS7ZzqkDSWlxzcEdWb5s/mZcDkji05uONhor1pGdN3gpN1ZfM6Dy5rYhw0dlwHl3XoeidqbF5vPafWQLXM5vZ6NXPzMrJpGTQttz65jGrctBoO6TMTX8rePG1TndOWxeQ8zTUcPJCZWNQ/vO0M1qxYOqffmSOFeBG3GEqLSVdX0DW/X0+rRayID/tIkmZmiEtSwQxxSSqYIS5JBTPEJalghrgkFcwQl6SCGeKSVLB5/cRmRAwDD89x9hOAp9pYTgns868H+7z4tdrfl2TmwEwj5jXEWxERQ4f72OliZZ9/Pdjnxe+F7K+nUySpYIa4JBWspBC/qtMFdIB9/vVgnxe/F6y/xZwTlyQdqqQjcUnSNIa4JBWsiBCPiAsiYmtEbIuIyztdz1xFxMkRcUtE3B0Rv4qIy6r21RFxU0TcVz33V+0REZ+u+v3LiDiraVkXV9PfFxEXd6pPsxUR3RFxe0TcUL0+JSI2VX37WkT0Vu1LqtfbqvEbmpZxRdW+NSLe3KGuzEpErIqI6yLinojYEhHnLPbtHBEfrH6v74qIayJi6WLbzhHxxYjYERF3NbW1bbtGxKsi4s5qnk9HzOKLRLP62qaF+gC6gfuBjUAv8AvgjE7XNce+rAXOqoaPA+4FzgD+Gbi8ar8c+Kdq+ELgf2h8Rd/ZwKaqfTXwQPXcXw33d7p/R+n7h4CvAjdUr78OXFQNfxb4y2r4r4DPVsMXAV+rhs+otv0S4JTqd6K70/06Qn+/DPxFNdwLrFrM2xlYBzwILGvavpcstu0MvB44C7irqa1t2xX4aTVtVPP+wVFr6vQPZRY/tHOAG5teXwFc0em62tS3bwFvBLYCa6u2tcDWavhzwLuapt9ajX8X8Lmm9inTLbQHsB64GXgDcEP1C/oUUJu+jYEbgXOq4Vo1XUzf7s3TLbQHsLIKtJjWvmi3cxXij1bBVKu285sX43YGNkwL8bZs12rcPU3tU6Y73KOE0ykTvxwTHqvaila9fTwT2ASsyczt1agngDXV8OH6XtrP5JPAh4F69fp44NnMHKteN9c/2bdq/HPV9CX1+RRgGLi6OoX0+YhYziLezpn5OPAJ4BFgO43ttpnFvZ0ntGu7rquGp7cfUQkhvuhExIuAbwAfyMxdzeOysQteNPd9RsRbgR2ZubnTtcyjGo233J/JzDOBPTTeZk9ahNu5H3gHjR3YScBy4IKOFtUBndiuJYT448DJTa/XV21FiogeGgH+lcy8vmp+MiLWVuPXAjuq9sP1vaSfybnA2yPiIeBaGqdUPgWsiohaNU1z/ZN9q8avBJ6mrD4/BjyWmZuq19fRCPXFvJ3PBx7MzOHMHAWup7HtF/N2ntCu7fp4NTy9/YhKCPGfAadWV7l7aVwE+XaHa5qT6krzF4AtmXll06hvAxNXqC+mca58ov291VXus4HnqrdtNwJvioj+6gjoTVXbgpOZV2Tm+szcQGPb/SAz3w3cAryzmmx6nyd+Fu+sps+q/aLqroZTgFNpXARacDLzCeDRiDitajoPuJtFvJ1pnEY5OyL6qt/ziT4v2u3cpC3btRq3KyLOrn6G721a1uF1+iLBLC8kXEjjTo77gY90up4W+vE6Gm+1fgncUT0upHEu8GbgPuD7wOpq+gD+rer3ncBg07L+HNhWPd7X6b7Nsv+/x8G7UzbS+OPcBvwXsKRqX1q93laN39g0/0eqn8VWZnHVvsN9fSUwVG3rb9K4C2FRb2fgY8A9wF3Af9C4w2RRbWfgGhrn/EdpvON6fzu3KzBY/fzuB/6VaRfHZ3r4sXtJKlgJp1MkSYdhiEtSwQxxSSqYIS5JBTPEJalghrgkFcwQl6SC/T+K77Sd5W6wAAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "dense1 = Layer_Dense(2,64)\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = Layer_Dense(64,3)\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "optimizer = Optimizer_Adagrad(decay = 1e-4)\n",
    "l = []\n",
    "ep = []\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "    # Forward pass\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "    loss = loss_activation.forward(dense2.output,y)\n",
    "\n",
    "    predictions = np.argmax(loss_activation.output, axis = 1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis = 1)\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    ep.append(epoch)\n",
    "    l.append(loss)\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' + \n",
    "              f'acc: {accuracy:.3f}, ' + \n",
    "              f'loss: {loss:.3f}, ' + \n",
    "              f'lr: {optimizer.current_learning_rate}')\n",
    "    \n",
    "    # Backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    \n",
    "    # Update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()\n",
    "plt.plot(ep,l)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSProp\n",
    "\n",
    "Continuing with Stochastic Gradient Descent adaptations, we reach RMSProp, short for Root \n",
    "Mean Square Propagation. Similar to AdaGrad, RMSProp calculates an adaptive learning rate \n",
    "per parameter; it’s just calculated in a different way than AdaGrad.\n",
    "\n",
    "`cache = rho * cache + (1 - rho) * gradient ** 2`\n",
    "\n",
    "Note that this is similar to both momentum with the SGD optimizer and cache with the AdaGrad. \n",
    "RMSProp adds a mechanism similar to momentum but also adds a per-parameter adaptive \n",
    "learning rate, so the learning rate changes are smoother."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_RMSprop:\n",
    "    \n",
    "    def __init__(self, learning_rate = 0.001, decay = 0.0, epsilon = 1e-7, rho = 0.9):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.rho = rho\n",
    "        \n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1 / (1 + self.decay * self.iterations))\n",
    "    \n",
    "    def update_params(self, layer):\n",
    "        \n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "            \n",
    "        layer.weight_cache = self.rho * layer.weight_cache + (1 - self.rho) * layer.dweights**2\n",
    "        layer.bias_cache = self.rho * layer.bias_cache + (1 - self.rho) * layer.dbiases**2\n",
    "        \n",
    "        layer.weights += -self.current_learning_rate * layer.dweights / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * layer.dbiases / (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "        \n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing the optimizer used in our main neural network testing code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.327, loss: 1.099, lr: 0.001\n",
      "epoch: 100, acc: 0.407, loss: 1.073, lr: 0.0009901970492127933\n",
      "epoch: 200, acc: 0.413, loss: 1.066, lr: 0.0009804882831650162\n",
      "epoch: 300, acc: 0.427, loss: 1.059, lr: 0.0009709680551509856\n",
      "epoch: 400, acc: 0.463, loss: 1.049, lr: 0.0009616309260505818\n",
      "epoch: 500, acc: 0.523, loss: 1.037, lr: 0.0009524716639679969\n",
      "epoch: 600, acc: 0.530, loss: 1.024, lr: 0.0009434852344560807\n",
      "epoch: 700, acc: 0.537, loss: 1.011, lr: 0.0009346667912889054\n",
      "epoch: 800, acc: 0.523, loss: 0.999, lr: 0.0009260116677470135\n",
      "epoch: 900, acc: 0.507, loss: 0.986, lr: 0.0009175153683824203\n",
      "epoch: 1000, acc: 0.523, loss: 0.974, lr: 0.0009091735612328393\n",
      "epoch: 1100, acc: 0.537, loss: 0.962, lr: 0.0009009820704567978\n",
      "epoch: 1200, acc: 0.543, loss: 0.951, lr: 0.0008929368693633361\n",
      "epoch: 1300, acc: 0.560, loss: 0.939, lr: 0.0008850340738118417\n",
      "epoch: 1400, acc: 0.573, loss: 0.928, lr: 0.0008772699359592947\n",
      "epoch: 1500, acc: 0.570, loss: 0.918, lr: 0.0008696408383337683\n",
      "epoch: 1600, acc: 0.567, loss: 0.908, lr: 0.0008621432882145013\n",
      "epoch: 1700, acc: 0.570, loss: 0.898, lr: 0.0008547739123001966\n",
      "epoch: 1800, acc: 0.573, loss: 0.889, lr: 0.0008475294516484448\n",
      "epoch: 1900, acc: 0.570, loss: 0.881, lr: 0.0008404067568703253\n",
      "epoch: 2000, acc: 0.573, loss: 0.873, lr: 0.0008334027835652972\n",
      "epoch: 2100, acc: 0.570, loss: 0.865, lr: 0.0008265145879824779\n",
      "epoch: 2200, acc: 0.573, loss: 0.858, lr: 0.0008197393228953193\n",
      "epoch: 2300, acc: 0.573, loss: 0.851, lr: 0.0008130742336775348\n",
      "epoch: 2400, acc: 0.570, loss: 0.844, lr: 0.0008065166545689168\n",
      "epoch: 2500, acc: 0.567, loss: 0.838, lr: 0.0008000640051204097\n",
      "epoch: 2600, acc: 0.570, loss: 0.832, lr: 0.0007937137868084768\n",
      "epoch: 2700, acc: 0.587, loss: 0.826, lr: 0.0007874635798094338\n",
      "epoch: 2800, acc: 0.583, loss: 0.821, lr: 0.0007813110399249941\n",
      "epoch: 2900, acc: 0.590, loss: 0.817, lr: 0.0007752538956508256\n",
      "epoch: 3000, acc: 0.590, loss: 0.812, lr: 0.0007692899453804139\n",
      "epoch: 3100, acc: 0.593, loss: 0.808, lr: 0.0007634170547370029\n",
      "epoch: 3200, acc: 0.613, loss: 0.803, lr: 0.0007576331540268202\n",
      "epoch: 3300, acc: 0.607, loss: 0.800, lr: 0.0007519362358072035\n",
      "epoch: 3400, acc: 0.620, loss: 0.796, lr: 0.000746324352563624\n",
      "epoch: 3500, acc: 0.617, loss: 0.792, lr: 0.0007407956144899621\n",
      "epoch: 3600, acc: 0.620, loss: 0.789, lr: 0.0007353481873667181\n",
      "epoch: 3700, acc: 0.617, loss: 0.786, lr: 0.0007299802905321557\n",
      "epoch: 3800, acc: 0.620, loss: 0.782, lr: 0.0007246901949416625\n",
      "epoch: 3900, acc: 0.623, loss: 0.779, lr: 0.0007194762213108858\n",
      "epoch: 4000, acc: 0.630, loss: 0.776, lr: 0.0007143367383384526\n",
      "epoch: 4100, acc: 0.630, loss: 0.773, lr: 0.0007092701610043266\n",
      "epoch: 4200, acc: 0.630, loss: 0.771, lr: 0.0007042749489400662\n",
      "epoch: 4300, acc: 0.627, loss: 0.768, lr: 0.0006993496048674733\n",
      "epoch: 4400, acc: 0.627, loss: 0.765, lr: 0.0006944926731022988\n",
      "epoch: 4500, acc: 0.633, loss: 0.763, lr: 0.0006897027381198704\n",
      "epoch: 4600, acc: 0.633, loss: 0.760, lr: 0.0006849784231796698\n",
      "epoch: 4700, acc: 0.637, loss: 0.758, lr: 0.0006803183890060548\n",
      "epoch: 4800, acc: 0.630, loss: 0.756, lr: 0.0006757213325224677\n",
      "epoch: 4900, acc: 0.633, loss: 0.754, lr: 0.0006711859856366199\n",
      "epoch: 5000, acc: 0.640, loss: 0.752, lr: 0.0006667111140742717\n",
      "epoch: 5100, acc: 0.643, loss: 0.750, lr: 0.000662295516259355\n",
      "epoch: 5200, acc: 0.633, loss: 0.748, lr: 0.0006579380222383051\n",
      "epoch: 5300, acc: 0.643, loss: 0.746, lr: 0.0006536374926465783\n",
      "epoch: 5400, acc: 0.653, loss: 0.744, lr: 0.000649392817715436\n",
      "epoch: 5500, acc: 0.640, loss: 0.742, lr: 0.0006452029163171817\n",
      "epoch: 5600, acc: 0.643, loss: 0.741, lr: 0.0006410667350471185\n",
      "epoch: 5700, acc: 0.657, loss: 0.739, lr: 0.0006369832473405949\n",
      "epoch: 5800, acc: 0.647, loss: 0.737, lr: 0.0006329514526235838\n",
      "epoch: 5900, acc: 0.663, loss: 0.735, lr: 0.0006289703754953141\n",
      "epoch: 6000, acc: 0.647, loss: 0.733, lr: 0.0006250390649415589\n",
      "epoch: 6100, acc: 0.667, loss: 0.732, lr: 0.0006211565935772407\n",
      "epoch: 6200, acc: 0.657, loss: 0.730, lr: 0.0006173220569170938\n",
      "epoch: 6300, acc: 0.673, loss: 0.728, lr: 0.0006135345726731702\n",
      "epoch: 6400, acc: 0.660, loss: 0.726, lr: 0.0006097932800780536\n",
      "epoch: 6500, acc: 0.667, loss: 0.724, lr: 0.0006060973392326808\n",
      "epoch: 6600, acc: 0.660, loss: 0.723, lr: 0.0006024459304777396\n",
      "epoch: 6700, acc: 0.670, loss: 0.721, lr: 0.0005988382537876519\n",
      "epoch: 6800, acc: 0.667, loss: 0.719, lr: 0.0005952735281862016\n",
      "epoch: 6900, acc: 0.673, loss: 0.718, lr: 0.0005917509911829102\n",
      "epoch: 7000, acc: 0.670, loss: 0.716, lr: 0.0005882698982293077\n",
      "epoch: 7100, acc: 0.677, loss: 0.714, lr: 0.0005848295221942804\n",
      "epoch: 7200, acc: 0.680, loss: 0.713, lr: 0.0005814291528577244\n",
      "epoch: 7300, acc: 0.683, loss: 0.711, lr: 0.0005780680964217585\n",
      "epoch: 7400, acc: 0.683, loss: 0.710, lr: 0.0005747456750387954\n",
      "epoch: 7500, acc: 0.697, loss: 0.708, lr: 0.0005714612263557917\n",
      "epoch: 7600, acc: 0.690, loss: 0.707, lr: 0.0005682141030740383\n",
      "epoch: 7700, acc: 0.680, loss: 0.705, lr: 0.0005650036725238715\n",
      "epoch: 7800, acc: 0.687, loss: 0.704, lr: 0.0005618293162537221\n",
      "epoch: 7900, acc: 0.697, loss: 0.702, lr: 0.0005586904296329404\n",
      "epoch: 8000, acc: 0.697, loss: 0.701, lr: 0.0005555864214678594\n",
      "epoch: 8100, acc: 0.693, loss: 0.699, lr: 0.0005525167136305873\n",
      "epoch: 8200, acc: 0.700, loss: 0.698, lr: 0.0005494807407000385\n",
      "epoch: 8300, acc: 0.697, loss: 0.697, lr: 0.0005464779496147331\n",
      "epoch: 8400, acc: 0.693, loss: 0.695, lr: 0.0005435077993369205\n",
      "epoch: 8500, acc: 0.703, loss: 0.694, lr: 0.000540569760527596\n",
      "epoch: 8600, acc: 0.707, loss: 0.692, lr: 0.0005376633152320017\n",
      "epoch: 8700, acc: 0.707, loss: 0.691, lr: 0.0005347879565752179\n",
      "epoch: 8800, acc: 0.697, loss: 0.690, lr: 0.0005319431884674717\n",
      "epoch: 8900, acc: 0.703, loss: 0.689, lr: 0.0005291285253188\n",
      "epoch: 9000, acc: 0.707, loss: 0.687, lr: 0.0005263434917627244\n",
      "epoch: 9100, acc: 0.707, loss: 0.686, lr: 0.0005235876223886069\n",
      "epoch: 9200, acc: 0.693, loss: 0.685, lr: 0.0005208604614823689\n",
      "epoch: 9300, acc: 0.707, loss: 0.684, lr: 0.0005181615627752734\n",
      "epoch: 9400, acc: 0.703, loss: 0.682, lr: 0.0005154904892004743\n",
      "epoch: 9500, acc: 0.710, loss: 0.681, lr: 0.0005128468126570593\n",
      "epoch: 9600, acc: 0.713, loss: 0.680, lr: 0.0005102301137813153\n",
      "epoch: 9700, acc: 0.707, loss: 0.679, lr: 0.0005076399817249606\n",
      "epoch: 9800, acc: 0.703, loss: 0.678, lr: 0.0005050760139400979\n",
      "epoch: 9900, acc: 0.707, loss: 0.677, lr: 0.0005025378159706519\n",
      "epoch: 10000, acc: 0.717, loss: 0.676, lr: 0.0005000250012500625\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "dense1 = Layer_Dense(2,64)\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = Layer_Dense(64,3)\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "optimizer = Optimizer_RMSprop(decay = 1e-4)\n",
    "l = []\n",
    "ep = []\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "    # Forward pass\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "    loss = loss_activation.forward(dense2.output,y)\n",
    "\n",
    "    predictions = np.argmax(loss_activation.output, axis = 1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis = 1)\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    ep.append(epoch)\n",
    "    l.append(loss)\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' + \n",
    "              f'acc: {accuracy:.3f}, ' + \n",
    "              f'loss: {loss:.3f}, ' + \n",
    "              f'lr: {optimizer.current_learning_rate}')\n",
    "    \n",
    "    # Backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    \n",
    "    # Update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()\n",
    "plt.plot(ep,l)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are not the greatest, but we can slightly tweak the hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.347, loss: 1.099, lr: 0.01\n",
      "epoch: 100, acc: 0.420, loss: 0.989, lr: 0.009901970492127933\n",
      "epoch: 200, acc: 0.460, loss: 0.930, lr: 0.009804882831650161\n",
      "epoch: 300, acc: 0.543, loss: 0.903, lr: 0.009709680551509855\n",
      "epoch: 400, acc: 0.533, loss: 0.873, lr: 0.009616309260505818\n",
      "epoch: 500, acc: 0.550, loss: 0.854, lr: 0.00952471663967997\n",
      "epoch: 600, acc: 0.530, loss: 0.841, lr: 0.009434852344560807\n",
      "epoch: 700, acc: 0.563, loss: 0.829, lr: 0.009346667912889054\n",
      "epoch: 800, acc: 0.540, loss: 0.819, lr: 0.009260116677470134\n",
      "epoch: 900, acc: 0.537, loss: 0.812, lr: 0.009175153683824204\n",
      "epoch: 1000, acc: 0.553, loss: 0.802, lr: 0.009091735612328393\n",
      "epoch: 1100, acc: 0.577, loss: 0.793, lr: 0.009009820704567979\n",
      "epoch: 1200, acc: 0.590, loss: 0.779, lr: 0.00892936869363336\n",
      "epoch: 1300, acc: 0.590, loss: 0.774, lr: 0.008850340738118417\n",
      "epoch: 1400, acc: 0.613, loss: 0.767, lr: 0.008772699359592948\n",
      "epoch: 1500, acc: 0.597, loss: 0.764, lr: 0.008696408383337682\n",
      "epoch: 1600, acc: 0.623, loss: 0.760, lr: 0.008621432882145013\n",
      "epoch: 1700, acc: 0.627, loss: 0.754, lr: 0.008547739123001967\n",
      "epoch: 1800, acc: 0.647, loss: 0.750, lr: 0.008475294516484448\n",
      "epoch: 1900, acc: 0.613, loss: 0.749, lr: 0.008404067568703252\n",
      "epoch: 2000, acc: 0.640, loss: 0.745, lr: 0.008334027835652971\n",
      "epoch: 2100, acc: 0.657, loss: 0.742, lr: 0.008265145879824779\n",
      "epoch: 2200, acc: 0.660, loss: 0.742, lr: 0.008197393228953192\n",
      "epoch: 2300, acc: 0.667, loss: 0.733, lr: 0.008130742336775347\n",
      "epoch: 2400, acc: 0.633, loss: 0.733, lr: 0.00806516654568917\n",
      "epoch: 2500, acc: 0.647, loss: 0.731, lr: 0.008000640051204096\n",
      "epoch: 2600, acc: 0.613, loss: 0.739, lr: 0.007937137868084768\n",
      "epoch: 2700, acc: 0.653, loss: 0.729, lr: 0.007874635798094339\n",
      "epoch: 2800, acc: 0.677, loss: 0.726, lr: 0.00781311039924994\n",
      "epoch: 2900, acc: 0.667, loss: 0.723, lr: 0.007752538956508256\n",
      "epoch: 3000, acc: 0.680, loss: 0.723, lr: 0.007692899453804139\n",
      "epoch: 3100, acc: 0.670, loss: 0.720, lr: 0.007634170547370029\n",
      "epoch: 3200, acc: 0.647, loss: 0.724, lr: 0.007576331540268202\n",
      "epoch: 3300, acc: 0.650, loss: 0.724, lr: 0.007519362358072035\n",
      "epoch: 3400, acc: 0.667, loss: 0.717, lr: 0.007463243525636241\n",
      "epoch: 3500, acc: 0.637, loss: 0.720, lr: 0.007407956144899621\n",
      "epoch: 3600, acc: 0.650, loss: 0.719, lr: 0.0073534818736671805\n",
      "epoch: 3700, acc: 0.673, loss: 0.715, lr: 0.0072998029053215575\n",
      "epoch: 3800, acc: 0.677, loss: 0.713, lr: 0.007246901949416624\n",
      "epoch: 3900, acc: 0.670, loss: 0.714, lr: 0.007194762213108857\n",
      "epoch: 4000, acc: 0.653, loss: 0.718, lr: 0.007143367383384527\n",
      "epoch: 4100, acc: 0.647, loss: 0.695, lr: 0.007092701610043266\n",
      "epoch: 4200, acc: 0.663, loss: 0.679, lr: 0.007042749489400662\n",
      "epoch: 4300, acc: 0.663, loss: 0.672, lr: 0.006993496048674734\n",
      "epoch: 4400, acc: 0.680, loss: 0.668, lr: 0.006944926731022988\n",
      "epoch: 4500, acc: 0.703, loss: 0.662, lr: 0.006897027381198704\n",
      "epoch: 4600, acc: 0.663, loss: 0.658, lr: 0.006849784231796698\n",
      "epoch: 4700, acc: 0.670, loss: 0.658, lr: 0.006803183890060548\n",
      "epoch: 4800, acc: 0.673, loss: 0.655, lr: 0.006757213325224677\n",
      "epoch: 4900, acc: 0.687, loss: 0.652, lr: 0.006711859856366199\n",
      "epoch: 5000, acc: 0.660, loss: 0.657, lr: 0.006667111140742717\n",
      "epoch: 5100, acc: 0.680, loss: 0.645, lr: 0.0066229551625935495\n",
      "epoch: 5200, acc: 0.723, loss: 0.647, lr: 0.006579380222383051\n",
      "epoch: 5300, acc: 0.747, loss: 0.614, lr: 0.006536374926465783\n",
      "epoch: 5400, acc: 0.747, loss: 0.589, lr: 0.00649392817715436\n",
      "epoch: 5500, acc: 0.740, loss: 0.581, lr: 0.006452029163171818\n",
      "epoch: 5600, acc: 0.740, loss: 0.569, lr: 0.006410667350471184\n",
      "epoch: 5700, acc: 0.733, loss: 0.563, lr: 0.006369832473405949\n",
      "epoch: 5800, acc: 0.750, loss: 0.559, lr: 0.0063295145262358375\n",
      "epoch: 5900, acc: 0.753, loss: 0.550, lr: 0.0062897037549531415\n",
      "epoch: 6000, acc: 0.767, loss: 0.540, lr: 0.00625039064941559\n",
      "epoch: 6100, acc: 0.770, loss: 0.534, lr: 0.006211565935772407\n",
      "epoch: 6200, acc: 0.757, loss: 0.529, lr: 0.006173220569170938\n",
      "epoch: 6300, acc: 0.750, loss: 0.526, lr: 0.006135345726731701\n",
      "epoch: 6400, acc: 0.743, loss: 0.524, lr: 0.006097932800780536\n",
      "epoch: 6500, acc: 0.787, loss: 0.515, lr: 0.006060973392326807\n",
      "epoch: 6600, acc: 0.777, loss: 0.515, lr: 0.006024459304777396\n",
      "epoch: 6700, acc: 0.773, loss: 0.510, lr: 0.005988382537876519\n",
      "epoch: 6800, acc: 0.760, loss: 0.509, lr: 0.005952735281862016\n",
      "epoch: 6900, acc: 0.770, loss: 0.502, lr: 0.005917509911829102\n",
      "epoch: 7000, acc: 0.773, loss: 0.498, lr: 0.005882698982293076\n",
      "epoch: 7100, acc: 0.803, loss: 0.492, lr: 0.005848295221942803\n",
      "epoch: 7200, acc: 0.787, loss: 0.494, lr: 0.005814291528577244\n",
      "epoch: 7300, acc: 0.787, loss: 0.488, lr: 0.005780680964217585\n",
      "epoch: 7400, acc: 0.793, loss: 0.484, lr: 0.005747456750387954\n",
      "epoch: 7500, acc: 0.770, loss: 0.490, lr: 0.005714612263557918\n",
      "epoch: 7600, acc: 0.790, loss: 0.479, lr: 0.005682141030740383\n",
      "epoch: 7700, acc: 0.803, loss: 0.480, lr: 0.005650036725238714\n",
      "epoch: 7800, acc: 0.787, loss: 0.471, lr: 0.005618293162537221\n",
      "epoch: 7900, acc: 0.803, loss: 0.469, lr: 0.005586904296329404\n",
      "epoch: 8000, acc: 0.810, loss: 0.462, lr: 0.005555864214678593\n",
      "epoch: 8100, acc: 0.823, loss: 0.457, lr: 0.005525167136305873\n",
      "epoch: 8200, acc: 0.800, loss: 0.458, lr: 0.005494807407000385\n",
      "epoch: 8300, acc: 0.777, loss: 0.466, lr: 0.005464779496147331\n",
      "epoch: 8400, acc: 0.797, loss: 0.459, lr: 0.005435077993369205\n",
      "epoch: 8500, acc: 0.803, loss: 0.450, lr: 0.005405697605275961\n",
      "epoch: 8600, acc: 0.820, loss: 0.443, lr: 0.005376633152320017\n",
      "epoch: 8700, acc: 0.810, loss: 0.441, lr: 0.005347879565752179\n",
      "epoch: 8800, acc: 0.830, loss: 0.436, lr: 0.005319431884674717\n",
      "epoch: 8900, acc: 0.827, loss: 0.435, lr: 0.005291285253188\n",
      "epoch: 9000, acc: 0.823, loss: 0.432, lr: 0.005263434917627243\n",
      "epoch: 9100, acc: 0.823, loss: 0.430, lr: 0.005235876223886068\n",
      "epoch: 9200, acc: 0.827, loss: 0.427, lr: 0.005208604614823689\n",
      "epoch: 9300, acc: 0.840, loss: 0.424, lr: 0.005181615627752734\n",
      "epoch: 9400, acc: 0.833, loss: 0.422, lr: 0.005154904892004743\n",
      "epoch: 9500, acc: 0.840, loss: 0.420, lr: 0.005128468126570593\n",
      "epoch: 9600, acc: 0.833, loss: 0.419, lr: 0.005102301137813153\n",
      "epoch: 9700, acc: 0.840, loss: 0.417, lr: 0.005076399817249606\n",
      "epoch: 9800, acc: 0.833, loss: 0.415, lr: 0.005050760139400979\n",
      "epoch: 9900, acc: 0.837, loss: 0.416, lr: 0.0050253781597065185\n",
      "epoch: 10000, acc: 0.837, loss: 0.413, lr: 0.005000250012500626\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "dense1 = Layer_Dense(2,64)\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = Layer_Dense(64,3)\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "optimizer = Optimizer_RMSprop(learning_rate = 0.01, decay = 1e-4, rho = 0.999)\n",
    "l = []\n",
    "ep = []\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "    # Forward pass\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "    loss = loss_activation.forward(dense2.output,y)\n",
    "\n",
    "    predictions = np.argmax(loss_activation.output, axis = 1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis = 1)\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    ep.append(epoch)\n",
    "    l.append(loss)\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' + \n",
    "              f'acc: {accuracy:.3f}, ' + \n",
    "              f'loss: {loss:.3f}, ' + \n",
    "              f'lr: {optimizer.current_learning_rate}')\n",
    "    \n",
    "    # Backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    \n",
    "    # Update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()\n",
    "plt.plot(ep,l)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_Adam:\n",
    "    \n",
    "    def __init__(self, learning_rate = 0.001, decay = 0.0, epsilon = 1e-7, beta_1 = 0.9, beta_2 = 0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "        \n",
    "        \n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "            (1. / (1. + self.decay * self.iterations))\n",
    "            \n",
    "    def update_params(self, layer):\n",
    "        \n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "            \n",
    "        layer.weight_momentums = self.beta_1 * \\\n",
    "        layer.weight_momentums + \\\n",
    "        (1 - self.beta_1) * layer.dweights\n",
    "        layer.bias_momentums = self.beta_1 * \\\n",
    "        layer.bias_momentums + \\\n",
    "        (1 - self.beta_1) * layer.dbiases\n",
    "        \n",
    "        weight_momentums_corrected = layer.weight_momentums / \\\n",
    "        (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        bias_momentums_corrected = layer.bias_momentums / \\\n",
    "        (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        \n",
    "        layer.weight_cache = self.beta_2 * layer.weight_cache + \\\n",
    "        (1 - self.beta_2) * layer.dweights**2\n",
    "        layer.bias_cache = self.beta_2 * layer.bias_cache + \\\n",
    "        (1 - self.beta_2) * layer.dbiases**2\n",
    "        \n",
    "        weight_cache_corrected = layer.weight_cache / \\\n",
    "        (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        bias_cache_corrected = layer.bias_cache / \\\n",
    "        (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        \n",
    "        layer.weights += -self.current_learning_rate * \\\n",
    "        weight_momentums_corrected / \\\n",
    "        (np.sqrt(weight_cache_corrected) +\n",
    "        self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * \\\n",
    "        bias_momentums_corrected / \\\n",
    "        (np.sqrt(bias_cache_corrected) +\n",
    "        self.epsilon)\n",
    "        \n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to our main neural network code. We can now set our optimizer to Adam, run the code, and \n",
    "see what impact these changes had:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.240, loss: 1.099, lr: 0.02\n",
      "epoch: 100, acc: 0.587, loss: 0.919, lr: 0.019803940984255867\n",
      "epoch: 200, acc: 0.713, loss: 0.682, lr: 0.019609765663300322\n",
      "epoch: 300, acc: 0.750, loss: 0.570, lr: 0.01941936110301971\n",
      "epoch: 400, acc: 0.743, loss: 0.527, lr: 0.019232618521011637\n",
      "epoch: 500, acc: 0.807, loss: 0.472, lr: 0.01904943327935994\n",
      "epoch: 600, acc: 0.840, loss: 0.440, lr: 0.018869704689121615\n",
      "epoch: 700, acc: 0.820, loss: 0.416, lr: 0.018693335825778108\n",
      "epoch: 800, acc: 0.837, loss: 0.398, lr: 0.01852023335494027\n",
      "epoch: 900, acc: 0.843, loss: 0.374, lr: 0.018350307367648408\n",
      "epoch: 1000, acc: 0.843, loss: 0.360, lr: 0.018183471224656786\n",
      "epoch: 1100, acc: 0.860, loss: 0.341, lr: 0.018019641409135957\n",
      "epoch: 1200, acc: 0.857, loss: 0.326, lr: 0.01785873738726672\n",
      "epoch: 1300, acc: 0.883, loss: 0.314, lr: 0.017700681476236834\n",
      "epoch: 1400, acc: 0.883, loss: 0.301, lr: 0.017545398719185895\n",
      "epoch: 1500, acc: 0.863, loss: 0.295, lr: 0.017392816766675364\n",
      "epoch: 1600, acc: 0.897, loss: 0.281, lr: 0.017242865764290027\n",
      "epoch: 1700, acc: 0.890, loss: 0.270, lr: 0.017095478246003933\n",
      "epoch: 1800, acc: 0.897, loss: 0.260, lr: 0.016950589032968897\n",
      "epoch: 1900, acc: 0.897, loss: 0.254, lr: 0.016808135137406505\n",
      "epoch: 2000, acc: 0.900, loss: 0.248, lr: 0.016668055671305942\n",
      "epoch: 2100, acc: 0.900, loss: 0.245, lr: 0.016530291759649558\n",
      "epoch: 2200, acc: 0.903, loss: 0.239, lr: 0.016394786457906384\n",
      "epoch: 2300, acc: 0.907, loss: 0.235, lr: 0.016261484673550694\n",
      "epoch: 2400, acc: 0.900, loss: 0.233, lr: 0.01613033309137834\n",
      "epoch: 2500, acc: 0.907, loss: 0.229, lr: 0.016001280102408193\n",
      "epoch: 2600, acc: 0.907, loss: 0.225, lr: 0.015874275736169535\n",
      "epoch: 2700, acc: 0.910, loss: 0.222, lr: 0.015749271596188677\n",
      "epoch: 2800, acc: 0.910, loss: 0.219, lr: 0.01562622079849988\n",
      "epoch: 2900, acc: 0.917, loss: 0.216, lr: 0.015505077913016512\n",
      "epoch: 3000, acc: 0.903, loss: 0.217, lr: 0.015385798907608278\n",
      "epoch: 3100, acc: 0.910, loss: 0.212, lr: 0.015268341094740057\n",
      "epoch: 3200, acc: 0.917, loss: 0.208, lr: 0.015152663080536404\n",
      "epoch: 3300, acc: 0.913, loss: 0.208, lr: 0.01503872471614407\n",
      "epoch: 3400, acc: 0.913, loss: 0.205, lr: 0.014926487051272481\n",
      "epoch: 3500, acc: 0.913, loss: 0.203, lr: 0.014815912289799242\n",
      "epoch: 3600, acc: 0.913, loss: 0.202, lr: 0.014706963747334361\n",
      "epoch: 3700, acc: 0.913, loss: 0.201, lr: 0.014599605810643115\n",
      "epoch: 3800, acc: 0.913, loss: 0.199, lr: 0.014493803898833249\n",
      "epoch: 3900, acc: 0.917, loss: 0.197, lr: 0.014389524426217715\n",
      "epoch: 4000, acc: 0.917, loss: 0.196, lr: 0.014286734766769053\n",
      "epoch: 4100, acc: 0.917, loss: 0.195, lr: 0.014185403220086532\n",
      "epoch: 4200, acc: 0.917, loss: 0.194, lr: 0.014085498978801325\n",
      "epoch: 4300, acc: 0.917, loss: 0.193, lr: 0.013986992097349467\n",
      "epoch: 4400, acc: 0.913, loss: 0.192, lr: 0.013889853462045977\n",
      "epoch: 4500, acc: 0.913, loss: 0.192, lr: 0.013794054762397407\n",
      "epoch: 4600, acc: 0.910, loss: 0.192, lr: 0.013699568463593397\n",
      "epoch: 4700, acc: 0.913, loss: 0.190, lr: 0.013606367780121096\n",
      "epoch: 4800, acc: 0.917, loss: 0.190, lr: 0.013514426650449354\n",
      "epoch: 4900, acc: 0.913, loss: 0.189, lr: 0.013423719712732398\n",
      "epoch: 5000, acc: 0.913, loss: 0.187, lr: 0.013334222281485434\n",
      "epoch: 5100, acc: 0.917, loss: 0.187, lr: 0.013245910325187099\n",
      "epoch: 5200, acc: 0.913, loss: 0.186, lr: 0.013158760444766103\n",
      "epoch: 5300, acc: 0.910, loss: 0.186, lr: 0.013072749852931565\n",
      "epoch: 5400, acc: 0.913, loss: 0.185, lr: 0.01298785635430872\n",
      "epoch: 5500, acc: 0.917, loss: 0.184, lr: 0.012904058326343636\n",
      "epoch: 5600, acc: 0.913, loss: 0.184, lr: 0.012821334700942369\n",
      "epoch: 5700, acc: 0.917, loss: 0.183, lr: 0.012739664946811897\n",
      "epoch: 5800, acc: 0.913, loss: 0.183, lr: 0.012659029052471675\n",
      "epoch: 5900, acc: 0.913, loss: 0.182, lr: 0.012579407509906283\n",
      "epoch: 6000, acc: 0.917, loss: 0.183, lr: 0.01250078129883118\n",
      "epoch: 6100, acc: 0.913, loss: 0.181, lr: 0.012423131871544814\n",
      "epoch: 6200, acc: 0.907, loss: 0.179, lr: 0.012346441138341876\n",
      "epoch: 6300, acc: 0.917, loss: 0.176, lr: 0.012270691453463402\n",
      "epoch: 6400, acc: 0.920, loss: 0.176, lr: 0.012195865601561072\n",
      "epoch: 6500, acc: 0.913, loss: 0.175, lr: 0.012121946784653614\n",
      "epoch: 6600, acc: 0.913, loss: 0.174, lr: 0.012048918609554793\n",
      "epoch: 6700, acc: 0.917, loss: 0.173, lr: 0.011976765075753038\n",
      "epoch: 6800, acc: 0.917, loss: 0.173, lr: 0.011905470563724032\n",
      "epoch: 6900, acc: 0.917, loss: 0.172, lr: 0.011835019823658205\n",
      "epoch: 7000, acc: 0.917, loss: 0.172, lr: 0.011765397964586153\n",
      "epoch: 7100, acc: 0.917, loss: 0.171, lr: 0.011696590443885607\n",
      "epoch: 7200, acc: 0.920, loss: 0.172, lr: 0.011628583057154487\n",
      "epoch: 7300, acc: 0.917, loss: 0.171, lr: 0.01156136192843517\n",
      "epoch: 7400, acc: 0.917, loss: 0.170, lr: 0.011494913500775908\n",
      "epoch: 7500, acc: 0.917, loss: 0.170, lr: 0.011429224527115835\n",
      "epoch: 7600, acc: 0.917, loss: 0.169, lr: 0.011364282061480767\n",
      "epoch: 7700, acc: 0.917, loss: 0.170, lr: 0.011300073450477429\n",
      "epoch: 7800, acc: 0.920, loss: 0.170, lr: 0.011236586325074443\n",
      "epoch: 7900, acc: 0.913, loss: 0.168, lr: 0.011173808592658808\n",
      "epoch: 8000, acc: 0.917, loss: 0.169, lr: 0.011111728429357186\n",
      "epoch: 8100, acc: 0.920, loss: 0.168, lr: 0.011050334272611746\n",
      "epoch: 8200, acc: 0.913, loss: 0.167, lr: 0.01098961481400077\n",
      "epoch: 8300, acc: 0.913, loss: 0.167, lr: 0.010929558992294662\n",
      "epoch: 8400, acc: 0.913, loss: 0.167, lr: 0.01087015598673841\n",
      "epoch: 8500, acc: 0.913, loss: 0.166, lr: 0.010811395210551923\n",
      "epoch: 8600, acc: 0.913, loss: 0.166, lr: 0.010753266304640035\n",
      "epoch: 8700, acc: 0.920, loss: 0.166, lr: 0.010695759131504359\n",
      "epoch: 8800, acc: 0.917, loss: 0.166, lr: 0.010638863769349433\n",
      "epoch: 8900, acc: 0.923, loss: 0.166, lr: 0.010582570506376\n",
      "epoch: 9000, acc: 0.910, loss: 0.165, lr: 0.010526869835254487\n",
      "epoch: 9100, acc: 0.920, loss: 0.165, lr: 0.010471752447772136\n",
      "epoch: 9200, acc: 0.910, loss: 0.164, lr: 0.010417209229647378\n",
      "epoch: 9300, acc: 0.913, loss: 0.164, lr: 0.010363231255505468\n",
      "epoch: 9400, acc: 0.913, loss: 0.164, lr: 0.010309809784009485\n",
      "epoch: 9500, acc: 0.913, loss: 0.163, lr: 0.010256936253141186\n",
      "epoch: 9600, acc: 0.913, loss: 0.163, lr: 0.010204602275626307\n",
      "epoch: 9700, acc: 0.917, loss: 0.163, lr: 0.010152799634499212\n",
      "epoch: 9800, acc: 0.920, loss: 0.163, lr: 0.010101520278801958\n",
      "epoch: 9900, acc: 0.917, loss: 0.162, lr: 0.010050756319413037\n",
      "epoch: 10000, acc: 0.920, loss: 0.163, lr: 0.010000500025001252\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "dense1 = Layer_Dense(2,64)\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = Layer_Dense(64,3)\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "optimizer = Optimizer_Adam(learning_rate = 0.02, decay = 1e-4)\n",
    "l = []\n",
    "ep = []\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "    # Forward pass\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "    loss = loss_activation.forward(dense2.output,y)\n",
    "\n",
    "    predictions = np.argmax(loss_activation.output, axis = 1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis = 1)\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    ep.append(epoch)\n",
    "    l.append(loss)\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' + \n",
    "              f'acc: {accuracy:.3f}, ' + \n",
    "              f'loss: {loss:.3f}, ' + \n",
    "              f'lr: {optimizer.current_learning_rate}')\n",
    "    \n",
    "    # Backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    \n",
    "    # Update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()\n",
    "plt.plot(ep,l)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the best result so far, but let’s adjust the learning rate to be a bit higher, to 0.05 and change \n",
    "decay to 5e-7:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.343, loss: 1.099, lr: 0.05\n",
      "epoch: 100, acc: 0.630, loss: 0.836, lr: 0.04999752512250644\n",
      "epoch: 200, acc: 0.723, loss: 0.697, lr: 0.04999502549496326\n",
      "epoch: 300, acc: 0.780, loss: 0.604, lr: 0.049992526117345455\n",
      "epoch: 400, acc: 0.843, loss: 0.523, lr: 0.04999002698961558\n",
      "epoch: 500, acc: 0.833, loss: 0.476, lr: 0.049987528111736124\n",
      "epoch: 600, acc: 0.847, loss: 0.437, lr: 0.049985029483669646\n",
      "epoch: 700, acc: 0.857, loss: 0.401, lr: 0.049982531105378675\n",
      "epoch: 800, acc: 0.860, loss: 0.386, lr: 0.04998003297682575\n",
      "epoch: 900, acc: 0.873, loss: 0.367, lr: 0.049977535097973466\n",
      "epoch: 1000, acc: 0.887, loss: 0.352, lr: 0.049975037468784345\n",
      "epoch: 1100, acc: 0.867, loss: 0.343, lr: 0.049972540089220974\n",
      "epoch: 1200, acc: 0.887, loss: 0.334, lr: 0.04997004295924593\n",
      "epoch: 1300, acc: 0.870, loss: 0.326, lr: 0.04996754607882181\n",
      "epoch: 1400, acc: 0.880, loss: 0.329, lr: 0.049965049447911185\n",
      "epoch: 1500, acc: 0.883, loss: 0.314, lr: 0.04996255306647668\n",
      "epoch: 1600, acc: 0.867, loss: 0.376, lr: 0.049960056934480884\n",
      "epoch: 1700, acc: 0.883, loss: 0.302, lr: 0.04995756105188642\n",
      "epoch: 1800, acc: 0.883, loss: 0.311, lr: 0.049955065418655915\n",
      "epoch: 1900, acc: 0.883, loss: 0.293, lr: 0.04995257003475201\n",
      "epoch: 2000, acc: 0.887, loss: 0.295, lr: 0.04995007490013731\n",
      "epoch: 2100, acc: 0.877, loss: 0.308, lr: 0.0499475800147745\n",
      "epoch: 2200, acc: 0.887, loss: 0.283, lr: 0.0499450853786262\n",
      "epoch: 2300, acc: 0.890, loss: 0.279, lr: 0.0499425909916551\n",
      "epoch: 2400, acc: 0.893, loss: 0.283, lr: 0.04994009685382384\n",
      "epoch: 2500, acc: 0.797, loss: 0.452, lr: 0.04993760296509512\n",
      "epoch: 2600, acc: 0.890, loss: 0.268, lr: 0.049935109325431604\n",
      "epoch: 2700, acc: 0.890, loss: 0.266, lr: 0.049932615934796004\n",
      "epoch: 2800, acc: 0.890, loss: 0.264, lr: 0.04993012279315098\n",
      "epoch: 2900, acc: 0.897, loss: 0.262, lr: 0.049927629900459285\n",
      "epoch: 3000, acc: 0.893, loss: 0.261, lr: 0.049925137256683606\n",
      "epoch: 3100, acc: 0.890, loss: 0.258, lr: 0.04992264486178666\n",
      "epoch: 3200, acc: 0.887, loss: 0.258, lr: 0.04992015271573119\n",
      "epoch: 3300, acc: 0.903, loss: 0.261, lr: 0.04991766081847992\n",
      "epoch: 3400, acc: 0.890, loss: 0.254, lr: 0.049915169169995596\n",
      "epoch: 3500, acc: 0.890, loss: 0.252, lr: 0.049912677770240964\n",
      "epoch: 3600, acc: 0.907, loss: 0.253, lr: 0.049910186619178794\n",
      "epoch: 3700, acc: 0.893, loss: 0.256, lr: 0.04990769571677183\n",
      "epoch: 3800, acc: 0.897, loss: 0.244, lr: 0.04990520506298287\n",
      "epoch: 3900, acc: 0.907, loss: 0.249, lr: 0.04990271465777467\n",
      "epoch: 4000, acc: 0.887, loss: 0.244, lr: 0.049900224501110035\n",
      "epoch: 4100, acc: 0.890, loss: 0.246, lr: 0.04989773459295174\n",
      "epoch: 4200, acc: 0.903, loss: 0.239, lr: 0.04989524493326262\n",
      "epoch: 4300, acc: 0.887, loss: 0.236, lr: 0.04989275552200545\n",
      "epoch: 4400, acc: 0.913, loss: 0.233, lr: 0.04989026635914307\n",
      "epoch: 4500, acc: 0.907, loss: 0.230, lr: 0.04988777744463829\n",
      "epoch: 4600, acc: 0.907, loss: 0.227, lr: 0.049885288778453954\n",
      "epoch: 4700, acc: 0.903, loss: 0.226, lr: 0.049882800360552884\n",
      "epoch: 4800, acc: 0.893, loss: 0.248, lr: 0.04988031219089794\n",
      "epoch: 4900, acc: 0.907, loss: 0.222, lr: 0.049877824269451976\n",
      "epoch: 5000, acc: 0.910, loss: 0.222, lr: 0.04987533659617785\n",
      "epoch: 5100, acc: 0.910, loss: 0.230, lr: 0.04987284917103844\n",
      "epoch: 5200, acc: 0.907, loss: 0.227, lr: 0.04987036199399661\n",
      "epoch: 5300, acc: 0.770, loss: 0.634, lr: 0.04986787506501525\n",
      "epoch: 5400, acc: 0.907, loss: 0.216, lr: 0.04986538838405724\n",
      "epoch: 5500, acc: 0.910, loss: 0.213, lr: 0.049862901951085496\n",
      "epoch: 5600, acc: 0.910, loss: 0.211, lr: 0.049860415766062906\n",
      "epoch: 5700, acc: 0.913, loss: 0.210, lr: 0.0498579298289524\n",
      "epoch: 5800, acc: 0.913, loss: 0.208, lr: 0.04985544413971689\n",
      "epoch: 5900, acc: 0.910, loss: 0.206, lr: 0.049852958698319315\n",
      "epoch: 6000, acc: 0.913, loss: 0.205, lr: 0.04985047350472258\n",
      "epoch: 6100, acc: 0.913, loss: 0.209, lr: 0.04984798855888967\n",
      "epoch: 6200, acc: 0.913, loss: 0.202, lr: 0.049845503860783506\n",
      "epoch: 6300, acc: 0.910, loss: 0.230, lr: 0.049843019410367055\n",
      "epoch: 6400, acc: 0.907, loss: 0.219, lr: 0.04984053520760327\n",
      "epoch: 6500, acc: 0.913, loss: 0.202, lr: 0.049838051252455155\n",
      "epoch: 6600, acc: 0.920, loss: 0.196, lr: 0.049835567544885655\n",
      "epoch: 6700, acc: 0.913, loss: 0.203, lr: 0.04983308408485778\n",
      "epoch: 6800, acc: 0.920, loss: 0.193, lr: 0.0498306008723345\n",
      "epoch: 6900, acc: 0.917, loss: 0.191, lr: 0.04982811790727884\n",
      "epoch: 7000, acc: 0.913, loss: 0.197, lr: 0.04982563518965381\n",
      "epoch: 7100, acc: 0.913, loss: 0.210, lr: 0.049823152719422406\n",
      "epoch: 7200, acc: 0.917, loss: 0.199, lr: 0.049820670496547675\n",
      "epoch: 7300, acc: 0.917, loss: 0.192, lr: 0.04981818852099264\n",
      "epoch: 7400, acc: 0.917, loss: 0.187, lr: 0.049815706792720335\n",
      "epoch: 7500, acc: 0.920, loss: 0.186, lr: 0.0498132253116938\n",
      "epoch: 7600, acc: 0.920, loss: 0.185, lr: 0.04981074407787611\n",
      "epoch: 7700, acc: 0.920, loss: 0.184, lr: 0.049808263091230306\n",
      "epoch: 7800, acc: 0.927, loss: 0.183, lr: 0.04980578235171948\n",
      "epoch: 7900, acc: 0.923, loss: 0.182, lr: 0.04980330185930667\n",
      "epoch: 8000, acc: 0.923, loss: 0.181, lr: 0.04980082161395499\n",
      "epoch: 8100, acc: 0.923, loss: 0.180, lr: 0.04979834161562752\n",
      "epoch: 8200, acc: 0.923, loss: 0.179, lr: 0.04979586186428736\n",
      "epoch: 8300, acc: 0.930, loss: 0.178, lr: 0.04979338235989761\n",
      "epoch: 8400, acc: 0.930, loss: 0.176, lr: 0.04979090310242139\n",
      "epoch: 8500, acc: 0.923, loss: 0.177, lr: 0.049788424091821805\n",
      "epoch: 8600, acc: 0.923, loss: 0.176, lr: 0.049785945328062006\n",
      "epoch: 8700, acc: 0.933, loss: 0.172, lr: 0.0497834668111051\n",
      "epoch: 8800, acc: 0.910, loss: 0.197, lr: 0.049780988540914256\n",
      "epoch: 8900, acc: 0.923, loss: 0.171, lr: 0.0497785105174526\n",
      "epoch: 9000, acc: 0.913, loss: 0.188, lr: 0.04977603274068329\n",
      "epoch: 9100, acc: 0.930, loss: 0.169, lr: 0.04977355521056952\n",
      "epoch: 9200, acc: 0.917, loss: 0.170, lr: 0.049771077927074414\n",
      "epoch: 9300, acc: 0.877, loss: 0.251, lr: 0.0497686008901612\n",
      "epoch: 9400, acc: 0.940, loss: 0.167, lr: 0.04976612409979302\n",
      "epoch: 9500, acc: 0.937, loss: 0.165, lr: 0.0497636475559331\n",
      "epoch: 9600, acc: 0.937, loss: 0.164, lr: 0.049761171258544616\n",
      "epoch: 9700, acc: 0.943, loss: 0.163, lr: 0.0497586952075908\n",
      "epoch: 9800, acc: 0.943, loss: 0.162, lr: 0.04975621940303483\n",
      "epoch: 9900, acc: 0.943, loss: 0.161, lr: 0.049753743844839965\n",
      "epoch: 10000, acc: 0.947, loss: 0.160, lr: 0.04975126853296942\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "dense1 = Layer_Dense(2,64)\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = Layer_Dense(64,3)\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "optimizer = Optimizer_Adam(learning_rate = 0.05, decay = 5e-7)\n",
    "l = []\n",
    "ep = []\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "    # Forward pass\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "    loss = loss_activation.forward(dense2.output,y)\n",
    "\n",
    "    predictions = np.argmax(loss_activation.output, axis = 1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis = 1)\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    ep.append(epoch)\n",
    "    l.append(loss)\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' + \n",
    "              f'acc: {accuracy:.3f}, ' + \n",
    "              f'loss: {loss:.3f}, ' + \n",
    "              f'lr: {optimizer.current_learning_rate}')\n",
    "    \n",
    "    # Backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    \n",
    "    # Update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()\n",
    "plt.plot(ep,l)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full code up to this point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "nnfs.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense Layer\n",
    "class Layer_Dense:\n",
    "    \n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1,n_neurons))\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs,self.weights) + self.biases\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis = 0, keepdims = True)\n",
    "        \n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU Activtion\n",
    "class Activation_ReLU:\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[self.inputs <= 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax Activation\n",
    "class Activation_Softmax:\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        \n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis = 1, keepdims = True))\n",
    "        probabilities = exp_values / np.sum(exp_values, axis = 1, keepdims = True)\n",
    "        \n",
    "        self.output = probabilities\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "        \n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
    "            single_output = single_output.reshape(-1,1)\n",
    "            jacobian_matrix = np.diagflat(signle_output) - np.dot(single_output, single_input.T)\n",
    "            \n",
    "            self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD Optimizer\n",
    "class Optimizer_SGD:\n",
    "    \n",
    "    def __init__(self, learning_rate = 1.0, decay = 0.0, momentum = 0.0):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "    \n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1.0 / (1.0 + self.decay * self.iterations))\n",
    "            \n",
    "    def update_params(self, layer):\n",
    "        if self.momentum:\n",
    "            if not hasattr(layer, 'weight_momentums'):\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "                \n",
    "                \n",
    "            weight_updates = self.momentum * layer.weight_momentums - self.current_learning_rate * layer.dweights\n",
    "            layer.weight_momentums = weight_updates\n",
    "            \n",
    "            bias_updates = self.momentum * layer.bias_momentums - self.current_learning_rate * layer.dbiases\n",
    "            layer.bias_momentums = bias_updates\n",
    "            \n",
    "        else:\n",
    "            weight_updates  = -self.current_learning_rate * layer.dweights\n",
    "            bias_updates = -self.current_learning_rate * layer.dbaises\n",
    "            \n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "        \n",
    "    \n",
    "    def post_params(self):\n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adagrad Optimizer\n",
    "class Optimizer_Adagrad:\n",
    "    \n",
    "    def __init__(self, learning_rate = 1.0, decay = 0.0, epsilon = 1e-7):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        \n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1.0 / (1.0 + self.decay * self.iterations))\n",
    "    \n",
    "    \n",
    "    def update_params(self,layer):\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "            \n",
    "        layer.weight_cache += layer.dweights**2\n",
    "        layer.bias_cache += layer.dbaises**2\n",
    "        \n",
    "        layer.weights += -self.current_learning_rate * layer.dweights / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * layer.dbiases / (np.sqrt(layer.bais_cache) + self.epsilon)\n",
    "        \n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSprop optimizer\n",
    "class Optimizer_RMSprop:\n",
    "    \n",
    "    def __init__(self, learning_rate = 0.001, decay = 0.0, epsilon = 1e-7, rho = 0.9):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.rho = rho\n",
    "        \n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1.0 / (1.0 + self.decay * self.iterations))\n",
    "            \n",
    "    def update_params(self, layer):\n",
    "        \n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bais_cache =  np.zeros_like(layer.biases)\n",
    "            \n",
    "        layer.weight_cache = self.rho * layer.weight_cache + (1 - self.rho) * layer.dweights**2\n",
    "        layer.bias_cache = self.rho * layer.bias_cache + (1 - self.rho) * layer.dbiases**2\n",
    "        \n",
    "        layer.weights += -self.current_learning_rate * layer.dweights / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += -slef.current_learning_rate * layer.dbiases / (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "        \n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_Adam:\n",
    "    \n",
    "    def __init__(self, learning_rate = 0.001, decay = 0.0, epsilon = 1e-7, beta_1 = 0.9, beta_2 = 0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "        \n",
    "        \n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "            (1. / (1. + self.decay * self.iterations))\n",
    "            \n",
    "    def update_params(self, layer):\n",
    "        \n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "            \n",
    "        layer.weight_momentums = self.beta_1 * \\\n",
    "        layer.weight_momentums + \\\n",
    "        (1 - self.beta_1) * layer.dweights\n",
    "        layer.bias_momentums = self.beta_1 * \\\n",
    "        layer.bias_momentums + \\\n",
    "        (1 - self.beta_1) * layer.dbiases\n",
    "        \n",
    "        weight_momentums_corrected = layer.weight_momentums / \\\n",
    "        (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        bias_momentums_corrected = layer.bias_momentums / \\\n",
    "        (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        \n",
    "        layer.weight_cache = self.beta_2 * layer.weight_cache + \\\n",
    "        (1 - self.beta_2) * layer.dweights**2\n",
    "        layer.bias_cache = self.beta_2 * layer.bias_cache + \\\n",
    "        (1 - self.beta_2) * layer.dbiases**2\n",
    "        \n",
    "        weight_cache_corrected = layer.weight_cache / \\\n",
    "        (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        bias_cache_corrected = layer.bias_cache / \\\n",
    "        (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        \n",
    "        layer.weights += -self.current_learning_rate * \\\n",
    "        weight_momentums_corrected / \\\n",
    "        (np.sqrt(weight_cache_corrected) +\n",
    "        self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * \\\n",
    "        bias_momentums_corrected / \\\n",
    "        (np.sqrt(bias_cache_corrected) +\n",
    "        self.epsilon)\n",
    "        \n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common Loss Class\n",
    "class Loss:\n",
    "    \n",
    "    def calculate(self, output, y):\n",
    "        sample_losses = self.forward(output, y)\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        return data_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Entropy Loss\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        samples = len(y_pred)\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        \n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[\n",
    "                range(samples),\n",
    "                y_true\n",
    "            ]\n",
    "        \n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(\n",
    "                y_pred_clipped * y_true,\n",
    "                axis = 1\n",
    "            )\n",
    "            \n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "    \n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "        lables = len(dvalues)\n",
    "        \n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(lables)[y_true]\n",
    "            \n",
    "        self.dinputs = - y_true / dvalues\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax And Cross-entropy combined for faster backward step\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "        \n",
    "    def forward(self, inputs, y_true):\n",
    "        self.activation.forward(inputs)\n",
    "        self.output = self.activation.output\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "        \n",
    "        if len(y_true) == 2:\n",
    "            y_true = np.argmax(y_true, axis = 1)\n",
    "        \n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.320, loss: 1.099, lr: 0.05\n",
      "epoch: 100, acc: 0.647, loss: 0.791, lr: 0.04999901001960161\n",
      "epoch: 200, acc: 0.723, loss: 0.591, lr: 0.04999801007919885\n",
      "epoch: 300, acc: 0.767, loss: 0.504, lr: 0.04999701017879131\n",
      "epoch: 400, acc: 0.787, loss: 0.463, lr: 0.049996010318376596\n",
      "epoch: 500, acc: 0.793, loss: 0.429, lr: 0.0499950104979523\n",
      "epoch: 600, acc: 0.830, loss: 0.401, lr: 0.049994010717516045\n",
      "epoch: 700, acc: 0.820, loss: 0.383, lr: 0.04999301097706541\n",
      "epoch: 800, acc: 0.840, loss: 0.355, lr: 0.049992011276598\n",
      "epoch: 900, acc: 0.833, loss: 0.347, lr: 0.04999101161611143\n",
      "epoch: 1000, acc: 0.873, loss: 0.336, lr: 0.049990011995603274\n",
      "epoch: 1100, acc: 0.877, loss: 0.321, lr: 0.04998901241507117\n",
      "epoch: 1200, acc: 0.883, loss: 0.298, lr: 0.049988012874512704\n",
      "epoch: 1300, acc: 0.887, loss: 0.298, lr: 0.04998701337392546\n",
      "epoch: 1400, acc: 0.873, loss: 0.285, lr: 0.049986013913307065\n",
      "epoch: 1500, acc: 0.887, loss: 0.280, lr: 0.049985014492655105\n",
      "epoch: 1600, acc: 0.887, loss: 0.280, lr: 0.0499840151119672\n",
      "epoch: 1700, acc: 0.890, loss: 0.260, lr: 0.04998301577124094\n",
      "epoch: 1800, acc: 0.890, loss: 0.264, lr: 0.04998201647047393\n",
      "epoch: 1900, acc: 0.893, loss: 0.247, lr: 0.049981017209663775\n",
      "epoch: 2000, acc: 0.870, loss: 0.261, lr: 0.04998001798880808\n",
      "epoch: 2100, acc: 0.927, loss: 0.231, lr: 0.049979018807904445\n",
      "epoch: 2200, acc: 0.900, loss: 0.245, lr: 0.04997801966695047\n",
      "epoch: 2300, acc: 0.893, loss: 0.246, lr: 0.04997702056594378\n",
      "epoch: 2400, acc: 0.930, loss: 0.219, lr: 0.049976021504881965\n",
      "epoch: 2500, acc: 0.923, loss: 0.224, lr: 0.04997502248376262\n",
      "epoch: 2600, acc: 0.927, loss: 0.214, lr: 0.04997402350258336\n",
      "epoch: 2700, acc: 0.927, loss: 0.211, lr: 0.04997302456134178\n",
      "epoch: 2800, acc: 0.930, loss: 0.208, lr: 0.049972025660035516\n",
      "epoch: 2900, acc: 0.923, loss: 0.214, lr: 0.04997102679866214\n",
      "epoch: 3000, acc: 0.923, loss: 0.210, lr: 0.049970027977219264\n",
      "epoch: 3100, acc: 0.927, loss: 0.198, lr: 0.049969029195704506\n",
      "epoch: 3200, acc: 0.920, loss: 0.204, lr: 0.04996803045411546\n",
      "epoch: 3300, acc: 0.940, loss: 0.195, lr: 0.049967031752449736\n",
      "epoch: 3400, acc: 0.930, loss: 0.197, lr: 0.049966033090704944\n",
      "epoch: 3500, acc: 0.930, loss: 0.191, lr: 0.04996503446887868\n",
      "epoch: 3600, acc: 0.933, loss: 0.187, lr: 0.049964035886968565\n",
      "epoch: 3700, acc: 0.933, loss: 0.182, lr: 0.04996303734497219\n",
      "epoch: 3800, acc: 0.887, loss: 0.271, lr: 0.04996203884288718\n",
      "epoch: 3900, acc: 0.940, loss: 0.179, lr: 0.049961040380711114\n",
      "epoch: 4000, acc: 0.940, loss: 0.176, lr: 0.04996004195844164\n",
      "epoch: 4100, acc: 0.943, loss: 0.175, lr: 0.049959043576076334\n",
      "epoch: 4200, acc: 0.943, loss: 0.173, lr: 0.04995804523361281\n",
      "epoch: 4300, acc: 0.940, loss: 0.168, lr: 0.04995704693104869\n",
      "epoch: 4400, acc: 0.940, loss: 0.166, lr: 0.049956048668381554\n",
      "epoch: 4500, acc: 0.940, loss: 0.163, lr: 0.049955050445609044\n",
      "epoch: 4600, acc: 0.933, loss: 0.164, lr: 0.049954052262728754\n",
      "epoch: 4700, acc: 0.943, loss: 0.162, lr: 0.04995305411973827\n",
      "epoch: 4800, acc: 0.943, loss: 0.159, lr: 0.04995205601663524\n",
      "epoch: 4900, acc: 0.907, loss: 0.215, lr: 0.04995105795341724\n",
      "epoch: 5000, acc: 0.913, loss: 0.187, lr: 0.049950059930081905\n",
      "epoch: 5100, acc: 0.947, loss: 0.155, lr: 0.049949061946626835\n",
      "epoch: 5200, acc: 0.937, loss: 0.155, lr: 0.04994806400304963\n",
      "epoch: 5300, acc: 0.947, loss: 0.152, lr: 0.04994706609934792\n",
      "epoch: 5400, acc: 0.940, loss: 0.152, lr: 0.04994606823551928\n",
      "epoch: 5500, acc: 0.947, loss: 0.150, lr: 0.04994507041156137\n",
      "epoch: 5600, acc: 0.947, loss: 0.152, lr: 0.04994407262747177\n",
      "epoch: 5700, acc: 0.943, loss: 0.156, lr: 0.049943074883248076\n",
      "epoch: 5800, acc: 0.947, loss: 0.147, lr: 0.049942077178887934\n",
      "epoch: 5900, acc: 0.940, loss: 0.174, lr: 0.04994107951438892\n",
      "epoch: 6000, acc: 0.947, loss: 0.146, lr: 0.04994008188974869\n",
      "epoch: 6100, acc: 0.943, loss: 0.144, lr: 0.0499390843049648\n",
      "epoch: 6200, acc: 0.920, loss: 0.158, lr: 0.04993808676003491\n",
      "epoch: 6300, acc: 0.950, loss: 0.146, lr: 0.04993708925495661\n",
      "epoch: 6400, acc: 0.947, loss: 0.146, lr: 0.04993609178972751\n",
      "epoch: 6500, acc: 0.947, loss: 0.140, lr: 0.049935094364345234\n",
      "epoch: 6600, acc: 0.913, loss: 0.179, lr: 0.049934096978807366\n",
      "epoch: 6700, acc: 0.940, loss: 0.157, lr: 0.04993309963311156\n",
      "epoch: 6800, acc: 0.953, loss: 0.137, lr: 0.0499321023272554\n",
      "epoch: 6900, acc: 0.947, loss: 0.138, lr: 0.049931105061236505\n",
      "epoch: 7000, acc: 0.940, loss: 0.143, lr: 0.0499301078350525\n",
      "epoch: 7100, acc: 0.947, loss: 0.138, lr: 0.049929110648700974\n",
      "epoch: 7200, acc: 0.940, loss: 0.145, lr: 0.049928113502179565\n",
      "epoch: 7300, acc: 0.947, loss: 0.137, lr: 0.04992711639548588\n",
      "epoch: 7400, acc: 0.940, loss: 0.147, lr: 0.04992611932861751\n",
      "epoch: 7500, acc: 0.953, loss: 0.134, lr: 0.04992512230157211\n",
      "epoch: 7600, acc: 0.950, loss: 0.135, lr: 0.049924125314347256\n",
      "epoch: 7700, acc: 0.947, loss: 0.133, lr: 0.049923128366940586\n",
      "epoch: 7800, acc: 0.953, loss: 0.132, lr: 0.04992213145934971\n",
      "epoch: 7900, acc: 0.950, loss: 0.133, lr: 0.049921134591572235\n",
      "epoch: 8000, acc: 0.943, loss: 0.131, lr: 0.04992013776360579\n",
      "epoch: 8100, acc: 0.947, loss: 0.136, lr: 0.04991914097544797\n",
      "epoch: 8200, acc: 0.950, loss: 0.135, lr: 0.049918144227096416\n",
      "epoch: 8300, acc: 0.920, loss: 0.179, lr: 0.04991714751854871\n",
      "epoch: 8400, acc: 0.947, loss: 0.130, lr: 0.0499161508498025\n",
      "epoch: 8500, acc: 0.943, loss: 0.138, lr: 0.0499151542208554\n",
      "epoch: 8600, acc: 0.937, loss: 0.137, lr: 0.049914157631704995\n",
      "epoch: 8700, acc: 0.937, loss: 0.134, lr: 0.04991316108234894\n",
      "epoch: 8800, acc: 0.947, loss: 0.127, lr: 0.04991216457278481\n",
      "epoch: 8900, acc: 0.947, loss: 0.135, lr: 0.049911168103010266\n",
      "epoch: 9000, acc: 0.847, loss: 0.623, lr: 0.0499101716730229\n",
      "epoch: 9100, acc: 0.950, loss: 0.130, lr: 0.04990917528282032\n",
      "epoch: 9200, acc: 0.950, loss: 0.126, lr: 0.049908178932400175\n",
      "epoch: 9300, acc: 0.950, loss: 0.124, lr: 0.049907182621760054\n",
      "epoch: 9400, acc: 0.957, loss: 0.123, lr: 0.04990618635089759\n",
      "epoch: 9500, acc: 0.957, loss: 0.123, lr: 0.04990519011981039\n",
      "epoch: 9600, acc: 0.960, loss: 0.122, lr: 0.04990419392849607\n",
      "epoch: 9700, acc: 0.960, loss: 0.122, lr: 0.04990319777695228\n",
      "epoch: 9800, acc: 0.960, loss: 0.121, lr: 0.04990220166517659\n",
      "epoch: 9900, acc: 0.960, loss: 0.121, lr: 0.04990120559316665\n",
      "epoch: 10000, acc: 0.943, loss: 0.147, lr: 0.04990020956092007\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAAsTAAALEwEAmpwYAAAf+klEQVR4nO3dd3gc9b3v8fd31WxL7pJ7xwZsUwzoEDsQSigxNg8+J+HcmOQCIYATEh4gJCeXFkK4yYHADSGUE+IAARJqqIYYHAgOLWAs947lgi1ZtiSrd+3u7/6xIyHJkrWSV17v6PN6nn08O/PT7Hc08mdnZ37zW3POISIiiS8Q7wJERCQ2FOgiIj6hQBcR8QkFuoiITyjQRUR8QoEuIuITnQa6mfUxs0/NbI2ZbTCzX7TTJs3MnjezXDNbZmYTeqRaERHpUDRH6PXAV51zJwIzgNlmNrNNmyuBUufcZOC3wK9jWqWIiHSq00B3EVXe0xTv0fZupHnAk970i8A5ZmYxq1JERDqVHE0jM0sCVgCTgYedc8vaNBkN7AZwzgXNrBwYChS3Wc8CYAFAenr6Kccee+yhVS8i0susWLGi2DmX1d6yqALdORcCZpjZIOAVMzvOObe+q4U45xYCCwGys7NdTk5OV1chItKrmdnnHS3rUi8X51wZsBSY3WZRPjDWe7FkYCCwv0tViojIIYmml0uWd2SOmfUFzgM2t2m2CLjcm74YeNdp1C8RkcMqmlMuI4EnvfPoAeAF59wbZnYnkOOcWwQ8BvzZzHKBEmB+j1UsIiLt6jTQnXNrgZPamX97i+k64D9jW5qIiHSF7hQVEfEJBbqIiE8o0EVEfEKBLiK9zua9FeTsLIl3GTEX1Y1FIiJ+Mvv+DwDYeffcOFcSWzpCFxHxCQW6iIhPKNBFRHxCgS4i4hMKdBERn1Cgi4j4hAJdRMQnFOgiIj6hQBcR8QkFuoiITyjQRUR8QoEuIuITCnQREZ9QoIuI+IQCXUTEJxToIiI+oUAXEfEJBbqIiE8o0EVEfEKBLiLiEwp0ERGfUKCLiPhEp4FuZmPNbKmZbTSzDWZ2fTttzjKzcjNb7T1u75lyRUSkI8lRtAkCP3bOrTSz/sAKM3vbObexTbsPnHMXxr5EERGJRqdH6M65AufcSm+6EtgEjO7pwkREpGu6dA7dzCYAJwHL2lk8y8zWmNmbZja9g59fYGY5ZpZTVFTU9WpFRKRDUQe6mWUALwE3OOcq2ixeCYx3zp0IPAi82t46nHMLnXPZzrnsrKysbpYsIiLtiSrQzSyFSJg/7Zx7ue1y51yFc67Km14MpJhZZkwrFRGRg4qml4sBjwGbnHP3ddBmhNcOMzvVW+/+WBYqIiIHF00vl9OAS4F1Zrbam3cLMA7AOfcIcDFwjZkFgVpgvnPOxb5cERHpSKeB7pz7ELBO2jwEPBSrokREpOt0p6iIiE8o0EVEfEKBLiLiEwp0ERGfUKCLiPiEAl1ExCcU6CIiPqFAFxHxCQW6iIhPKNBFRHxCgS4i4hMKdBERn1Cgi4j4hAJdRMQnFOgiIj6hQBcR8QkFuoiITyjQRUR8QoEuIuITCnQREZ9QoIuI+IQCXUTEJxToIiI+oUAXEfEJBbqIiE8o0EVEfEKBLiLiE50GupmNNbOlZrbRzDaY2fXttDEze8DMcs1srZmd3DPliohIR5KjaBMEfuycW2lm/YEVZva2c25jizYXAFO8x5eA33v/iojIYdLpEbpzrsA5t9KbrgQ2AaPbNJsHPOUiPgEGmdnImFcrIiId6tI5dDObAJwELGuzaDSwu8XzPA4MfcxsgZnlmFlOUVFRF0sVEZGDiTrQzSwDeAm4wTlX0Z0Xc84tdM5lO+eys7KyurMKERHpQFSBbmYpRML8aefcy+00yQfGtng+xpsnIiKHSTS9XAx4DNjknLuvg2aLgMu83i4zgXLnXEEM6xQRkU5E08vlNOBSYJ2Zrfbm3QKMA3DOPQIsBuYAuUANcEXMKxURkYPqNNCdcx8C1kkbB/wwVkWJiEjX6U5RERGfUKCLiPiEAl1ExCcU6CIiPqFAFxHxCQW6iIhPKNBFRHxCgS4i4hMKdBERn1Cgi4j4hAJdRMQnFOgiIj6hQBcR8QkFuoiITyjQRUR8QoEuIuITCnQREZ9QoIuI+IQCXUTEJxToIiI+oUAXEfEJBbqIiE8o0EVEfEKBLiLiEwp0ERGfUKCLSK/lnIt3CTGlQBcR8YlOA93MHjezQjNb38Hys8ys3MxWe4/bY1+miIh0JjmKNk8ADwFPHaTNB865C2NSkYjIYeIcmMW7itjp9AjdOfc+UHIYahERkUMQq3Pos8xsjZm9aWbTY7ROERHpgmhOuXRmJTDeOVdlZnOAV4Ep7TU0swXAAoBx48bF4KVFRLrPX31cYnCE7pyrcM5VedOLgRQzy+yg7ULnXLZzLjsrK+tQX1pERFo45EA3sxFmkcsKZnaqt879h7peERHpmk5PuZjZs8BZQKaZ5QE/B1IAnHOPABcD15hZEKgF5ju/9dYXEV+KRJV/url0GujOuUs6Wf4QkW6NIiISR7pTVOLmllfWMfv+9+NdhvRifjuVEIteLiLd8syyXfEuQcRXdIQuIuITCnQR6bX81n1DgS4i4hMKdBERn1Cgi0iv5XzWz0WBLiLiEwp0ERGfUKCLSK+lXi4iInJEUqCLiPiEAl1ExCcU6CIiPqFAFxHxCQW6iPRa6uUiIiJHJAW6iIhPKNBFpNfSWC4iInJEUqCLiPiEAl1Eei31comzwso63lq/l9qGULxLERE5oiRcoC/fUcr3/7KCnfur412KiCQ4nx2gJ16gjxzUB4CC8to4VyIiiS5nZ0m8S4iphAv0UQP7ArCnrC7OlUisbCuqincJ0ks1hvx1jJ5wgZ7VP43kgJFfpiN0v3hvS1G8S5BeKuyzq6KdBrqZPW5mhWa2voPlZmYPmFmuma01s5NjX+YXkgLG5GEZbCqo6MmXEZFewPW2QAeeAGYfZPkFwBTvsQD4/aGXdXDTRw1kfX55T7+MiPhc2F953nmgO+feBw525WAe8JSL+AQYZGYjY1Vge44enkFxVQMVdY09+TIi4nM+O0CPyTn00cDuFs/zvHk9ZvTgpgujOo8uIt0X8lmiH9aLoma2wMxyzCynqKj7F8JGD4oEen6pAl1Euq83nkPvTD4wtsXzMd68AzjnFjrnsp1z2VlZWd1+waYjdPV0EZFD4bM8j0mgLwIu83q7zATKnXMFMVhvhzLT00hNDpCnI3QROQR+67aY3FkDM3sWOAvINLM84OdACoBz7hFgMTAHyAVqgCt6qtgmgYAxZlBfnXIRkUPit14unQa6c+6STpY74IcxqyhKowf3JU+nXETkEPjtCD3h7hRtMmZwX/JLa+JdhsSAWbwrkF7LX3meyIHej+KqBg2j6wM+O0iSBKJui0eI5q6LOu0iIt3kszxP3EAf43VdzNNpFxHpJn1J9BFizOB+AOq66AM6hy4SGwkb6MP6p5GSpGF0RaT7dMrlCBEIGKMG9dURuoh0m8/yPHEDHSIXRtfllcW7DDlEfjtKksTht7FcOr2x6Ej2r237gcioi6O8Xi8iItHyWZ4n9hH6xMx0AHaVqKdLItNFUYkXvx2hJ3SgnzZ5KABX/Gl5nCsRkUTkrzhP8ED/8XnHAJEvjhYR6SqfHaAndqAPTk8FdMpFRLrHZ3me2IEuInIodA79CHPb3KkAFFbWxbkS6a6qumC8SxDxhYQP9JPGDQJg5edlca1Duu83b38W7xJEfCHhA336qIEA3Pn6hjhXIiKJRl9wcYTpk5IEwJ5ynXIRkd4t4QNdRKS7Nu+tjHcJMeWLQD936nAASqob4lyJiCSSl1fmx7uEmPJFoF/9lYkALN1cGOdKRETixxeBPsPr6fLjv66JbyEiInHki0BPS05qng6H/XXVWkQkWr4IdIBrz54M+O8ih4hItHwT6JfOGg/A79/bFudKRETiwzeBPnxAHwBeX7OH9z8rinM1IiKHn28CHeDiU8YAcNnjn/pu0B0Rkc5EFehmNtvMtphZrpnd1M7y75hZkZmt9h5Xxb7Uzt178QnN07e9uj4eJYiIxE2ngW5mScDDwAXANOASM5vWTtPnnXMzvMejMa4zKmbGjLGDAHh62a54lCAiEjfRHKGfCuQ657Y75xqA54B5PVtW9710zZebp298YXX8ChEROcyiCfTRwO4Wz/O8eW19w8zWmtmLZjY2JtV1Q1LAeOhbJwGR23o37qmIVykiIodVrC6Kvg5McM6dALwNPNleIzNbYGY5ZpZTVNRzPVEuPGEUmRmRr6eb88AHvLmuoMdeS0TkSBFNoOcDLY+4x3jzmjnn9jvn6r2njwKntLci59xC51y2cy47KyurO/VGbfmt5zZPX/P0Sq57dhWvrMpj1a7SHn1dEZF4iSbQlwNTzGyimaUC84FFLRuY2cgWTy8CNsWuxO4xM3bcNaf5+aI1e/jR82v4j//5Fzk7S7jqyeWENEyAiPhIp4HunAsC1wJLiAT1C865DWZ2p5ld5DW7zsw2mNka4DrgOz1VcFc0hfrXT2p9yv/qp3J4Z1Mhf/xgOzc8typO1YmIxFZyNI2cc4uBxW3m3d5i+mbg5tiWFhtmxn3fnMFFM0bxnT8tB6C0phGAu9/cDMD980+KW30iIrHiqztFD+asY4bx6S3ntLts6z4N6CUiia/XBDrAsAF92Hn3XBZe2vqa7Xm/fZ/vPrGcdzbu4+2N++JUnYjIoelVgd7k/Okj2HHXHEYO7NM8793NhVz1VA5XP5WjcWBEJCH1ykCHyLn1j28+h3/+5CxmThrSatnEmxfzzT98fMDPVNUHqW0IHa4SRUS6pNcGepMJmek8t2AWf7wsu9X8ZTtKmHDT37j6qRzW55dT2xDiuJ8v4Sv3LG3VLhx2OqIXkSNCrw/0JudNG87Ou+fylyu/1Gr+2xv3ceGDHzL19rcAKK6K3D/12up8quqDTLplsb7LVESOCAr0Nk6fksnOu+fyyg++3GGbxz/cwfXPrea6ZyN92F9emd9hW+nYV6ZkxrsE6aXGD+0X7xJ6hAK9AyeNG8zOu+fy6S3nHBA8d76xEYhcSG3y4dZi1uwuY39VPe0pr2mkIRjuuYITUHLA4l2CiK9EdWNRbzZsQB/+fOWXCIUdP1+0nr980v446//7sWWtnn9t+nCyxw8ho08ymRlpXP1UDudOHc6jl0fO1YfCjltfWceVp09kyvD+Pb4dIuJ/CvQoJQWMX/778fzy34+nuj7IvUu28MS/dnbYfsmGfSzZ0LpP+zub9nH0bW8yOSuDb88cx3PLd/Pc8t3svHtuc5uiynr2VdRx3OiBPbUpIuJTCvRuSE9L5o6LpnPHRdMBKK9t5J9bCrn+udWd/mxDMMzGggpufeWLr8j729oC7lmymVPGD+a11XsIhR077ppDbmEVQ9JTSQoYhZX1HK0jeRE5CAV6DAzsm8K8GaOZNyMyCFhNQ5CPcvfzt7V7eHX1nk5//ofPrATg8/01zfMm3rz4gHYPfeskNuypoKY+yNwTRnHqxCEHtGmyr6KOl1bmcc2ZR2F2ZJ6rVmdPkdhSoPeAfqnJnDdtOOdNG9488Fc47NhYUMGWvZU89cnnrNld1uX1XvvMFyNDPvnx583TY4f0ZezgfhRX1ZM9YQiFFXXkldayeW8lU0cM4Oxjh/HB1iJuemkdb994Bv1Sk3lxRR4TM/txyviO3xR6Wsvu+9uKqjgqKyNutXTFmt1lHDOiP31SkuJdinSTX28dUaAfJoGAcdzogRw3eiDfOGVMq2XBUJh3Nu3j9TUFLNuxn+Kqhi6te3dJLbtLagH4bF9Vq2VXPLGcP1x6Ct/78woArnt2NY2hMO99FvnGqONHD+QrUzJJDhh7K+r48lGZTBs1gMq6RqaPGkhFXSMP/iOXCZnpXHn6RIKhMA2hMP1SkwmHHTWNITLSvvgzWrWrlHFD+jE0I63TusMt/lcVVtTHNdBDYUdSFL1u9pbXMe/hj/jGyWP4zf868TBUJj3B+fTzoQL9CJCcFGD2cSOZfdzIVvMbQ2H2lNWSW1jF2xv3UVxVzzubCjtYS8eawhwiF2ZbWpdfzrr88ubnL+TkdbiegrJa3tqwl7zSWiZlpuOAHcXV/G7+DIqrGrh81nj+43/+xbEj+nP5lyfw1vq9/OzCqRSU1/FvE4aQX1bbKrRbfsFIUQfdPQ+Fc46d+2uYmJl+0Hbbi6r46m/e4+FvnczcE0YetG1tY2Toh+U7S2JWpxx+OkKXwy4lKcD4oemMH5rOOVOHH7A8GApT3RBid0kNxVX1rM8vZ/nOUjYWRL4Yu6gytiH56Ic7mqe3F1c3TzddDP6/Xv/8zXsrufnldQC8d1/kk8CZR2fx3mdFXHv2ZF5Zlc/3zzqK1S1OO1337CouOnFUTOt9IWc3/+eldfz1+7M4edxgws6RkvTFrRcVdZFx8Zve0N7asLfTQG8a5qG9g/lXVuUxa1ImI1oM+tYbfbavktdW5/OT8485Iq/fOOfIK61tfh4OOwI+uSdCgZ7AkpMCDOwbYKDXxfGsY4Z12LYhGKa6PogDcgurKCivZeOeChpDjv3V9Xy8bT/FVfX01LfyNZ3ieWhpLgA/e3X9AW3W7C5jyvAM+qXG5s/yk+2Ro+gdRdXc89ZmVu8uY+uv5lBZ10j/PimccMffMYN7vnECACXV9TSGwtyxaAPfP/Moxg6J3E1YWdfIB1uLmXP8yOZPFTu9C9hFlfWU1jQwYmAffvT8GqaOHMCb13+lVR2l1Q1sL65qvl6xbPt+vrnwE9658QxKqhvJK63h6ye3Pg3XVc45HvtwB/NmjGZwvxQKyusYO6QfBeW11DaEmNTik1HTG9mAPindeq3GUBjnIDW5/fsSL31sGfsq6rnq9EkMTk/t1mu0VF7biHOOQf1ar2tfRR2vrMrne2dM6tIbxyurWt/ZHXKOAAp0SSCpyQFSkyP/IZp6xzT1yjkY5xyhsKMhFKawoh4HlFQ3UFrdQH5ZLa+v2cPRI/qTu6+KbUVV7K/u2vn/6746mQfejYT8vIc/AmBwvxTS05LJSEsm3Xv0T0smPS3pgPkZaUlkpKWQnpbUPL/p30ovuH760trm1/vDe9u4683NLLnhDG/7oN67g/ej3P18lFvM08t2sWFPBeccO4zhA/rw9qbIOPnv/9fZzadcmsy+/332VzfwwU/PBmBTQQUb91Swo7iauSeMZH9VPd9+dBmb91ay/b/nEAgYL62MnNZasmEf9y7ZAtAc6Kt2lfLNhZ/w4U/PZtiA1kf69y7ZTH1jmNsunEZhZR1D+qWS7H3i2LCngl/+bRMf5hYzfkg/nvz4c1bcdi6z7noXoNW9Difc8fcD5jnn+K8X1zL/38aSPeHgF8pn3/8+u0tq+exXF7S7vK4x8vtsDMXmzugz711KWU1jq3oBrvnLClbuKuPcqcOZPCz66y9bC1tfZwqFHX65vq1Al4MyM5KTjOSkABMyI38uLc9JX/7lCZ2uI+wd1RZV1VNZF6SspoGkgFHbGGLmxKHceP4xbN1Xybaiarbuq6Swsp6q+iBV9UGq64OU1zSQX1pDdX0oMq8h2O1zoHd5Xzv4tfvfb553W4tPCw96by6rd5e1OiUEcMa9S8nq/8XF3s/2VTa/gV3xxPLm+XMe+ACAZTvG81SL3kjn3PcepTUNlNdG3miawhzgxRV5LN1cyKc7S2gIhrnllXWcP20E724u5JTxgxk+sA8PL90GwPXnTuHUX/2DuSeM5Jozj+LCBz/krq8fD8A/txQ117h+T0Xz+uuDIZ74aCffnjm+eV447Fi+s4SJmemU1Tby4oo8XlyRx86751JeG/nkMG3kAJ74105mHzeC19fs4aisDLYVfXG6raW/b9jLgL4pzdv31xV5/PDsyUDkU8GPX1jDLy6azqhBfamuDxIMO1KTAmwsKG/V22pncTUBM8Z5462UeV8Z2WTX/hqGD0xrvu5SWtPAq6vyueH51az62XkMTk+lrKaBS/64jNsvnEZdY4gBfVM4ZfxgABrbDMGRW1h1WG/kC4cdZvTI6SiL19Cv2dnZLicnJy6vLYktHHbUNoaobg79UKs3gKZ/K+oam0Owb0oStY0hUpMCNMToyDHRTR6WQW6bo1WA382f0XxdJClgrS5et/Td0yby+EeR6ypTRw5gU0HFAW2eufpL3PPWFuoaQ2zeG/mqx+mjBrDBe7OZlJXO9qJqfnHRdH6+aAOXnDqOZz+NDK9x57zpbN5byTPLIs9f++FpTBs1gCm3vsnRwzNa9eg6YcxA1uaVc+3Zk/nJ145h4fvb+O/Fm0kOGEGv/nV3nM99b39GSXUDr7W4P+T0yZn85arWo6z2lIZgmKNve5Mbzzua686Z0q11mNkK51x2u8sU6NLbhcKOgEFFXZDKukbSkpMipzTSU8krrWVAnxRWfF7KkPRU0tOS+NvaAp5bvpvvnTGJAX1TePDdrRw7YgCFFXXsKa/j3KnDeWfTPi46cRRb9layraiqOVTSkgMEw67DkJSDG9wvpflL3mPpxvOOZvKwDLL6p9G/j3faLjWZlOQAKUlGSiAQkwunu/bXcMa9SzlxzEBeu/b0bq1DgS6SYMLhL3pKB8Nh6oNhSqoa2FtRR3pqMslJxsY9Fcw6aigb9lQQDIXZVVLDsh0lzJ4+gldX55OZkcYxI/rzwD+2Mm/GKNKSkyirbaRfShLP5+zmjKOzaAyGqW0MUVbTQFFlPdUNIeYcP4KiynqW7ywlMyO1+b6I/n2SqawLxu+XEmO3zplKXmkNf9+4j4Lyuk7bJwWMgEHAzHt404EOps0IBCLTwZCjMRSm0Ot59tyCmcycNLRbdSvQRSQhBUNh6oJh0lOTCLtId9H6YJj6xjD1oRAD+qSw1wvjlOQA1fVBJgxNp7KukeKqBuqDIfqlJrG3vJ6jR2RQUx+irLaRoempjBnct/k8dk1DkO1F1ZTWNFBZF6SqLnKtJhiKdAho9B5hF7khzrnIm26oadrrPBB2kQvMkedfTCcFAqQmGylJATIz0vjBWUc1X9DuqoMFui6KisgRKzkpQIYXfEneGY8+KUnesAuRbpcT2rlxbGhGWqu7lScP8wa262B8u36pyb4Y4VRfcCEi4hMKdBERn1Cgi4j4RFSBbmazzWyLmeWa2U3tLE8zs+e95cvMbELMKxURkYPqNNDNLAl4GLgAmAZcYmbT2jS7Eih1zk0Gfgv8OtaFiojIwUVzhH4qkOuc2+6cawCeA+a1aTMPeNKbfhE4x47EYdZERHwsmkAfDexu8TzPm9duG+dcECgHutdrXkREuuWwXhQ1swVmlmNmOUVFRYfzpUVEfC+aG4vygbEtno/x5rXXJs/MkoGBwP62K3LOLQQWAphZkZl93rZNlDKB4m7+bKLSNvcO2ube4VC2eXxHC6IJ9OXAFDObSCS45wPfatNmEXA58DFwMfCu62RMAedcVhSv3S4zy+no1le/0jb3Dtrm3qGntrnTQHfOBc3sWmAJkAQ87pzbYGZ3AjnOuUXAY8CfzSwXKCES+iIichhFNZaLc24xsLjNvNtbTNcB/xnb0kREpCsS9U7RhfEuIA60zb2Dtrl36JFtjtvwuSIiEluJeoQuIiJtKNBFRHwi4QK9s4HCEoWZjTWzpWa20cw2mNn13vwhZva2mW31/h3szTcze8Db7rVmdnKLdV3utd9qZpfHa5uiZWZJZrbKzN7wnk/0BnXL9QZ5S/Xmdzjom5nd7M3fYmZfi9OmRMXMBpnZi2a22cw2mdksv+9nM/uR93e93syeNbM+ftvPZva4mRWa2foW82K2X83sFDNb5/3MA2ZRDKfinEuYB5Fuk9uASUAqsAaYFu+6urktI4GTven+wGdEBj+7B7jJm38T8Gtveg7wJmDATGCZN38IsN37d7A3PTje29fJtt8IPAO84T1/AZjvTT8CXONN/wB4xJueDzzvTU/z9n0aMNH7m0iK93YdZHufBK7yplOBQX7ez0SGAtkB9G2xf7/jt/0MnAGcDKxvMS9m+xX41Gtr3s9e0GlN8f6ldPEXOAtY0uL5zcDN8a4rRtv2GnAesAUY6c0bCWzxpv8AXNKi/RZv+SXAH1rMb9XuSHsQudP4H8BXgTe8P9ZiILntPiZy78MsbzrZa2dt93vLdkfag8hd0zvwOiC03X9+3M98MbbTEG+/vQF8zY/7GZjQJtBjsl+9ZZtbzG/VrqNHop1yiWagsITjfcQ8CVgGDHfOFXiL9gLDvemOtj3Rfif3Az8Fwt7zoUCZiwzqBq3r72jQt0Ta5olAEfAn7zTTo2aWjo/3s3MuH/h/wC6ggMh+W4G/93OTWO3X0d502/kHlWiB7jtmlgG8BNzgnKtoucxF3pp906/UzC4ECp1zK+Jdy2GUTORj+e+dcycB1UQ+ijfz4X4eTGRI7YnAKCAdmB3XouIgHvs10QI9moHCEoaZpRAJ86edcy97s/eZ2Uhv+Uig0Jvf0bYn0u/kNOAiM9tJZFz9rwK/AwZZZFA3aF1/87ZZ60HfEmmb84A859wy7/mLRALez/v5XGCHc67IOdcIvExk3/t5PzeJ1X7N96bbzj+oRAv05oHCvCvk84kMDJZwvCvWjwGbnHP3tVjUNNAZ3r+vtZh/mXe1fCZQ7n20WwKcb2aDvSOj8715Rxzn3M3OuTHOuQlE9t27zrlvA0uJDOoGB25z0++i5aBvi4D5Xu+IicAUIheQjjjOub3AbjM7xpt1DrARH+9nIqdaZppZP+/vvGmbfbufW4jJfvWWVZjZTO93eFmLdXUs3hcVunERYg6RHiHbgFvjXc8hbMfpRD6OrQVWe485RM4d/gPYCrwDDPHaG5GvAtwGrAOyW6zru0Cu97gi3tsW5fafxRe9XCYR+Y+aC/wVSPPm9/Ge53rLJ7X4+Vu938UWorj6H+dtnQHkePv6VSK9GXy9n4FfAJuB9cCfifRU8dV+Bp4lco2gkcgnsStjuV+BbO/3tw14iDYX1tt76NZ/ERGfSLRTLiIi0gEFuoiITyjQRUR8QoEuIuITCnQREZ9QoIuI+IQCXUTEJ/4/8g4QiRGGi/UAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "dense1 = Layer_Dense(2,64)\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = Layer_Dense(64,3)\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "optimizer = Optimizer_Adam(learning_rate = 0.05, decay = 2e-7)\n",
    "l = []\n",
    "ep = []\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "    # Forward pass\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "    loss = loss_activation.forward(dense2.output,y)\n",
    "\n",
    "    predictions = np.argmax(loss_activation.output, axis = 1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis = 1)\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    ep.append(epoch)\n",
    "    l.append(loss)\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' + \n",
    "              f'acc: {accuracy:.3f}, ' + \n",
    "              f'loss: {loss:.3f}, ' + \n",
    "              f'lr: {optimizer.current_learning_rate}')\n",
    "    \n",
    "    # Backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    \n",
    "    # Update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()\n",
    "plt.plot(ep,l)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_SQRT:\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.abs_ = np.abs(inputs)\n",
    "        self.output = self.abs_\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "#         dsqrt = 1 / (2 * np.sqrt(self.abs_) + 1e-7)\n",
    "        dabs = np.where(self.inputs!=0,self.inputs / self.abs_,0)\n",
    "        self.dinputs = dabs * dvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.333, loss: 1.099, lr: 0.05\n",
      "epoch: 100, acc: 0.657, loss: 0.814, lr: 0.04999752512250644\n",
      "epoch: 200, acc: 0.733, loss: 0.706, lr: 0.04999502549496326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-49-ae0c223f2f0c>:9: RuntimeWarning: invalid value encountered in divide\n",
      "  dabs = np.where(self.inputs!=0,self.inputs / self.abs_,0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 300, acc: 0.760, loss: 0.640, lr: 0.049992526117345455\n",
      "epoch: 400, acc: 0.797, loss: 0.584, lr: 0.04999002698961558\n",
      "epoch: 500, acc: 0.817, loss: 0.524, lr: 0.049987528111736124\n",
      "epoch: 600, acc: 0.817, loss: 0.496, lr: 0.049985029483669646\n",
      "epoch: 700, acc: 0.837, loss: 0.474, lr: 0.049982531105378675\n",
      "epoch: 800, acc: 0.767, loss: 0.476, lr: 0.04998003297682575\n",
      "epoch: 900, acc: 0.773, loss: 0.466, lr: 0.049977535097973466\n",
      "epoch: 1000, acc: 0.803, loss: 0.434, lr: 0.049975037468784345\n",
      "epoch: 1100, acc: 0.840, loss: 0.393, lr: 0.049972540089220974\n",
      "epoch: 1200, acc: 0.867, loss: 0.365, lr: 0.04997004295924593\n",
      "epoch: 1300, acc: 0.867, loss: 0.365, lr: 0.04996754607882181\n",
      "epoch: 1400, acc: 0.857, loss: 0.364, lr: 0.049965049447911185\n",
      "epoch: 1500, acc: 0.860, loss: 0.356, lr: 0.04996255306647668\n",
      "epoch: 1600, acc: 0.880, loss: 0.325, lr: 0.049960056934480884\n",
      "epoch: 1700, acc: 0.893, loss: 0.313, lr: 0.04995756105188642\n",
      "epoch: 1800, acc: 0.867, loss: 0.311, lr: 0.049955065418655915\n",
      "epoch: 1900, acc: 0.903, loss: 0.311, lr: 0.04995257003475201\n",
      "epoch: 2000, acc: 0.887, loss: 0.299, lr: 0.04995007490013731\n",
      "epoch: 2100, acc: 0.883, loss: 0.287, lr: 0.0499475800147745\n",
      "epoch: 2200, acc: 0.890, loss: 0.290, lr: 0.0499450853786262\n",
      "epoch: 2300, acc: 0.883, loss: 0.274, lr: 0.0499425909916551\n",
      "epoch: 2400, acc: 0.893, loss: 0.266, lr: 0.04994009685382384\n",
      "epoch: 2500, acc: 0.877, loss: 0.283, lr: 0.04993760296509512\n",
      "epoch: 2600, acc: 0.880, loss: 0.274, lr: 0.049935109325431604\n",
      "epoch: 2700, acc: 0.883, loss: 0.273, lr: 0.049932615934796004\n",
      "epoch: 2800, acc: 0.883, loss: 0.260, lr: 0.04993012279315098\n",
      "epoch: 2900, acc: 0.890, loss: 0.257, lr: 0.049927629900459285\n",
      "epoch: 3000, acc: 0.893, loss: 0.259, lr: 0.049925137256683606\n",
      "epoch: 3100, acc: 0.883, loss: 0.262, lr: 0.04992264486178666\n",
      "epoch: 3200, acc: 0.883, loss: 0.254, lr: 0.04992015271573119\n",
      "epoch: 3300, acc: 0.893, loss: 0.251, lr: 0.04991766081847992\n",
      "epoch: 3400, acc: 0.893, loss: 0.245, lr: 0.049915169169995596\n",
      "epoch: 3500, acc: 0.893, loss: 0.251, lr: 0.049912677770240964\n",
      "epoch: 3600, acc: 0.880, loss: 0.257, lr: 0.049910186619178794\n",
      "epoch: 3700, acc: 0.893, loss: 0.246, lr: 0.04990769571677183\n",
      "epoch: 3800, acc: 0.887, loss: 0.250, lr: 0.04990520506298287\n",
      "epoch: 3900, acc: 0.887, loss: 0.252, lr: 0.04990271465777467\n",
      "epoch: 4000, acc: 0.883, loss: 0.257, lr: 0.049900224501110035\n",
      "epoch: 4100, acc: 0.890, loss: 0.243, lr: 0.04989773459295174\n",
      "epoch: 4200, acc: 0.883, loss: 0.253, lr: 0.04989524493326262\n",
      "epoch: 4300, acc: 0.887, loss: 0.238, lr: 0.04989275552200545\n",
      "epoch: 4400, acc: 0.903, loss: 0.235, lr: 0.04989026635914307\n",
      "epoch: 4500, acc: 0.903, loss: 0.236, lr: 0.04988777744463829\n",
      "epoch: 4600, acc: 0.890, loss: 0.244, lr: 0.049885288778453954\n",
      "epoch: 4700, acc: 0.897, loss: 0.237, lr: 0.049882800360552884\n",
      "epoch: 4800, acc: 0.900, loss: 0.238, lr: 0.04988031219089794\n",
      "epoch: 4900, acc: 0.890, loss: 0.232, lr: 0.049877824269451976\n",
      "epoch: 5000, acc: 0.890, loss: 0.237, lr: 0.04987533659617785\n",
      "epoch: 5100, acc: 0.900, loss: 0.233, lr: 0.04987284917103844\n",
      "epoch: 5200, acc: 0.897, loss: 0.240, lr: 0.04987036199399661\n",
      "epoch: 5300, acc: 0.893, loss: 0.226, lr: 0.04986787506501525\n",
      "epoch: 5400, acc: 0.907, loss: 0.231, lr: 0.04986538838405724\n",
      "epoch: 5500, acc: 0.900, loss: 0.230, lr: 0.049862901951085496\n",
      "epoch: 5600, acc: 0.897, loss: 0.229, lr: 0.049860415766062906\n",
      "epoch: 5700, acc: 0.903, loss: 0.223, lr: 0.0498579298289524\n",
      "epoch: 5800, acc: 0.900, loss: 0.220, lr: 0.04985544413971689\n",
      "epoch: 5900, acc: 0.893, loss: 0.226, lr: 0.049852958698319315\n",
      "epoch: 6000, acc: 0.897, loss: 0.233, lr: 0.04985047350472258\n",
      "epoch: 6100, acc: 0.900, loss: 0.227, lr: 0.04984798855888967\n",
      "epoch: 6200, acc: 0.900, loss: 0.223, lr: 0.049845503860783506\n",
      "epoch: 6300, acc: 0.903, loss: 0.221, lr: 0.049843019410367055\n",
      "epoch: 6400, acc: 0.897, loss: 0.234, lr: 0.04984053520760327\n",
      "epoch: 6500, acc: 0.897, loss: 0.220, lr: 0.049838051252455155\n",
      "epoch: 6600, acc: 0.903, loss: 0.215, lr: 0.049835567544885655\n",
      "epoch: 6700, acc: 0.907, loss: 0.215, lr: 0.04983308408485778\n",
      "epoch: 6800, acc: 0.900, loss: 0.213, lr: 0.0498306008723345\n",
      "epoch: 6900, acc: 0.900, loss: 0.225, lr: 0.04982811790727884\n",
      "epoch: 7000, acc: 0.903, loss: 0.219, lr: 0.04982563518965381\n",
      "epoch: 7100, acc: 0.900, loss: 0.224, lr: 0.049823152719422406\n",
      "epoch: 7200, acc: 0.897, loss: 0.225, lr: 0.049820670496547675\n",
      "epoch: 7300, acc: 0.893, loss: 0.234, lr: 0.04981818852099264\n",
      "epoch: 7400, acc: 0.903, loss: 0.219, lr: 0.049815706792720335\n",
      "epoch: 7500, acc: 0.900, loss: 0.221, lr: 0.0498132253116938\n",
      "epoch: 7600, acc: 0.897, loss: 0.216, lr: 0.04981074407787611\n",
      "epoch: 7700, acc: 0.907, loss: 0.218, lr: 0.049808263091230306\n",
      "epoch: 7800, acc: 0.903, loss: 0.217, lr: 0.04980578235171948\n",
      "epoch: 7900, acc: 0.903, loss: 0.213, lr: 0.04980330185930667\n",
      "epoch: 8000, acc: 0.900, loss: 0.215, lr: 0.04980082161395499\n",
      "epoch: 8100, acc: 0.900, loss: 0.218, lr: 0.04979834161562752\n",
      "epoch: 8200, acc: 0.900, loss: 0.218, lr: 0.04979586186428736\n",
      "epoch: 8300, acc: 0.900, loss: 0.214, lr: 0.04979338235989761\n",
      "epoch: 8400, acc: 0.900, loss: 0.224, lr: 0.04979090310242139\n",
      "epoch: 8500, acc: 0.897, loss: 0.224, lr: 0.049788424091821805\n",
      "epoch: 8600, acc: 0.897, loss: 0.220, lr: 0.049785945328062006\n",
      "epoch: 8700, acc: 0.903, loss: 0.209, lr: 0.0497834668111051\n",
      "epoch: 8800, acc: 0.903, loss: 0.221, lr: 0.049780988540914256\n",
      "epoch: 8900, acc: 0.893, loss: 0.225, lr: 0.0497785105174526\n",
      "epoch: 9000, acc: 0.900, loss: 0.217, lr: 0.04977603274068329\n",
      "epoch: 9100, acc: 0.897, loss: 0.223, lr: 0.04977355521056952\n",
      "epoch: 9200, acc: 0.900, loss: 0.214, lr: 0.049771077927074414\n",
      "epoch: 9300, acc: 0.900, loss: 0.216, lr: 0.0497686008901612\n",
      "epoch: 9400, acc: 0.900, loss: 0.220, lr: 0.04976612409979302\n",
      "epoch: 9500, acc: 0.903, loss: 0.208, lr: 0.0497636475559331\n",
      "epoch: 9600, acc: 0.900, loss: 0.217, lr: 0.049761171258544616\n",
      "epoch: 9700, acc: 0.907, loss: 0.209, lr: 0.0497586952075908\n",
      "epoch: 9800, acc: 0.900, loss: 0.218, lr: 0.04975621940303483\n",
      "epoch: 9900, acc: 0.900, loss: 0.211, lr: 0.049753743844839965\n",
      "epoch: 10000, acc: 0.893, loss: 0.218, lr: 0.04975126853296942\n",
      "epoch: 10100, acc: 0.900, loss: 0.215, lr: 0.04974879346738644\n",
      "epoch: 10200, acc: 0.897, loss: 0.213, lr: 0.04974631864805425\n",
      "epoch: 10300, acc: 0.900, loss: 0.204, lr: 0.04974384407493612\n",
      "epoch: 10400, acc: 0.903, loss: 0.214, lr: 0.0497413697479953\n",
      "epoch: 10500, acc: 0.903, loss: 0.209, lr: 0.04973889566719507\n",
      "epoch: 10600, acc: 0.910, loss: 0.210, lr: 0.04973642183249868\n",
      "epoch: 10700, acc: 0.900, loss: 0.209, lr: 0.049733948243869425\n",
      "epoch: 10800, acc: 0.900, loss: 0.217, lr: 0.04973147490127059\n",
      "epoch: 10900, acc: 0.907, loss: 0.208, lr: 0.04972900180466547\n",
      "epoch: 11000, acc: 0.913, loss: 0.205, lr: 0.049726528954017385\n",
      "epoch: 11100, acc: 0.900, loss: 0.216, lr: 0.049724056349289614\n",
      "epoch: 11200, acc: 0.913, loss: 0.206, lr: 0.0497215839904455\n",
      "epoch: 11300, acc: 0.913, loss: 0.209, lr: 0.04971911187744835\n",
      "epoch: 11400, acc: 0.910, loss: 0.216, lr: 0.04971664001026152\n",
      "epoch: 11500, acc: 0.913, loss: 0.209, lr: 0.04971416838884832\n",
      "epoch: 11600, acc: 0.923, loss: 0.200, lr: 0.049711697013172115\n",
      "epoch: 11700, acc: 0.910, loss: 0.219, lr: 0.04970922588319624\n",
      "epoch: 11800, acc: 0.917, loss: 0.203, lr: 0.049706754998884085\n",
      "epoch: 11900, acc: 0.907, loss: 0.217, lr: 0.049704284360199\n",
      "epoch: 12000, acc: 0.910, loss: 0.220, lr: 0.04970181396710436\n",
      "epoch: 12100, acc: 0.913, loss: 0.208, lr: 0.04969934381956355\n",
      "epoch: 12200, acc: 0.907, loss: 0.211, lr: 0.04969687391753997\n",
      "epoch: 12300, acc: 0.913, loss: 0.202, lr: 0.049694404260997\n",
      "epoch: 12400, acc: 0.913, loss: 0.205, lr: 0.04969193484989806\n",
      "epoch: 12500, acc: 0.907, loss: 0.206, lr: 0.049689465684206555\n",
      "epoch: 12600, acc: 0.903, loss: 0.216, lr: 0.04968699676388591\n",
      "epoch: 12700, acc: 0.903, loss: 0.214, lr: 0.04968452808889953\n",
      "epoch: 12800, acc: 0.900, loss: 0.211, lr: 0.04968205965921089\n",
      "epoch: 12900, acc: 0.903, loss: 0.206, lr: 0.04967959147478339\n",
      "epoch: 13000, acc: 0.890, loss: 0.219, lr: 0.04967712353558049\n",
      "epoch: 13100, acc: 0.900, loss: 0.207, lr: 0.04967465584156567\n",
      "epoch: 13200, acc: 0.903, loss: 0.211, lr: 0.049672188392702356\n",
      "epoch: 13300, acc: 0.903, loss: 0.208, lr: 0.049669721188954054\n",
      "epoch: 13400, acc: 0.900, loss: 0.206, lr: 0.04966725423028421\n",
      "epoch: 13500, acc: 0.907, loss: 0.211, lr: 0.04966478751665633\n",
      "epoch: 13600, acc: 0.893, loss: 0.215, lr: 0.04966232104803389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 13700, acc: 0.903, loss: 0.215, lr: 0.04965985482438041\n",
      "epoch: 13800, acc: 0.903, loss: 0.204, lr: 0.04965738884565937\n",
      "epoch: 13900, acc: 0.900, loss: 0.212, lr: 0.04965492311183431\n",
      "epoch: 14000, acc: 0.897, loss: 0.200, lr: 0.04965245762286873\n",
      "epoch: 14100, acc: 0.900, loss: 0.203, lr: 0.04964999237872617\n",
      "epoch: 14200, acc: 0.910, loss: 0.206, lr: 0.049647527379370164\n",
      "epoch: 14300, acc: 0.897, loss: 0.213, lr: 0.04964506262476426\n",
      "epoch: 14400, acc: 0.903, loss: 0.208, lr: 0.04964259811487198\n",
      "epoch: 14500, acc: 0.900, loss: 0.207, lr: 0.04964013384965692\n",
      "epoch: 14600, acc: 0.897, loss: 0.199, lr: 0.04963766982908261\n",
      "epoch: 14700, acc: 0.910, loss: 0.208, lr: 0.049635206053112654\n",
      "epoch: 14800, acc: 0.900, loss: 0.210, lr: 0.049632742521710606\n",
      "epoch: 14900, acc: 0.923, loss: 0.204, lr: 0.049630279234840056\n",
      "epoch: 15000, acc: 0.917, loss: 0.205, lr: 0.04962781619246462\n",
      "epoch: 15100, acc: 0.923, loss: 0.202, lr: 0.04962535339454786\n",
      "epoch: 15200, acc: 0.917, loss: 0.203, lr: 0.04962289084105342\n",
      "epoch: 15300, acc: 0.910, loss: 0.212, lr: 0.049620428531944885\n",
      "epoch: 15400, acc: 0.913, loss: 0.204, lr: 0.04961796646718591\n",
      "epoch: 15500, acc: 0.923, loss: 0.201, lr: 0.04961550464674008\n",
      "epoch: 15600, acc: 0.903, loss: 0.206, lr: 0.049613043070571086\n",
      "epoch: 15700, acc: 0.923, loss: 0.196, lr: 0.04961058173864252\n",
      "epoch: 15800, acc: 0.907, loss: 0.215, lr: 0.049608120650918075\n",
      "epoch: 15900, acc: 0.907, loss: 0.207, lr: 0.04960565980736138\n",
      "epoch: 16000, acc: 0.913, loss: 0.209, lr: 0.04960319920793612\n",
      "epoch: 16100, acc: 0.907, loss: 0.211, lr: 0.04960073885260595\n",
      "epoch: 16200, acc: 0.900, loss: 0.208, lr: 0.049598278741334566\n",
      "epoch: 16300, acc: 0.910, loss: 0.202, lr: 0.049595818874085644\n",
      "epoch: 16400, acc: 0.907, loss: 0.205, lr: 0.04959335925082289\n",
      "epoch: 16500, acc: 0.907, loss: 0.205, lr: 0.049590899871509976\n",
      "epoch: 16600, acc: 0.897, loss: 0.214, lr: 0.049588440736110656\n",
      "epoch: 16700, acc: 0.893, loss: 0.221, lr: 0.04958598184458861\n",
      "epoch: 16800, acc: 0.897, loss: 0.210, lr: 0.049583523196907585\n",
      "epoch: 16900, acc: 0.893, loss: 0.219, lr: 0.04958106479303129\n",
      "epoch: 17000, acc: 0.903, loss: 0.205, lr: 0.04957860663292346\n",
      "epoch: 17100, acc: 0.890, loss: 0.218, lr: 0.04957614871654788\n",
      "epoch: 17200, acc: 0.897, loss: 0.211, lr: 0.04957369104386825\n",
      "epoch: 17300, acc: 0.907, loss: 0.199, lr: 0.04957123361484837\n",
      "epoch: 17400, acc: 0.907, loss: 0.198, lr: 0.04956877642945198\n",
      "epoch: 17500, acc: 0.893, loss: 0.217, lr: 0.04956631948764287\n",
      "epoch: 17600, acc: 0.897, loss: 0.211, lr: 0.04956386278938481\n",
      "epoch: 17700, acc: 0.907, loss: 0.202, lr: 0.049561406334641595\n",
      "epoch: 17800, acc: 0.907, loss: 0.203, lr: 0.04955895012337701\n",
      "epoch: 17900, acc: 0.897, loss: 0.212, lr: 0.04955649415555487\n",
      "epoch: 18000, acc: 0.903, loss: 0.202, lr: 0.049554038431138966\n",
      "epoch: 18100, acc: 0.913, loss: 0.198, lr: 0.04955158295009314\n",
      "epoch: 18200, acc: 0.907, loss: 0.205, lr: 0.04954912771238119\n",
      "epoch: 18300, acc: 0.907, loss: 0.201, lr: 0.04954667271796697\n",
      "epoch: 18400, acc: 0.893, loss: 0.215, lr: 0.0495442179668143\n",
      "epoch: 18500, acc: 0.897, loss: 0.196, lr: 0.04954176345888703\n",
      "epoch: 18600, acc: 0.903, loss: 0.206, lr: 0.04953930919414901\n",
      "epoch: 18700, acc: 0.910, loss: 0.195, lr: 0.04953685517256412\n",
      "epoch: 18800, acc: 0.900, loss: 0.198, lr: 0.0495344013940962\n",
      "epoch: 18900, acc: 0.897, loss: 0.208, lr: 0.049531947858709124\n",
      "epoch: 19000, acc: 0.903, loss: 0.203, lr: 0.0495294945663668\n",
      "epoch: 19100, acc: 0.903, loss: 0.210, lr: 0.04952704151703309\n",
      "epoch: 19200, acc: 0.897, loss: 0.203, lr: 0.04952458871067191\n",
      "epoch: 19300, acc: 0.903, loss: 0.194, lr: 0.04952213614724714\n",
      "epoch: 19400, acc: 0.903, loss: 0.201, lr: 0.049519683826722706\n",
      "epoch: 19500, acc: 0.910, loss: 0.200, lr: 0.049517231749062515\n",
      "epoch: 19600, acc: 0.910, loss: 0.197, lr: 0.049514779914230506\n",
      "epoch: 19700, acc: 0.903, loss: 0.191, lr: 0.04951232832219058\n",
      "epoch: 19800, acc: 0.890, loss: 0.202, lr: 0.049509876972906715\n",
      "epoch: 19900, acc: 0.900, loss: 0.205, lr: 0.049507425866342825\n",
      "epoch: 20000, acc: 0.900, loss: 0.208, lr: 0.04950497500246288\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "dense1 = Layer_Dense(2,64)\n",
    "activation1 = Activation_SQRT()\n",
    "dense2 = Layer_Dense(64,3)\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "optimizer = Optimizer_Adam(learning_rate = 0.05, decay = 5e-7)\n",
    "l = []\n",
    "ep = []\n",
    "# Train in loop\n",
    "for epoch in range(20001):\n",
    "    # Forward pass\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "    loss = loss_activation.forward(dense2.output,y)\n",
    "\n",
    "    predictions = np.argmax(loss_activation.output, axis = 1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis = 1)\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    ep.append(epoch)\n",
    "    l.append(loss)\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' + \n",
    "              f'acc: {accuracy:.3f}, ' + \n",
    "              f'loss: {loss:.3f}, ' + \n",
    "              f'lr: {optimizer.current_learning_rate}')\n",
    "    \n",
    "    # Backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    \n",
    "    # Update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()\n",
    "plt.plot(ep,l)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
