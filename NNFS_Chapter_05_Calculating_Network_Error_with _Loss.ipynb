{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Cross-Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35667494393873245"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# An example output from the output layer of the neural network\n",
    "softmax_output = [0.7, 0.1, 0.2]\n",
    "# Ground truth\n",
    "target_output = [1, 0, 0]\n",
    "\n",
    "loss = - (math.log(softmax_output[0]) * target_output[0] +\n",
    "          math.log(softmax_output[1]) * target_output[1] +\n",
    "          math.log(softmax_output[2]) * target_output[2])\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log to the base e or ln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6486586255873816\n"
     ]
    }
   ],
   "source": [
    "b= 5.2\n",
    "print(np.log(5.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.199999999999999\n"
     ]
    }
   ],
   "source": [
    "# We can confirm this by exponentiating our reslut:\n",
    "print(math.e ** 1.6486586255873816)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Work on batches and Make negative log calculation dynamic to the target index\n",
    "let's assume the target class is sparse means 0 represent dog, 1 cat and 2 human\n",
    "The target classes is not one hot encoded in which the array is 2D and the correct label is represented by 1 while the rest with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7\n",
      "0.5\n",
      "0.9\n"
     ]
    }
   ],
   "source": [
    "softmax_outputs = [[0.7, 0.1, 0.2],\n",
    "                  [0.1, 0.5, 0.4],\n",
    "                  [0.02, 0.9, 0.08]]\n",
    "\n",
    "class_targets = [0, 1, 1]\n",
    "\n",
    "for targ_idx, distribution in zip(class_targets, softmax_outputs):\n",
    "    print(distribution[targ_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7 0.5 0.9]\n"
     ]
    }
   ],
   "source": [
    "oftmax_outputs = np.array([[0.7, 0.1, 0.2],\n",
    "                           [0.1, 0.5, 0.4],\n",
    "                           [0.02, 0.9, 0.08]])\n",
    "\n",
    "class_targets = [0, 1, 1]\n",
    "print(softmax_outputs[[0, 1, 2], class_targets])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use arange instead of the hard coded indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7 0.5 0.9]\n"
     ]
    }
   ],
   "source": [
    "# Example softmax outputs (as a NumPy array)\n",
    "softmax_outputs = np.array([[0.7, 0.1, 0.2],\n",
    "                            [0.1, 0.5, 0.4],\n",
    "                            [0.02, 0.9, 0.08]])\n",
    "\n",
    "# Corresponding class targets (as a NumPy array)\n",
    "class_targets = np.array([0, 1, 1])\n",
    "\n",
    "# Using array indexing to get the values\n",
    "output_values = softmax_outputs[range(len(class_targets)), class_targets]\n",
    "\n",
    "# Print the extracted values\n",
    "print(output_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now apply Negative Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.35667494 0.69314718 0.10536052]\n"
     ]
    }
   ],
   "source": [
    "print(-np.log(softmax_outputs[range(len(softmax_outputs)), class_targets]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find average loss per batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "neg_log = -np.log(softmax_outputs[\n",
    "    range(len(softmax_outputs)), class_targets\n",
    "])\n",
    "\n",
    "average_loss = np.mean(neg_log)\n",
    "print(average_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generalize for the One-Hot Encoded Labels and Sparse (Categorical) Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "softmax_outputs = np.array([[0.7, 0.1, 0.2],\n",
    "                            [0.1, 0.5, 0.4],\n",
    "                            [0.02, 0.9, 0.08]])\n",
    "\n",
    "class_targets = np.array([[1, 0, 0],\n",
    " [0, 1, 0],\n",
    " [0, 1, 0]])\n",
    "\n",
    "\n",
    "# Probabilities for target values only if categoricals labels (sparse)\n",
    "if len(class_targets.shape) == 1:\n",
    "    correct_confidences = softmax_output[\n",
    "        range(len(softmax_outputs)),\n",
    "        class_targets\n",
    "    ]\n",
    "    \n",
    "# Mask values - Only for one-hot encoded labels\n",
    "elif len(class_targets.shape) == 2:\n",
    "    correct_confidences = np.sum(\n",
    "    softmax_outputs * class_targets,\n",
    "    axis = 1)\n",
    "    \n",
    "# Losses\n",
    "neg_log = -np.log(correct_confidences)\n",
    "\n",
    "average_loss = np.mean(neg_log)\n",
    "print(average_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### log(0)\n",
    "if model predict 0 for a given class category and the actual label is true then what will be the cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-40-029f64eda5e8>:1: RuntimeWarning: divide by zero encountered in log\n",
      "  print(-np.log(0))\n"
     ]
    }
   ],
   "source": [
    "print(-np.log(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(np.e**(-np.inf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there is an infinity in the list and we want the mean error then the result will be infinity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-42-a63331cb6c12>:1: RuntimeWarning: divide by zero encountered in log\n",
      "  print(np.mean([1, 2, 3, -np.log(0)]))\n"
     ]
    }
   ],
   "source": [
    "print(np.mean([1, 2, 3, -np.log(0)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could add a very small value to the confidence to prevent it from being a zero, for example, \n",
    "1e-7:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.11809565095832\n"
     ]
    }
   ],
   "source": [
    "print(-np.log(1e-7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding a very small value, one-tenth of a million, to the confidence at its far edge will \n",
    "insignificantly impact the result, but this method yields an additional 2 issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-9.999999505838704e-08\n"
     ]
    }
   ],
   "source": [
    "print(-np.log(1+1e-7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the model is fully correct in a prediction and puts all the confidence in the correct label, \n",
    "loss becomes a negative value instead of being 0. The other problem here is shifting confidence \n",
    "towards 1, even if by a very small value.\n",
    "\n",
    "To prevent both issues, it’s better to clip values from both \n",
    "sides by the same number, 1e-7 in our case. That means that the lowest possible value will become \n",
    "1e-7 (like in the demonstration we just performed) but the highest possible value, instead of being \n",
    "1+1e-7, will become 1-1e-7 (so slightly less than 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0000000494736474e-07\n"
     ]
    }
   ],
   "source": [
    "print(-np.log(1-1e-7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will prevent loss from being exactly 0, making it a very small value instead, but won’t make \n",
    "it a negative value and won’t bias overall loss towards 1. Within our code and using numpy, we’ll \n",
    "accomplish that using np.clip() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-c9df22fde2c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my_pred_clipped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1e-7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1e-7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'y_pred' is not defined"
     ]
    }
   ],
   "source": [
    "y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.7  0.1  0.2 ]\n",
      " [0.1  0.5  0.4 ]\n",
      " [0.02 0.9  0.08]]\n"
     ]
    }
   ],
   "source": [
    "softmax_outputs = np.array([[0.7, 0.1, 0.2],\n",
    "                            [0.1, 0.5, 0.4],\n",
    "                            [0.02, 0.9, 0.08]])\n",
    "\n",
    "y_pred_clipped = np.clip(softmax_outputs, 1e-7, 1 - 1e-7)\n",
    "print(y_pred_clipped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Loss Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    \n",
    "    # Calculate the data and the regularization losses given the model output and ground truth values\n",
    "    def calculate(self, output, y):\n",
    "        \n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "        \n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        \n",
    "        # Return loss\n",
    "        return data_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Cross-entropy loss class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_CategoricalCrossentropy(Loss): # Inhirits Loss class\n",
    "    \n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "        \n",
    "        # Number of samples in a bathc\n",
    "        samples = len(y_pred)\n",
    "        \n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        \n",
    "        # Probabilities for target values - only if categorical\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[\n",
    "                range(samples),\n",
    "                y_true\n",
    "            ]\n",
    "        \n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(\n",
    "            y_pred_clipped * y_true,\n",
    "            axis = 1\n",
    "            )\n",
    "            \n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the manually-created \n",
    "output and targets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "loss = loss_function.calculate(softmax_outputs, class_targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining everything up to this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "nnfs.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dense Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Dense:\n",
    "    \n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  ReLU activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_ReLU:\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0,inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Softmax:\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis = 1, keepdims = True))\n",
    "        probabilities = exp_values / np.sum(exp_values, axis = 1, keepdims = True)\n",
    "        self.output = probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    \n",
    "    def calculate(self, output, y):\n",
    "        sample_losses = self.forward(output, y)\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        return data_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_Categorical_Crossentropy(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        samples = len(y_pred)\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[\n",
    "                range(samples),\n",
    "                y_true\n",
    "            ]\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(\n",
    "                y_pred_clipped * y_true, axis = 1)\n",
    "        negative_log_likelihoods = - np.log(correct_confidences)\n",
    "        return negative_log_likelihoods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333334 0.33333334 0.33333334]\n",
      " [0.33333343 0.33333355 0.33333296]\n",
      " [0.33333376 0.33333385 0.3333323 ]\n",
      " [0.333334   0.3333343  0.3333318 ]\n",
      " [0.33333403 0.33333457 0.3333315 ]]\n",
      "Loss:  1.098608\n"
     ]
    }
   ],
   "source": [
    "X, y = spiral_data(samples = 100, classes = 3)\n",
    "dense1 = Layer_Dense(2,3)\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = Layer_Dense(3,3)\n",
    "activation2 = Activation_Softmax()\n",
    "loss_function = Loss_Categorical_Crossentropy()\n",
    "\n",
    "dense1.forward(X)\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "dense2.forward(activation1.output)\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "print(activation2.output[:5])\n",
    "\n",
    "loss = loss_function.calculate(activation2.output, y)\n",
    "print('Loss: ',loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "# Probabilities of 3 samples\n",
    "softmax_outputs = np.array([[0.7, 0.2, 0.1],\n",
    "                            [0.5, 0.1, 0.4],\n",
    "                            [0.02, 0.9, 0.08]])\n",
    "\n",
    "class_targets = np.array([0, 1, 1])\n",
    "\n",
    "predictions = np.argmax(softmax_outputs, axis = 1)\n",
    "\n",
    "# if class targets are one-hot encoded labels - convert them\n",
    "if len(class_targets.shape) == 2:\n",
    "    class_targets = np.argmax(class_target, axis = 1)\n",
    "\n",
    "accuracy = np.mean(predictions == class_targets)\n",
    "print('acc', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add the following to the end of our full script above to calculate its accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc 0.35\n"
     ]
    }
   ],
   "source": [
    "predictions = np.argmax(activation2.output, axis = 1)\n",
    "if len(y.shape) == 2:\n",
    "    y = np.argmax(y, axis = 1)\n",
    "\n",
    "accuracy = np.mean(predictions == y)\n",
    "print('acc', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
